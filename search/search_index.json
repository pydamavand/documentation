{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Damavand","text":""},{"location":"#introduction","title":"Introduction","text":"<p>Damavand is a package to simplify rotary machines vibration-based analysis, through standardizing downloading, loading and transforming processes. The main motivation behind developing it is to democratize rotary machine intelligent predictive maintenance, through the development of an end-to-end unified data processing framework, covering from downloading the raw data to data preprocessing.</p>"},{"location":"#installation","title":"Installation","text":"<p>Currently, Damavand is accessible through the official Github repository, as below:</p> <pre><code>git clone https://github.com/amirberenji1995/damavand\n</code></pre> <p>Once the repository is cloned, install the dependencies as below:</p> <pre><code>pip install -r damavand/requirements.txt\n</code></pre>"},{"location":"#quickstart","title":"Quickstart","text":"<p>Once the package is installed, its whole functionality is accessible; the code snippet below, demonstrate a simple usage scenario, where a dataset is downloaded, loaded and processed.</p> <pre><code># Importings\nfrom damavand.damavand.datasets.downloaders import read_addresses, ZipDatasetDownloader\nfrom damavand.damavand.datasets.digestors import UoO\nimport pandas as pd\n\n# Downloading the dataset\naddresses = read_addresses() # reading the addresses\ndownloader = ZipDatasetDownloader(addresses['UoO']) # instantiating the downloader to download the UoO dataset (https://data.mendeley.com/datasets/v43hmbwxpm/1)\ndownloader.download_extract('UoO.zip', 'UoO/')  # downloading and extracting the dataset\n\n# Mining the dataset\ndataset = UoO('UoO/', ['Channel_1', 'Channel_2'], [1]) # instantiating the dataset\nmining_params = {'win_len': 10000, 'hop_len': 10000} # defining the mining parameters\ndataset.mine(mining_params) # mining the dataset\n\n# Aggregating the mined data over the first channel\ndf = pd.concat(dataset.data['Channel_1']).reset_index(drop = True)\n\n# Signal/Metadata split\nsignals, metadata = df.iloc[:, : -3], df.iloc[:, -3 :] # last three columns are state, loading and repetition; therefore, they are excluded into metadata\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<p>Detailed API reference of each module is accessible through the links, below: - utils - signal_processing - datasets - augmentations</p>"},{"location":"#demonstrations-and-tutorials","title":"Demonstrations and Tutorials","text":"<p>For each dataset available in this package, a detailed demonstration is provided that includes downloading, mining and time-domain visualization. These demonstrations can be found here.</p> <p>Additionally, following tutorials are provided:</p> <ol> <li>Signal Processing 101</li> <li>How to develop a digestor for a custom dataset?</li> <li>How to develop a custom feature to extract?</li> <li>Anomaly detection (Comming soon!)</li> <li>Health state classification</li> </ol>"},{"location":"#license","title":"License","text":"<p>Damavand is dual-licensed: free for non-commercial use under the PolyForm Noncommercial License 1.0.0.</p> <p>Use within a commercial product or for internal business operations in a for-profit organization requires a separate commercial license. Please contact ahberenji@gmail.com for inquiries.  </p> <p>See LICENSES/LICENSE.md for full details.</p>"},{"location":"#next-steps","title":"Next steps","text":"<p>We highly encourage other developers to help in extending the Damavand, particularly in the following directions:</p> <ol> <li>Adding new datasets</li> <li>Adding new signal processing methods</li> </ol>"},{"location":"#cite","title":"Cite","text":""},{"location":"advanced/","title":"Advanced Topics","text":""},{"location":"advanced/#terminology-and-lifecycle","title":"Terminology and Lifecycle","text":"<p>Damavand simplifies the development of an Extract-Transform-Load (ETL) pipeline for a rich collection of benchmark rotary machines dataset; followings are the essential steps of such pipelines:</p> <ol> <li>Downloading the dataset: datasets are downloadable using either custom or general downloaders.</li> <li>Mining the dataset: digestors transform raw dataset files into structured pairs of signals and metadata (mining for short).</li> <li>Application of signal processing: signal processing tehcniques are employed to process and enrich the signal banks, for the downstream analysis.</li> </ol> <p>The image below, illustrates the lifecycle of a Damavand pipeline. As highlighted in the image, employment of Damavand makes the development of ETL pipelines highly repeatable, resulting in faster iterative trials.</p> <p></p> <p>It is worth mentioning that we do not regard data augmentation as an essential step of the pipeline; therefore, we have not included that in this section; complementary explanations on data augmentation using Damavand is provided late on this page.</p>"},{"location":"advanced/#anatomy","title":"Anatomy","text":"<p>Damavand currently consists of four modules:</p> <ul> <li>utils: a submodule to include genral and basic functions</li> <li>signal_processing: implementation of the most frequently-used signal processing transforms and features used for vibration analysis</li> <li>datasets: this submodule consists of two parts:<ul> <li>downloaders: helping classes to download benchmark datasets</li> <li>digestors: helping classes to process raw dataset files into structured pairs of signal banks and their corresponding metadata</li> </ul> </li> <li>augmentations: implementation of a collection of data augmentation techniques, suiting vibration data</li> </ul> <p>The image below, illustrates an overview of the Damavand.</p> <p></p>"},{"location":"advanced/#datasets","title":"Datasets","text":"<p>Available datasets are listed in the table below:</p> Dataset \\(F_s\\) (kHz) Rotational Speed Multiple Loads (Loading pattern) Classes Available Channels Source MFPT 97.656  and 48.828 25 Hz Yes (Running load) Normal BIR BOR 1 Accelerometer https://www.mfpt.org/fault-data-sets/ KAIST 25.6 680 RPM to 2460 RPM Yes (running torque: 0 Nm, 2 Nm and 4 Nm) Normal BIR BOR M U 4 Accelerometers (vertical and horizontal per each bearing housing) https://data.mendeley.com/datasets/ztmf3m7h5x/6 CWRU 12 and 48 1730 RPM 1750 RPM 1772 RPM 1790 RPM Yes (rotational speed variation) Normal BIR BOR BBP 2 Accelerometers (one for drive-end bearing and one for the fan-end one) https://engineering.case.edu/bearingdatacenter SEU 2 20 Hz 30 Hz Yes (rotational speed variation) Normal BIR BOR BIO BBP 8 Accelerometers https://ieeexplore.ieee.org/abstract/document/8432110 https://github.com/cathysiyu/Mechanical-datasets/tree/master/gearbox MaFaulda 51.2 Variable (tachometer) Yes (rotational speed variation) Normal M (vertical/horizontal) U UHB (OR, CP \\&amp; BP) OHB (OR, CP \\&amp; BP) 1 tachometer Triaxial acceleration from underhang bearing Triaxial acceleration from overhang bearing Microphone https://www02.smt.ufrj.br/~offshore/mfs/page_01.html MEUT 10 Variable Yes (running power: 100, 200 &amp; 300 Watts) Normal (with &amp; without pulley) BIR BOR Triaxial acceleration https://data.mendeley.com/datasets/fm6xzxnf36/2 UoO 200 Variable Yes (variation of rotational speed: increasing, decreasing increasing-decreasing decreasing-increasing) Normal BIR BOR 1 Accelerometer 1 Encoder (measuring rotational speed) https://data.mendeley.com/datasets/v43hmbwxpm/1 PU 64 Variable Yes (rotational speed load torque radial force ) Normal Bearing inner race Bearing outer race Bearing inner/outer race 1 Accelerometer 2 Current sensors (measuring phase currents) https://mb.uni-paderborn.de/kat/forschung/kat-datacenter/bearing-datacenter/data-sets-and-download <p>In the above table, \\(F_s\\), BIR, BOR, M, U, BBP, BIO, UHB, OHB and BCP correspond to the sampling frequency, bearing inner race fault, bearing outer race fault, misalignment, unbalance, bearing ball problem, combinatory inner and outer races fault, underhang bearing, overhang bearing and bearing cage problem.</p>"},{"location":"augmentations/","title":"Damavand Documention - Augmentations Module API Reference","text":""},{"location":"augmentations/#gaussian_noisesignals-snr_level-return_noise-false","title":"<code>gaussian_noise(signals, SNR_level, return_noise = False)</code>","text":""},{"location":"augmentations/#augmenting-signals-with-a-signal-to-noise-ratio-level-through-adding-gaussian-noise","title":"Augmenting signals with a Signal-to-Noise Ratio level, through adding Gaussian noise","text":""},{"location":"augmentations/#arguments","title":"Arguments:","text":"<ul> <li>signals: A <code>pandas.DataFrame</code> incuding signals in its rows.</li> <li>SNR_level: the desired SNR level (in dB) of the augmented signals.</li> <li>return_noise: a flag to also return the pure noises; default is <code>False</code> and you need to set it <code>True</code> to also return the noises.</li> </ul>"},{"location":"augmentations/#return-values","title":"Return Values:","text":"<ul> <li>A <code>pandas.DataFrame</code> including noise contaminated signals.</li> <li>A <code>pandas.DataFrame</code> including the pure noises (only returned when <code>return_noise</code> is set to <code>True</code>).</li> </ul>"},{"location":"augmentations/#descriptions","title":"Descriptions:","text":"<p>Using this function, one is able to contaminate the original signals with zero-mean Gaussian noises (white noises), through summing each signal with a noise interfere. Noise level is determined using the <code>SNR_level</code>. For each row of <code>signals</code> a noise interfere with unique power is generated and once all noises are generated, original signals are summed with their corresponding interferes to result in contaminated signals. Noise power for each observation is calculated as below:</p> \\[ SNR_{dB} = 10 \\log_{10} \\left( \\frac{P_{signal}}{P_{noise}} \\right) \\] \\[ P_x = \\frac{1}{N} \\sum_{n=1}^{N} [x(n)]^2 \\] \\[ \\text{var}(x) = E\\left( [x - \\mu]^2 \\right) \\quad \\text{(where } x \\text{ is white noise, } \\mu \\text{ would be } 0\\text{)} \\] \\[ \\text{var}(x) = E(x^2) = \\frac{1}{N} \\sum_{n=1}^{N} [x(n)]^2 \\] \\[ \\text{var}(x) = \\frac{1}{N} \\sum_{n=1}^{N} [x(n)]^2 = P_x \\]"},{"location":"augmentations/#usage-example","title":"Usage example:","text":"<pre><code># Importings\nfrom damavand.damavand.datasets.downloaders import read_addresses, ZipDatasetDownloader\nfrom damavand.damavand.datasets.digestors import MFPT\nfrom damavand.damavand.augmentations import gaussian_noise\n\n# Downloading the MFPT dataset\naddresses = read_addresses()\ndownloader = ZipDatasetDownloader(addresses['MFPT'])\ndownloader.download_extract('MFPT.zip', 'MFPT/')\n\nmfpt = MFPT('MFPT/MFPT Fault Data Sets/', [\n\u00a0 \u00a0 '1 - Three Baseline Conditions',\n\u00a0 \u00a0 '2 - Three Outer Race Fault Conditions',\n\u00a0 \u00a0 '3 - Seven More Outer Race Fault Conditions',\n\u00a0 \u00a0 '4 - Seven Inner Race Fault Conditions',\n])\n\n# Mining the dataset\nmining_params = {\n\u00a0 \u00a0 97656: {'win_len': 16671, 'hop_len': 2000},\n\u00a0 \u00a0 48828: {'win_len': 8337, 'hop_len': 1000},\n}\nmfpt.mine(mining_params)\n\n# Signal/Metadata split\ndf = pd.concat(mfpt.data[48828]).reset_index(drop = True)\nsignals, metadata = df.iloc[:, : - 4], df.iloc[:, - 4 :]\n\n# Augmenting the dataset with 20 dB noise\naugmented_signals = gaussian_noise(signals, 20)\n\n# Augmenting the dataset with 20 dB noise and returing also the pure noises\naugmented_signals, noises = gaussian_noise(signals, 20, return_noise = True)\n</code></pre>"},{"location":"augmentations/#masking_noisesignals-ratio-uniformity-false-return_mask-false","title":"<code>masking_noise(signals, ratio, uniformity = False, return_mask = False)</code>","text":""},{"location":"augmentations/#augmenting-signals-with-binary-random-masks","title":"Augmenting signals with binary random masks","text":""},{"location":"augmentations/#arguments_1","title":"Arguments:","text":"<ul> <li>signals: A <code>pandas.DataFrame</code> incuding signals in its rows.</li> <li>ratio: The ratio of masks to be set to zeros.</li> <li>uniformity: Is a flag to get identical binary masks for all observations; default is <code>False</code> (meaning that masks unidentical masks) and you need to set it <code>True</code> to zero-out observations, uniformly.</li> <li>return_mask: Is a flag to return masks too; its default value is <code>False</code> and you need to set it to <code>True</code> to return masks.</li> </ul>"},{"location":"augmentations/#return-values_1","title":"Return Values:","text":"<ul> <li>A <code>pandas.DataFrame</code> including the masked out observations.</li> <li>A <code>pandas.DataFrame</code> (or <code>np.array</code> if <code>uniformity</code> is set <code>True</code>) including the masks (only returned when <code>return_noise</code> is set to <code>True</code>).</li> </ul>"},{"location":"augmentations/#descriptions_1","title":"Descriptions:","text":"<p>Using this function, one is able to mask out original signals using binary masks.</p>"},{"location":"augmentations/#usage-example_1","title":"Usage example:","text":"<pre><code># Importings\nfrom damavand.damavand.datasets.downloaders import read_addresses, ZipDatasetDownloader\nfrom damavand.damavand.datasets.digestors import MFPT\nfrom damavand.damavand.augmentations import masking_noise\n\n# Downloading the MFPT dataset\naddresses = read_addresses()\ndownloader = ZipDatasetDownloader(addresses['MFPT'])\ndownloader.download_extract('MFPT.zip', 'MFPT/')\n\nmfpt = MFPT('MFPT/MFPT Fault Data Sets/', [\n\u00a0 \u00a0 '1 - Three Baseline Conditions',\n\u00a0 \u00a0 '2 - Three Outer Race Fault Conditions',\n\u00a0 \u00a0 '3 - Seven More Outer Race Fault Conditions',\n\u00a0 \u00a0 '4 - Seven Inner Race Fault Conditions',\n])\n\n# Mining the dataset\nmining_params = {\n\u00a0 \u00a0 97656: {'win_len': 16671, 'hop_len': 2000},\n\u00a0 \u00a0 48828: {'win_len': 8337, 'hop_len': 1000},\n}\nmfpt.mine(mining_params)\n\n# Signal/Metadata split\ndf = pd.concat(mfpt.data[48828]).reset_index(drop = True)\nsignals, metadata = df.iloc[:, : - 4], df.iloc[:, - 4 :]\n\n# Augmenting the dataset with a ratio of 0.2 to zero out\naugmented_signals = masking_noise(signals, 0.2)\n\n# Augmenting the dataset with a ratio of 0.2 to zero out and returing also the masks\naugmented_signals, noises = masking_noise(signals, 0.2, uniformity = False, return_mask = True)\n</code></pre>"},{"location":"augmentations/#amplitude_shiftingsignals-coefficients","title":"<code>amplitude_shifting(signals, coefficients)</code>","text":""},{"location":"augmentations/#augmenting-signals-by-amplitude-shifting","title":"Augmenting signals by amplitude shifting","text":""},{"location":"augmentations/#arguments_2","title":"Arguments:","text":"<ul> <li>signals: A <code>pandas.DataFrame</code> incuding signals in its rows.</li> <li>coefficients: The coefficients to multiply signals by; to multiply all signals with an identical coefficient, pass a <code>float</code> or <code>int</code>. Alternatively, pass a <code>list</code> with a length identical to the numbers of rows in <code>signals</code> to multiply each row with an identical coefficient.</li> </ul>"},{"location":"augmentations/#return-values_2","title":"Return Values:","text":"<ul> <li>A <code>pandas.DataFrame</code> including the scaled observations.</li> </ul>"},{"location":"augmentations/#descriptions_2","title":"Descriptions:","text":"<p>By this function, one can augment signals by scaling their amplitudes using a scaler. Scalers can be identical for all signals (if the <code>coefficients</code> is either a <code>float</code> or an <code>int</code>) or unique for each row of signal by passing a <code>list</code>.</p>"},{"location":"augmentations/#usage-example_2","title":"Usage example:","text":"<pre><code># Importings\nfrom damavand.damavand.datasets.downloaders import read_addresses, ZipDatasetDownloader\nfrom damavand.damavand.datasets.digestors import MFPT\nfrom damavand.damavand.augmentations import masking_noise\n\n# Downloading the MFPT dataset\naddresses = read_addresses()\ndownloader = ZipDatasetDownloader(addresses['MFPT'])\ndownloader.download_extract('MFPT.zip', 'MFPT/')\n\nmfpt = MFPT('MFPT/MFPT Fault Data Sets/', [\n\u00a0 \u00a0 '1 - Three Baseline Conditions',\n\u00a0 \u00a0 '2 - Three Outer Race Fault Conditions',\n\u00a0 \u00a0 '3 - Seven More Outer Race Fault Conditions',\n\u00a0 \u00a0 '4 - Seven Inner Race Fault Conditions',\n])\n\n# Mining the dataset\nmining_params = {\n\u00a0 \u00a0 97656: {'win_len': 16671, 'hop_len': 2000},\n\u00a0 \u00a0 48828: {'win_len': 8337, 'hop_len': 1000},\n}\nmfpt.mine(mining_params)\n\n# Signal/Metadata split\ndf = pd.concat(mfpt.data[48828]).reset_index(drop = True)\nsignals, metadata = df.iloc[:, : - 4], df.iloc[:, - 4 :]\n\n# Augmenting the dataset with by scaling all the signals by 5\naugmented_signals = amplitude_shifting(signals, 5)\n\n# Augmenting the first 5 signals by 5 different coefficients\naugmented_signals = amplitude_shifting(signals.iloc[:5, :], [1, 2, 3, 4, 5])\n</code></pre>"},{"location":"augmentations/#resamplingsignals-target_len","title":"<code>resampling(signals, target_len)</code>","text":""},{"location":"augmentations/#resample-signals-to-a-target-length","title":"Resample signals to a target length","text":""},{"location":"augmentations/#arguments_3","title":"Arguments:","text":"<ul> <li>signals: A <code>pandas.DataFrame</code> incuding signals in its rows.</li> <li>target_len: The target length of the resampled signals, as an <code>int</code>.</li> </ul>"},{"location":"augmentations/#return-values_3","title":"Return Values:","text":"<ul> <li>A <code>pandas.DataFrame</code> including the resampled signals.</li> </ul>"},{"location":"augmentations/#descriptions_3","title":"Descriptions:","text":"<p>By this function, one can resample signals to a desired (lower or higher than the original) length.</p>"},{"location":"augmentations/#usage-example_3","title":"Usage example:","text":"<pre><code># Importings\nfrom damavand.damavand.datasets.downloaders import read_addresses, ZipDatasetDownloader\nfrom damavand.damavand.datasets.digestors import MFPT\nfrom damavand.damavand.augmentations import resampling\n\n# Downloading the MFPT dataset\naddresses = read_addresses()\ndownloader = ZipDatasetDownloader(addresses['MFPT'])\ndownloader.download_extract('MFPT.zip', 'MFPT/')\n\nmfpt = MFPT('MFPT/MFPT Fault Data Sets/', [\n\u00a0 \u00a0 '1 - Three Baseline Conditions',\n\u00a0 \u00a0 '2 - Three Outer Race Fault Conditions',\n\u00a0 \u00a0 '3 - Seven More Outer Race Fault Conditions',\n\u00a0 \u00a0 '4 - Seven Inner Race Fault Conditions',\n])\n\n# Mining the dataset\nmining_params = {\n\u00a0 \u00a0 97656: {'win_len': 16671, 'hop_len': 2000},\n\u00a0 \u00a0 48828: {'win_len': 8337, 'hop_len': 1000},\n}\nmfpt.mine(mining_params)\n\n# Signal/Metadata split\ndf = pd.concat(mfpt.data[48828]).reset_index(drop = True)\nsignals, metadata = df.iloc[:, : - 4], df.iloc[:, - 4 :]\n\n# Augmenting the dataset by resampling the original signals (8337 points) to 10000\nsignals_upsampled = resampling(signals, 10000)\n</code></pre>"},{"location":"datasets/","title":"Damavand Documention - Datasets Module API Reference","text":"<p>Data is the essential ingridient for any data-driven analysis, including rotating machinery intelligent condition monitoring. On this page, we go through both submodules of the datasets module: 1)Downloaders and 1)Digestors. While the former is focused around downloading the raw dataset from internet, the latter is developed around structurizing the raw data.</p>"},{"location":"datasets/#downloaders-submodule","title":"Downloaders Submodule","text":""},{"location":"datasets/#read_addresses","title":"<code>read_addresses</code>","text":""},{"location":"datasets/#loading-the-addresses-of-datasets-to-download-them","title":"Loading the addresses of datasets to download them","text":""},{"location":"datasets/#arguments","title":"Arguments:","text":"<ul> <li>This function recieves no argument</li> </ul>"},{"location":"datasets/#return-value","title":"Return Value:","text":"<ul> <li>A <code>dict</code> object whose keys are dataset names and values are download addresses.</li> </ul>"},{"location":"datasets/#description","title":"Description:","text":"<p>Using this function, one is able to load the download addresses of the available datasets, as a python dictionary. For single-file datasets, the value corresponding to the dataset name key, is the download link. For datasets consisting of various files, value corresponding to the dataset name will be another python dictionary whose keys are file names and corresponding values are download links.</p>"},{"location":"datasets/#usage-example","title":"Usage example:","text":"<pre><code># Importing\nfrom damavand.damavand.datasets.downloaders import read_addresses\n\n# Loading the addresses\naddresses = read_addresses()\n</code></pre>"},{"location":"datasets/#zipdatasetdownloader","title":"<code>ZipDatasetDownloader</code>","text":""},{"location":"datasets/#downloading-datasets-stored-in-single-zip-files","title":"Downloading datasets stored in single Zip files","text":""},{"location":"datasets/#description_1","title":"Description","text":"<p>Using this class, one is able to download and extract datasets that are available as single zip files (e.g., SEU).</p>"},{"location":"datasets/#instantiation-zipdatasetdownloaderurl","title":"Instantiation: <code>ZipDatasetDownloader(url)</code>","text":"<ul> <li><code>url</code> is the download link.</li> </ul>"},{"location":"datasets/#downloading-zipdatasetdownloaderdownloaddownload_file","title":"Downloading: <code>ZipDatasetDownloader.download(download_file)</code>","text":"<ul> <li><code>download_file</code> is the directory in which the zip file is downloaded.  </li> <li>This value is stored in <code>ZipDatasetDownloader.download_file</code> once <code>ZipDatasetDownloader.download()</code> is called.</li> </ul>"},{"location":"datasets/#extraction-zipdatasetdownloaderextractextraction_path","title":"Extraction: <code>ZipDatasetDownloader.extract(extraction_path)</code>","text":"<ul> <li><code>extraction_path</code> is the directory where the zip file is extracted.  </li> <li>This value is stored in <code>ZipDatasetDownloader.extraction_path</code> once <code>ZipDatasetDownloader.extract()</code> is called.</li> </ul>"},{"location":"datasets/#merging-downloading-and-extraction-steps-zipdatasetdownloaderdownload_extractdownload_path-extraction_path","title":"Merging downloading and extraction steps: <code>ZipDatasetDownloader.download_extract(download_path, extraction_path)</code>","text":"<ul> <li><code>download_file</code> is the directory in which the zip file is downloaded.     This value is stored in <code>ZipDatasetDownloader.download_file</code> once <code>ZipDatasetDownloader.download()</code> is called.  </li> <li><code>extraction_path</code> is the directory where the zip file is extracted.     This value is stored in <code>ZipDatasetDownloader.extraction_path</code> once <code>ZipDatasetDownloader.download_extract()</code> is called.</li> </ul>"},{"location":"datasets/#usage-example_1","title":"Usage example:","text":"<pre><code># Importing\nfrom damavand.damavand.datasets.downloaders import read_addresses, ZipDatasetDownloader\n\n# Loading the addresses\naddresses = read_addresses()\n\n# Downloading the dataset\ndownloader = zipDatasetDownloader(addresses['SEU'])\ndownloader.download_extract('SEU.zip', 'SEU/)\n</code></pre>"},{"location":"datasets/#cwrudownloader","title":"<code>CwruDownloader</code>","text":""},{"location":"datasets/#a-custom-downloader-for-cwru-dataset","title":"A custom downloader for CWRU dataset.","text":""},{"location":"datasets/#description_2","title":"Description:","text":""},{"location":"datasets/#instantiation-cwrudownloaderfiles","title":"Instantiation: <code>CwruDownloader(files)</code>","text":"<ul> <li><code>files</code> is a python <code>dictionary</code> whose keys are file names and corresponding values are the download links.</li> </ul>"},{"location":"datasets/#downloading-cwrudownloaderdownloaddownload_path-chunk_size-512-delay-1","title":"Downloading: <code>CwruDownloader.download(download_path, chunk_size = 512, delay = 1)</code>","text":"<ul> <li><code>download_path</code> is the directory where desired files are downloaded to. This value is stored in <code>CwruDownloader.download_path</code>, once <code>CwruDownloader.download()</code> is called.</li> <li><code>chunk_size</code> to avoid corrupted downloading, responses are read in chunks; this variable controls the size of the chunks. Default value is 512\u00a0 Bytes.</li> <li><code>delay</code> to avoid server overload, it is better to place a small delay between requesting consecutive files. This argument controls the dealy time interval, in the unit of seconds. The default value is 1 second.</li> </ul>"},{"location":"datasets/#redownloading-errored-downlaods-cwrudownloaderredownloadchunk_size-512-delay-1","title":"Redownloading errored downlaods: <code>CwruDownloader.redownload(chunk_size = 512, delay = 1)</code>","text":"<ul> <li><code>chunk_size</code> to avoid corrupted downloading, responses are read in chunks; this variable controls the size of the chunks. Default value is 512 Bytes.</li> <li><code>delay</code> to avoid server overload, it is better to place a small delay between requesting consecutive files. This argument controls the dealy time interval, in the unit of seconds. The default value is 1 second.</li> </ul>"},{"location":"datasets/#undownloaded-files-cwrudownloaderundownloaded","title":"Undownloaded files: <code>CwruDownloader.undownloaded</code>","text":"<p>If a file is not downloaded properly - either during the <code>CwruDownloader.download()</code> or <code>CwruDownloader.redownload()</code> - it is added to <code>CwruDownloader.undownloaded</code> as a pair of key (file name) and value (corresponding error). This can be later used to complete the downloading process.</p>"},{"location":"datasets/#usage-example_2","title":"Usage example:","text":"<pre><code># Importings\nfrom damavand.damavand.datasets.downloaders import read_addresses, CwruDownloader\n\n# Loading the addresses\naddresses = read_addresses()\n\n# Downloading the addresses\ndownloader = CwruDownloader(addresses['CWRU'])\ndownloader.download('CWRU/')\nwhile len(list(downloader.undownloaded.keys())) &gt; 0:\n\u00a0 \u00a0 downloader.redownload()\n</code></pre>"},{"location":"datasets/#pudownloader","title":"<code>PuDownloader</code>","text":""},{"location":"datasets/#a-custom-downloader-for-pu-dataset","title":"A custom downloader for PU dataset.","text":""},{"location":"datasets/#description_3","title":"Description:","text":""},{"location":"datasets/#instantiation-pudownloaderfiles","title":"Instantiation: <code>PuDownloader(files)</code>","text":"<ul> <li><code>files</code> is a python <code>dictionary</code> whose keys are file names and corresponding values are the download links.</li> </ul>"},{"location":"datasets/#downloading-pudownloaderdownloaddownload_path-timeout-10","title":"Downloading: <code>PuDownloader.download(download_path, timeout = 10)</code>","text":"<ul> <li><code>download_path</code> is the directory where rar files are donwloaded to. This value is stored in <code>PuDownloader.download_path</code>, once <code>PuDownloader.download()</code> is called.</li> <li><code>timeout</code> is the number of seconds that downloader waits to download a file.</li> </ul>"},{"location":"datasets/#extracting-pudownloaderextractextraction_path","title":"Extracting: <code>PuDownloader.extract(extraction_path)</code>","text":"<ul> <li><code>extraction_path</code> is the directory that rar files are extracted to. This value is stored in <code>PuDownloader.extraction_path</code>, once <code>PuDownloader.extract()</code> is called.</li> </ul>"},{"location":"datasets/#merging-downloading-and-extraction-steps-pudownloaderdownload_extractdownload_path-extraction_path","title":"Merging downloading and extraction steps: <code>PuDownloader.download_extract(download_path, extraction_path)</code>","text":"<ul> <li><code>download_path</code> is the directory in which the rar files are downloaded to. This value is stored in <code>PuDownloader.download_path</code>, once <code>PuDownloader.download_extract()</code> is called.</li> <li><code>extraction_path</code> is the directory that rar files are extracted to. This value is stored in <code>PuDownloader.extraction_path</code>, once <code>PuDownloader.download_extract()</code> is called.</li> </ul>"},{"location":"datasets/#usage-example_3","title":"Usage example:","text":"<pre><code># Importings\nfrom damavand.damavand.datasets.downloaders import read_addresses, PuDownloader\n\n# Loading the addresses\naddresses = read_addresses()\n\n# Downloading the dataset\ndownloader = PuDownloader(addresses['PU'])\nPuDownloader(addresses['PU']).download_extract('PU_rarfiles', 'PU/')\n</code></pre>"},{"location":"datasets/#mafauldadownloader","title":"<code>MaFaulDaDownloader</code>","text":""},{"location":"datasets/#a-custom-downloader-for-mafaulda-dataset","title":"A custom downloader for MaFaulDa dataset.","text":""},{"location":"datasets/#description_4","title":"Description:","text":""},{"location":"datasets/#instantiation-mafauldadownloaderfiles","title":"Instantiation: <code>MaFaulDaDownloader(files)</code>","text":"<ul> <li><code>files</code> is a python <code>dictionary</code> whose keys are file names and corresponding values are the download links.</li> </ul>"},{"location":"datasets/#downloading-mafauldadownloaderdownloaddownload_path","title":"Downloading: <code>MaFaulDaDownloader.download(download_path)</code>","text":"<ul> <li><code>download_path</code> is the directory where desired files are donwloaded to. This value is stored in <code>MaFaulDaDownloader.download_path</code>, once <code>MaFaulDaDownloader.download()</code> is called.</li> </ul>"},{"location":"datasets/#extracting-mafauldadownloaderextractextraction_path","title":"Extracting: <code>MaFaulDaDownloader.extract(extraction_path)</code>","text":"<ul> <li><code>extraction_path</code> is the directory that zip files are extracted to. This value is stored in <code>MaFaulDaDownloader.extraction_path</code>, once <code>MaFaulDaDownloader.extract()</code> is called.</li> </ul>"},{"location":"datasets/#merging-downloading-and-extraction-steps-mafauldadownloaderdownload_extractdownload_path-extraction_path","title":"Merging downloading and extraction steps: <code>MaFaulDaDownloader.download_extract(download_path, extraction_path)</code>","text":"<ul> <li><code>download_path</code> is the directory in which the zip file is downloaded to. This value is stored in <code>MaFaulDaDownloader.download_path</code>, once <code>MaFaulDaDownloader.download_extract()</code> is called.</li> <li><code>extraction_path</code> is the directory that zip file is extracted to. This value is stored in <code>MaFaulDaDownloader.extraction_path</code>, once <code>MaFaulDaDownloader.download_extract()</code> is called.</li> </ul>"},{"location":"datasets/#usage-example_4","title":"Usage example:","text":"<pre><code># Importings\nfrom damavand.damavand.datasets.downloaders import read_addresses, MaFaulDaDownloader\n\n# Loading the addresses\naddresses = read_addresses()\n# Downloading the dataset\ndownloader = MaFaulDaDownloader(addresses['MaFaulDa'])\nPuDownloader(addresses['MaFaulDa']).download_extract('MaFaulDa_zipfiles', 'MaFaulDa/')\n</code></pre>"},{"location":"datasets/#digestors-submodule","title":"Digestors Submodule","text":""},{"location":"datasets/#kaist","title":"<code>KAIST</code>","text":""},{"location":"datasets/#a-digestor-to-mine-the-dataset-by-korea-advanced-institute-of-science-and-technology-kaist","title":"A digestor to mine the dataset by Korea Advanced Institute of Science and Technology (KAIST)","text":""},{"location":"datasets/#original-title-vibration-acoustic-temperature-and-motor-current-dataset-of-rotating-machine-under-varying-operating-conditions-for-fault-diagnosis","title":"Original title: Vibration, Acoustic, Temperature, and Motor Current Dataset of Rotating Machine Under Varying Operating Conditions for Fault Diagnosis","text":""},{"location":"datasets/#external-resources","title":"External resources:","text":"<ul> <li>https://data.mendeley.com/datasets/ztmf3m7h5x/6</li> <li>https://www.sciencedirect.com/science/article/pii/S2352340923001671</li> </ul>"},{"location":"datasets/#description_5","title":"Description:","text":""},{"location":"datasets/#instantiation-kaistbase_directory-files-channels","title":"Instantiation: <code>KAIST(base_directory, files, channels)</code>","text":"<ul> <li><code>base_directory</code> is the home directory of the extracted files.</li> <li><code>files</code> is the list of files of interest; to include all files, use <code>os.listdir(base_directory)</code></li> <li><code>channels</code>is the ist of channels to include; 0, 1, 2 and 3 correspond to x direction - housing A, y direction - housing A, x direction - housing B and y direction - housing B, respectively. Default value is [0, 1, 2, 3].</li> </ul>"},{"location":"datasets/#mining-kaistminemining_params","title":"Mining: <code>KAIST.mine(mining_params)</code>","text":"<ul> <li><code>mining_params</code> is a python dictonary whose keys are <code>win_len</code> and <code>hop_len</code> with their correponding values.</li> </ul>"},{"location":"datasets/#accessing-data-kaistdata","title":"Accessing data: <code>KAIST.data</code>","text":"<p>Mined data is presented as a python dictonary whose keys correspond to the <code>channels</code> and values are list of <code>pd.DataFrame</code> objects.</p>"},{"location":"datasets/#usage-example_5","title":"Usage example:","text":"<pre><code># Importings\nfrom damavand.damavand.datasets.downloaders import read_addresses, ZipDatasetDownloader\nfrom damavand.damavand.datasets.digestors import KAIST\n\n# Downloading the dataset\naddresses = read_addresses()\ndownloader = ZipDatasetDownloader(addresses['KAIST'])\ndownloader.download_extract('KAIST.zip', 'KAIST/')\n\n# Mining the dataset (using only two channels out of four)\nkaist = KAIST('KAIST/', os.listdir('KAIST/'), list(range(2)))\nmining_params = {\n\u00a0 \u00a0 'win_len': 20000,\n\u00a0 \u00a0 'hop_len': 20000,\n}\nkaist.mine(mining_params)\n</code></pre>"},{"location":"datasets/#mfpt","title":"<code>MFPT</code>","text":""},{"location":"datasets/#a-digestor-to-mine-the-dataset-by-society-for-machinery-failure-prevention-technology-mfpt","title":"A digestor to mine the dataset by Society for Machinery Failure Prevention Technology (MFPT)","text":""},{"location":"datasets/#original-title-condition-based-maintenance-fault-database-for-testing-of-diagnostic-and-prognostics-algorithms-bearing-fault-dataset","title":"Original title: Condition Based Maintenance Fault Database for Testing of Diagnostic and Prognostics Algorithms - Bearing Fault Dataset","text":""},{"location":"datasets/#external-resources_1","title":"External resources:","text":"<ul> <li>https://www.mfpt.org/fault-data-sets/</li> <li>https://mfpt.org/wp-content/uploads/2018/03/MFPT-Bearing-Envelope-Analysis.pdf</li> </ul>"},{"location":"datasets/#description_6","title":"Description:","text":""},{"location":"datasets/#instantiation-mfptbase_directory-folders","title":"Instantiation: <code>MFPT(base_directory, folders)</code>","text":"<ul> <li><code>base_directory</code> is the home directory of the extracted file.</li> <li><code>folders</code> is the list of folders to include; valid elements are 1 - Three Baseline Conditions, 2 - Three Outer Race Fault Conditions, 3 - Seven More Outer Race Fault Conditions and 4 - Seven Inner Race Fault Conditions.</li> </ul>"},{"location":"datasets/#mining-mfptminemining_params","title":"Mining: <code>MFPT.mine(mining_params)</code>","text":"<ul> <li><code>mining_params</code> is a nested python dictonary whose keys are 97656 and 48828 (sampling frequencies the dataset is collected by) and the corresponding values are objects of python dictonary. Secondary dictonaries each have two keys: <code>win_len</code> and <code>hop_len</code> with correponding values.</li> </ul>"},{"location":"datasets/#accessing-data-mfptdata","title":"Accessing data: <code>MFPT.data</code>","text":"<ul> <li>Mined data is presented as a python dictonary whose keys are 97656 and 48828. Corresponding values are lists of <code>pd.DataFrame</code> objects, belonging to the data files recorded to the corresponding sampling frequency.</li> </ul>"},{"location":"datasets/#usage-example_6","title":"Usage example:","text":"<pre><code># Importings\nfrom damavand.damavand.datasets.downloaders import read_addresses, ZipDatasetDownloader\nfrom damavand.damavand.datasets.digestors import MFPT\n\n# Downloading the dataset\naddresses = read_addresses()\ndownloader = ZipDatasetDownloader(addresses['MFPT'])\ndownloader.download_extract('MFPT.zip', 'MFPT/')\n\n# Mining the dataset\nmfpt = MFPT('MFPT/MFPT Fault Data Sets/', [\n\u00a0 \u00a0 '1 - Three Baseline Conditions',\n\u00a0 \u00a0 '2 - Three Outer Race Fault Conditions',\n\u00a0 \u00a0 '3 - Seven More Outer Race Fault Conditions',\n\u00a0 \u00a0 '4 - Seven Inner Race Fault Conditions',\n])\nmining_params = {\n\u00a0 \u00a0 97656: {'win_len': 16671, 'hop_len': 2000},\n\u00a0 \u00a0 48828: {'win_len': 8337, 'hop_len': 1000},\n}\nmfpt.mine(mining_params)\n</code></pre>"},{"location":"datasets/#cwru","title":"<code>CWRU</code>","text":""},{"location":"datasets/#a-digestor-to-mine-the-bearing-dataset-by-case-western-reserve-university-cwru","title":"A digestor to mine the bearing dataset by Case Western Reserve University (CWRU)","text":""},{"location":"datasets/#external-resource","title":"External resource:","text":"<ul> <li>https://engineering.case.edu/bearingdatacenter</li> </ul>"},{"location":"datasets/#description_7","title":"Description:","text":""},{"location":"datasets/#instantiation-cwrubase_directory-channels","title":"Instantiation: <code>CWRU(base_directory, channels)</code>","text":"<ul> <li><code>base_directory</code> is the home directory of the downloaded files.</li> <li><code>channels</code> is a list of strings to include the desired measurement channels; 'FE' and 'DE' corresponding to fan-end acceleration, drive-end acceleration and base acceleration respectively, are available choices. Default value is ['FE', 'DE'].</li> </ul>"},{"location":"datasets/#mining-cwruminemining_params-synchronous_only","title":"Mining: <code>CWRU.mine(mining_params, synchronous_only)</code>","text":"<ul> <li><code>mining_params</code> is a nested python dictonary whose keys are '12K' and '48K' (sampling frequencies used to collect the dataset) and values are again python dictionaries whose keys are <code>win_len</code> and <code>hop_len</code>.</li> <li><code>synchronous_only</code> is a boolean variable to be used as a flag; once this flag is set <code>True</code>, only files which contain all the desired channels are mined and ones missing one of the channels are skipped. Default value is <code>False</code>.</li> </ul>"},{"location":"datasets/#accessing-data-cwrudata","title":"Accessing data: <code>CWRU.data</code>","text":"<ul> <li>Mined data is organized as a nested python dictonary whose keys are elements of the <code>channels</code>; corresponding values again python dictionaries whose keys are the sampling frequencies; '12K' and '48K'.</li> </ul>"},{"location":"datasets/#usage-example_7","title":"Usage example:","text":"<pre><code># Importings\nfrom damavand.damavand.datasets.downloaders import read_addresses, CwruDownloader\nfrom damavand.damavand.datasets.digestors import CWRU\n\n# Downloading the dataset\naddresses = read_addresses()\ndownloader = CwruDownloader(addresses['CWRU'])\ndownloader.download('CWRU/')\nwhile len(list(downloader.undownloaded.keys())) &gt; 0:\n\u00a0 downloader.redownload()\n\n# Mining the dataset\nmining_params = {\n\u00a0 \u00a0 '12K': {'win_len': 12000, 'hop_len': 3000},\n\u00a0 \u00a0 '48K': {'win_len': 48000, 'hop_len': 16000},\n}\ncwru = CWRU('CWRU/')\ncwru.mine(mining_params, synchronous_only = True)\n</code></pre>"},{"location":"datasets/#seu","title":"<code>SEU</code>","text":""},{"location":"datasets/#a-digestor-to-mine-the-gearbox-dataset-from-southeast-university-seu","title":"A digestor to mine the gearbox dataset from Southeast University (SEU)","text":""},{"location":"datasets/#original-title-gearbox-dataset-from-highly-accurate-machine-fault-diagnosis-using-deep-transfer-learning","title":"Original title: Gearbox dataset from Highly Accurate Machine Fault Diagnosis Using Deep Transfer Learning","text":""},{"location":"datasets/#external-resources_2","title":"External resources:","text":"<ul> <li>https://ieeexplore.ieee.org/abstract/document/8432110</li> <li>https://github.com/cathysiyu/Mechanical-datasets/tree/master/gearbox</li> </ul>"},{"location":"datasets/#description_8","title":"Description","text":""},{"location":"datasets/#instantiation-seubase_directory-channels","title":"Instantiation: <code>SEU(base_directory, channels)</code>","text":"<ul> <li><code>base_directory</code> is the home directory of the downloaded files.</li> <li><code>channels</code> is a list of integers (from 0 to 7), corresponding to 8 accelerometers. Default value is [0, 1, 2, 3, 4, 5, 6, 7].</li> </ul>"},{"location":"datasets/#mining-seuminemining_params","title":"Mining: <code>SEU.mine(mining_params)</code>","text":"<ul> <li><code>mining_params</code> is a python dictonary whose keys are <code>win_len</code> and <code>hop_len</code>.\u00a0</li> </ul>"},{"location":"datasets/#accessing-data-seudata","title":"Accessing data: <code>SEU.data</code>","text":"<p>Mined data is organized as a python dictonary whose keys are elements of the <code>channels</code>; corresponding values are lists of <code>pd.DataFrame</code> objects.</p>"},{"location":"datasets/#usage-example_8","title":"Usage example:","text":"<pre><code># Importings\nfrom damavand.damavand.datasets.downloaders import read_addresses, ZipDatasetDownloader\nfrom damavand.damavand.datasets.digestors import SEU\n\n# Downloading the dataset\naddresses = read_addresses()\ndownloader = ZipDatasetDownloader(addresses['SEU'])\ndownloader.download_extract('SEU.zip', 'SEU/')\n\n# Mining the dataset\nmining_params = {'win_len': 10000, 'hop_len': 10000}\nseu = SEU('SEU/')\nseu.mine(mining_params)\n</code></pre>"},{"location":"datasets/#mafaulda","title":"<code>MaFaulda</code>","text":""},{"location":"datasets/#a-digestor-to-mine-the-machinery-fault-database-mafaulda","title":"A digestor to mine the Machinery Fault Database (MaFaulDa)","text":""},{"location":"datasets/#original-title-machinery-fault-database","title":"Original title: Machinery Fault Database","text":""},{"location":"datasets/#external-resources_3","title":"External resources:","text":"<p>- https://www02.smt.ufrj.br/~offshore/mfs/page_01.html</p>"},{"location":"datasets/#description_9","title":"Description:","text":""},{"location":"datasets/#instantiation-mafaulddabase_directory-folders-channels","title":"Instantiation: <code>MaFauldDa(base_directory, folders, channels)</code>","text":"<p>- <code>base_directory</code> is the home directory of the extracted folders. \u00a0 \u00a0 - <code>folders</code> is a list to include folders of interest, during the mining process. \u00a0 \u00a0 - <code>channels</code> is a list of integers (from 0 to 7), corresponding to the tachometer, 3 accelerometers on the underhang bearing (axial, radial and tangential), 3 accelerometers on the overhang bearing (axial, radial and tangential) and a microphone.</p>"},{"location":"datasets/#mining-mafauldaminemining_params","title":"Mining: <code>MaFaulda.mine(mining_params)</code>","text":"<p>- <code>mining_params</code> is a python dictonary whose keys are <code>win_len</code> and <code>hop_len</code>.\u00a0</p>"},{"location":"datasets/#accessing-data-mafauldadata","title":"Accessing data: <code>MaFaulda.data</code>","text":"<p>Mined data is organized as a python dictonary whose keys are elements of the <code>channels</code>; corresponding values are lists of <code>pd.DataFrame</code> objects.</p>"},{"location":"datasets/#usage-example_9","title":"Usage example:","text":"<pre><code># Importings\nfrom damavand.damavand.datasets.downloaders import read_addresses, MaFaulDaDownloader\nfrom damavand.damavand.datasets.digestors import MaFauldDa\n\n# Downloading the dataset\naddresses = read_addresses()\ndownloader = MaFaulDaDownloader({key: addresses['MaFaulDa'][key] for key in ['normal.zip', 'imbalance.zip']})\ndownloader.download_extract('mafaulda_zip_files/', 'mafaulda/')\n\n# Mining the dataset (using only the third channel)\nmafaulda = MaFauldDa('mafaulda/', os.listdir('mafaulda/'), channels = [2])\nmining_params = {\n\u00a0 \u00a0 'win_len': 50000,\n\u00a0 \u00a0 'hop_len': 50000\n}\nmafaulda.mine(mining_params)\n</code></pre>"},{"location":"datasets/#meut","title":"<code>MEUT</code>","text":""},{"location":"datasets/#a-digestor-to-mine-the-dataset-by-mehran-university-of-engineering-technology-meut","title":"A digestor to mine the dataset by Mehran University of Engineering &amp; Technology (MEUT)","text":""},{"location":"datasets/#original-title-triaxial-bearing-vibration-dataset-of-induction-motor-under-varying-load-conditions","title":"Original title: Triaxial bearing vibration dataset of induction motor under varying load conditions","text":""},{"location":"datasets/#external-resources_4","title":"External resources:","text":"<ul> <li>https://data.mendeley.com/datasets/fm6xzxnf36/2</li> <li>https://www.sciencedirect.com/science/article/pii/S2352340922005170</li> </ul>"},{"location":"datasets/#description_10","title":"Description:","text":""},{"location":"datasets/#instantiation-muetbase_directory-folders-channels","title":"Instantiation: <code>MUET(base_directory, folders, channels)</code>","text":"<ul> <li><code>base_directory</code> is the home directory of the extracted folders.</li> <li><code>folders</code> is the list of folders to include during the mining process.</li> <li><code>channels</code>is the list of integers, corresponding to the triaxial acceleration signals; 1, 2 and 3 correspond to X-axis, Y-axis, and Z-axis. The default value is [1, 2, 3].</li> </ul>"},{"location":"datasets/#mining-muetminemining_params","title":"Mining: <code>MUET.mine(mining_params)</code>","text":"<ul> <li><code>mining_params</code> is a python dictonary whose keys are <code>win_len</code> and <code>hop_len</code>.\u00a0</li> </ul>"},{"location":"datasets/#accessing-data-muetdata","title":"Accessing data: <code>MUET.data</code>","text":"<p>Mined data is organized as a python dictonary whose keys are elements of the <code>channels</code>; corresponding values are lists of <code>pd.DataFrame</code> objects.</p>"},{"location":"datasets/#usage-example_10","title":"Usage example:","text":"<pre><code># Importings\nfrom damavand.damavand.datasets.downloaders import read_addresses, ZipDatasetDownloader\nfrom damavand.damavand.datasets.digestors import MUET\n\n# Downloading the dataset\naddresses = read_addresses()\ndownloader = ZipDatasetDownloader(addresses['MEUT'])\ndownloader.download_extract('MEUT.zip', 'MEUT/')\n\n# Mining the dataset\ndataset = MUET('MEUT/fm6xzxnf36-2/', os.listdir('MEUT/fm6xzxnf36-2/'), [3])\nmining_params = {\n\u00a0 \u00a0 'win_len': 10000,\n\u00a0 \u00a0 'hop_len': 5000\n}\ndataset.mine(mining_params)\n</code></pre>"},{"location":"datasets/#uoo","title":"<code>UoO</code>","text":""},{"location":"datasets/#a-digestor-to-mine-the-bearing-dataset-from-university-of-ottawa-uoo","title":"A digestor to mine the bearing dataset from University of Ottawa (UoO)","text":""},{"location":"datasets/#original-title-bearing-vibration-data-collected-under-time-varying-rotational-speed-conditions","title":"Original title: Bearing vibration data collected under time-varying rotational speed conditions","text":""},{"location":"datasets/#external-resources_5","title":"External resources:","text":"<ul> <li>https://www.sciencedirect.com/science/article/pii/S2352340918314124</li> <li>https://data.mendeley.com/datasets/v43hmbwxpm/1</li> </ul>"},{"location":"datasets/#description_11","title":"Description:","text":""},{"location":"datasets/#instantiation-uoobase_directory-channels-reps","title":"Instantiation: <code>UoO(base_directory, channels, reps)</code>","text":"<ul> <li><code>base_directory</code> is the home directory of the extracted folders.</li> <li><code>channels</code> is a list of strings, specifying the desired channels; available choices are 'channel_1' and 'channel_2', corresponding to the acceleration and the rotational speed, respectively. The default value is ['channel_1', 'channel_2'].</li> <li><code>reps</code> is a list of integers that specifies the number of measurement repetitions (1, 2 and 3), to include. Default value is [1, 2, 3].</li> </ul>"},{"location":"datasets/#mining-uoominemining_params","title":"Mining: <code>UoO.mine(mining_params)</code>","text":"<ul> <li><code>mining_params</code> is a python dictonary whose keys are <code>win_len</code> and <code>hop_len</code>.\u00a0</li> </ul>"},{"location":"datasets/#accessing-data-uoodata","title":"Accessing data: <code>UoO.data</code>","text":"<p>Mined data is organized as a python dictonary whose keys are elements of the <code>channels</code>; corresponding values are lists of <code>pd.DataFrame</code> objects.</p>"},{"location":"datasets/#usage-example_11","title":"Usage example:","text":"<pre><code># Importings\nfrom damavand.damavand.datasets.downloaders import read_addresses, ZipDatasetDownloader\nfrom damavand.damavand.datasets.digestors import UoO\n\n# Downloading the dataset\naddresses = read_addresses()\ndownloader = ZipDatasetDownloader(addresses['UoO'])\ndownloader.download_extract('UoO.zip', 'UoO/')\n\n# Mining the dataset\ndataset = UoO('UoO/', ['Channel_1', 'Channel_2'], [1])\nmining_params = {'win_len': 10000, 'hop_len': 10000}\ndataset.mine(mining_params)\n</code></pre>"},{"location":"datasets/#pu","title":"<code>PU</code>","text":""},{"location":"datasets/#a-digestor-to-mine-the-bearing-dataset-by-paderborn-university-pu","title":"A digestor to mine the bearing dataset by Paderborn University (PU)","text":""},{"location":"datasets/#original-title-condition-monitoring-of-bearing-damage-in-electromechanical-drive-systems-by-using-motor-current-signals-of-electric-motors-a-benchmark-data-set-for-data-driven-classification","title":"Original title: Condition Monitoring of Bearing Damage in Electromechanical Drive Systems by Using Motor Current Signals of Electric Motors: A Benchmark Data Set for Data-Driven Classification","text":""},{"location":"datasets/#external-resources_6","title":"External resources:","text":"<ul> <li>https://www.papers.phmsociety.org/index.php/phme/article/view/1577</li> <li>https://mb.uni-paderborn.de/kat/forschung/kat-datacenter/bearing-datacenter/data-sets-and-download</li> </ul>"},{"location":"datasets/#description_12","title":"Description:","text":""},{"location":"datasets/#instantiation-pubase_directory-folders-channels-reps","title":"Instantiation: <code>PU(base_directory, folders, channels, reps)</code>","text":"<ul> <li><code>base_directory</code> is the home directory of the extracted folders.</li> <li><code>folders</code> is the list of the extracted folders, to include.</li> <li><code>channels</code> is the ist of strings, specifying the desired channels; available choices are 'CP1', 'CP2' and 'Vib' corresponding to the current phases (1 and 2) and acceleration. The default value is ['CP1', 'CP2', 'Vib'].</li> <li><code>reps</code> is a list of integers that specifies the number of repetitions (from 1 to 20), to include. The default value is [1, 2, 3, ... , 19, 20].</li> </ul>"},{"location":"datasets/#mining-puminemining_params","title":"Mining: <code>PU.mine(mining_params)</code>","text":"<ul> <li><code>mining_params</code> is a python dictonary whose keys are <code>win_len</code> and <code>hop_len</code>.\u00a0</li> </ul>"},{"location":"datasets/#accessing-data-pudata","title":"Accessing data: <code>PU.data</code>","text":"<p>Mined data is organized as a python dictonary whose keys are elements of the <code>channels</code>; corresponding values are lists of <code>pd.DataFrame</code> objects.</p>"},{"location":"datasets/#usage-example_12","title":"Usage example:","text":"<pre><code># Importings\nfrom damavand.damavand.datasets.downloaders import read_addresses, PuDownloader\nfrom damavand.damavand.datasets.digestors import PU\n\n# Downloading the dataset\naddresses = read_addresses()\naddresses['PU'].pop('real_damage')\ndownloader = PuDownloader(addresses['PU'])\ndownloader.download_extract(download_path = 'PU_compressed/', extraction_path = 'PU/', timeout = 10)\n\n#Mining the dataset\nmining_params = {'win_len': 16000, 'hop_len': 16000}\npu = PU('PU/', os.listdir('PU/'), ['Vib'],reps = [1])\npu.mine(mining_params)\n</code></pre>"},{"location":"inspiration/","title":"Inspiration","text":"<p> Source </p> <p>This package is named after Mount Damavand in Iran. Damavand is not only the highest mountain peak in Iran, but also a national symbol of strength, endurance, and steadiness \u2014 an image that recurs across Persian literature, myth, and travel writing. May this package carry forward not just its name, but also a share of the resilience and endurance that Damavand has long stood for.</p>"},{"location":"inspiration/#about-damavand","title":"About Damavand","text":"<ul> <li>Elevation: ~5,609 metres (18,403 feet) above sea level :contentReference[oaicite:0]{index=0}  </li> <li>Coordinates: 35.9545\u00b0 N, 52.1100\u00b0 E</li> <li>Prominence: ~4,751 metres \u2014 making Damavand among the most prominent peaks globally</li> <li>Location: Alborz mountain range, Mazandaran Province, Iran</li> <li>Distance from Tehran: ~66-70 km northeast (line distance)</li> </ul>"},{"location":"inspiration/#appearance-of-damavand-in-artistry","title":"Appearance of Damavand in Artistry","text":"<p>As both a natural wonder and a cultural symbol, Damavand has inspired countless works of literature, music, and visual art across centuries. Below are selected examples that reflect its presence in Persian artistry.</p>"},{"location":"inspiration/#damavand-in-persian-literature","title":"Damavand in Persian Literature","text":"<p>Below are three notable, precisely rendered references to Damavand from Persian sources (original Persian lines followed by a careful literal translation and a more natural contemporary English rendering). Each entry includes a short note on context.</p>"},{"location":"inspiration/#ferdowsi-wikipedia-shahnameh-10th11th-century","title":"Ferdowsi (Wikipedia) \u2014 Sh\u0101hn\u0101meh (10th\u201311th century)","text":"<p>In Ferdowsi\u2019s epic, the Sh\u0101hn\u0101meh, Damavand becomes the eternal prison of Zahh\u0101k, a mythical tyrant defeated by the hero Fereydun.  </p> <p>\u0628\u06cc\u0622\u0648\u0631\u062f \u0636\u062d\u0627\u06a9 \u0631\u0627 \u0686\u0648\u0646 \u0646\u0648\u0646\u062f \u0628\u0647 \u06a9\u0648\u0647 \u062f\u0645\u0627\u0648\u0646\u062f \u06a9\u0631\u062f\u0634 \u0628\u0647 \u0628\u0646\u062f  </p> <p>English translation: \u201cThey quickly brought Zahh\u0101k and chained him within Mount Damavand.\u201d</p>"},{"location":"inspiration/#nasir-i-khusraw-wikipedia-safarnameh-11th-century","title":"N\u0101\u1e63ir-i Khusraw (Wikipedia) \u2014 Safarn\u0101meh (11th century)","text":"<p>The philosopher and traveler N\u0101\u1e63ir-i Khusraw describes Damavand not in myth, but in reality \u2014 as a towering dome between Ray and Amol, known for its mineral wealth.  </p> <p>\u0648 \u0645\u06cc\u0627\u0646 \u0631\u06cc \u0648 \u0622\u0645\u0644 \u06a9\u0648\u0647 \u062f\u0645\u0627\u0648\u0646\u062f \u0627\u0633\u062a \u0645\u0627\u0646\u0646\u062f \u06af\u0646\u0628\u062f\u06cc \u0648 \u0622\u0646 \u0631\u0627 \u0644\u0648\u0627\u0633\u0627\u0646 \u06af\u0648\u06cc\u0646\u062f\u060c \u0648 \u06af\u0648\u06cc\u0646\u062f \u0628\u0631 \u0633\u0631 \u0622\u0646 \u0686\u0627\u0647\u06cc\u0633\u062a \u06a9\u0647 \u0646\u0648\u0634\u0627\u062f\u0631 \u0627\u0632 \u0622\u0646 \u062d\u0627\u0635\u0644 \u0645\u06cc\u200c\u0634\u0648\u062f \u0648 \u06af\u0648\u06cc\u0646\u062f \u06a9\u0647 \u06a9\u0628\u0631\u06cc\u062a \u0646\u06cc\u0632\u060c \u0648 \u0645\u0631\u062f\u0645 \u067e\u0648\u0633\u062a \u06af\u0627\u0648 \u0628\u0628\u0631\u0646\u062f \u0648 \u067e\u0631 \u0646\u0648\u0634\u0627\u062f\u0631 \u06a9\u0646\u0646\u062f \u0648 \u0627\u0632 \u0633\u0631 \u06a9\u0648\u0647 \u0628\u063a\u0644\u062a\u0627\u0646\u0646\u062f \u06a9\u0647 \u0628\u0647 \u0631\u0627\u0647 \u0646\u062a\u0648\u0627\u0646 \u0641\u0631\u0648\u062f \u0622\u0648\u0631\u062f\u0646.  </p> <p>English translation: \u201cBetween Ray and Amol stands Mount Damavand, dome-like. They say on its summit lies a source of natron and sulphur; people fill cowhides with the mineral and roll them down the mountain since the steep paths prevent carrying them down.\u201d</p>"},{"location":"inspiration/#mohammad-taqi-bahar-wikipedia-damavandieh-early-20th-century","title":"Mohammad-Taqi Bah\u0101r (Wikipedia) \u2014 Dam\u0101vandieh (early 20th century)","text":"<p>The modern poet Bah\u0101r personifies Damavand as a chained giant \u2014 snow-capped, iron-girdled, yet unyielding \u2014 a powerful symbol of resistance and endurance.  </p> <p>\u0627\u06cc \u062f\u06cc\u0648 \u0633\u067e\u06cc\u062f \u067e\u0627\u06cc \u062f\u0631 \u0628\u0646\u062f \u0627\u06cc \u06af\u0646\u0628\u062f \u06af\u06cc\u062a\u06cc \u0627\u06cc \u062f\u0645\u0627\u0648\u0646\u062f  </p> <p>\u0627\u0632 \u0633\u06cc\u0645 \u0628\u0647 \u0633\u0631\u060c \u06cc\u06a9\u06cc \u06a9\u0644\u0647\u200c\u062e\u0648\u062f \u0632 \u0622\u0647\u0646 \u0628\u0647 \u0645\u06cc\u0627\u0646 \u06cc\u06a9\u06cc \u06a9\u0645\u0631\u0628\u0646\u062f  </p> <p>English translation: \u201cO Damavand \u2014 white giant, bound at the feet; a silvery helm crowns your head, and an iron girdle clasps your waist.\u201d</p>"},{"location":"inspiration/#damavand-in-music","title":"Damavand in Music","text":"<p>One of the most glorious appearances of Damavand in music, is defiently the Symphony No. 6 by Shahin Farhat; Iranian musician and composer.</p> <p>   Sources:   left,    right </p> <p> Listen on SoundCloud </p>"},{"location":"inspiration/#damavand-through-the-lenses","title":"Damavand through the Lenses","text":"<p>Photography has captured Damavand\u2019s towering presence, often highlighting its snow-covered peak against Iran\u2019s landscapes.  </p> <p> </p> <p>   Sources:    top-left,    top-middle,    top-right,    bottom </p>"},{"location":"signal_processing/","title":"Damavand Documention - Signal Processing Module API Reference","text":"<p>Signal processing is of great importance in rotating machinery condition monitoring. On this page, we go through both submodules of the signal processing module: 1)Transformations and 2)Feature extraction. While the former is focused around the application of signal processing transformations (e.g. Hilbert Transform, Discrete Fourier Transform and ...) to process the raw time-series, the latter is developed around the extraction of hand-crafted features.</p>"},{"location":"signal_processing/#transformations-submodule","title":"Transformations Submodule","text":"<p>In this section, we discuss various signal processing transformations that are available.</p>"},{"location":"signal_processing/#envsignals","title":"<code>env(signals)</code>","text":""},{"location":"signal_processing/#extracting-the-envelope-of-a-set-of-signals","title":"Extracting the envelope of a set of signals","text":""},{"location":"signal_processing/#arguments","title":"Arguments:","text":"<ul> <li>signals: A <code>pandas.DataFrame</code> incuding signals in its rows.</li> </ul>"},{"location":"signal_processing/#return-value","title":"Return Value:","text":"<ul> <li>A <code>pandas.DataFrame</code> whose rows are the envelopes of the signals stored in the inputted DataFrame.</li> </ul>"},{"location":"signal_processing/#descriptions","title":"Descriptions:","text":"<p>This function extracts the envelope of signals, stored in a <code>pandas.DataFrame</code> object. This is done through the application of Hilbert transform (<code>scipy.signal.hilbert</code>); also <code>numpy.abs</code> is used to calculate the absolute magnitude, from both the imaginery and real parts.</p>"},{"location":"signal_processing/#usage-example","title":"Usage example:","text":"<pre><code>from damavand.damavand.datasets.downloaders import read_addresses, ZipDatasetDownloader\nfrom damavand.damavand.datasets.digestors import MFPT\nfrom damavand.damavand.signal_processing.transformations import env\n\n# Downloading the MFPT dataset\naddresses = read_addresses()\ndownloader = ZipDatasetDownloader(addresses['MFPT'])\ndownloader.download_extract('MFPT.zip', 'MFPT/')\n\nmfpt = MFPT('MFPT/MFPT Fault Data Sets/', [\n\u00a0 \u00a0 '1 - Three Baseline Conditions',\n\u00a0 \u00a0 '2 - Three Outer Race Fault Conditions',\n\u00a0 \u00a0 '3 - Seven More Outer Race Fault Conditions',\n\u00a0 \u00a0 '4 - Seven Inner Race Fault Conditions',\n])\n\n# Mining the dataset\nmining_params = {\n\u00a0 \u00a0 97656: {'win_len': 16671, 'hop_len': 2000},\n\u00a0 \u00a0 48828: {'win_len': 8337, 'hop_len': 1000},\n}\nmfpt.mine(mining_params)\n\n# Signal/Metadata split\ndf = pd.concat(mfpt.data[48828]).reset_index(drop = True)\nsignals, metadata = df.iloc[:, : - 4], df.iloc[:, - 4 :]\n\n# Envelope extraction\nsignals_env = env(signals)\n</code></pre>"},{"location":"signal_processing/#fftsignals-freq_filter-none-window-none","title":"<code>fft(signals, freq_filter = None, window = None)</code>","text":""},{"location":"signal_processing/#applying-the-fast-fourier-transform-algorithim-to-derive-frequency-domain-representation-of-a-set-of-signals","title":"Applying the Fast-Fourier Transform algorithim to derive frequency domain representation of a set of signals","text":""},{"location":"signal_processing/#arguemnts","title":"Arguemnts:","text":"<ul> <li>signals: A <code>pandas.DataFrame()</code> incuding signals in its rows.</li> <li>freq_filter:\u00a0 A frequency filter object from <code>scipy.signal</code> module (e.g. <code>scipy.signal.butter</code>) to avoid aliasing.</li> <li>window: A window object from <code>scipy.signal.windows</code> module (e.g. <code>scipy.signal.windows.hann</code>) to encounter the leakage error.</li> </ul>"},{"location":"signal_processing/#return-value_1","title":"Return Value:","text":"<ul> <li>A <code>pandas.DataFrame</code> whose rows are the frequency representations of the inputted DataFrame. As only the real frequency axis is of importance, the lenght of the frequency domain signals is half of the original time domain signal.</li> </ul>"},{"location":"signal_processing/#descriptions_1","title":"Descriptions:","text":"<p>This function computes the Discrete Fourier Transform (DFT) of a set of signals, through the application of Fast-Fourier Transform (<code>scipy.fft.fft</code>) algorithm. As it returns only the coeeficients correpsonding to real frequency components (not the imaginery ones), lenght of the returned <code>pandas.DataFrame</code> is half of the inputted <code>pandas.DataFrame</code>. <code>freq_filter</code> and <code>window</code> are not mandatory arguments and a function call without them is valid, however, we recommend using them to avoid aliasing (and of course near-zero/DC filtering through band-pass filters) and leakage error. We encourage you to use frequency axis for the sake of visualization; this can be done using either of the followings: <code>scipy.fft.fftfreq</code>, <code>numpy.linspace</code> and damavand.utils.fft_freq_axis.</p>"},{"location":"signal_processing/#usage-example_1","title":"Usage example:","text":"<pre><code>from damavand.damavand.datasets.downloaders import read_addresses, ZipDatasetDownloader\nfrom damavand.damavand.datasets.digestors import MFPT\nfrom damavand.damavand.signal_processing.transformations import fft\nimport scipy\n\n# Downloading the MFPT dataset\naddresses = read_addresses()\ndownloader = ZipDatasetDownloader(addresses['MFPT'])\ndownloader.download_extract('MFPT.zip', 'MFPT/')\n\nmfpt = MFPT('MFPT/MFPT Fault Data Sets/', [\n\u00a0 \u00a0 '1 - Three Baseline Conditions',\n\u00a0 \u00a0 '2 - Three Outer Race Fault Conditions',\n\u00a0 \u00a0 '3 - Seven More Outer Race Fault Conditions',\n\u00a0 \u00a0 '4 - Seven Inner Race Fault Conditions',\n])\n\n# Mining the dataset\nmining_params = {\n\u00a0 \u00a0 97656: {'win_len': 16671, 'hop_len': 2000},\n\u00a0 \u00a0 48828: {'win_len': 8337, 'hop_len': 1000},\n}\nmfpt.mine(mining_params)\n\n# Signal/Metadata split\ndf = pd.concat(mfpt.data[48828]).reset_index(drop = True)\nsignals, metadata = df.iloc[:, : - 4], df.iloc[:, - 4 :]\n\n# Frequency spectra extraction, through FFT\nwindow = scipy.signal.windows.hann(signals_env.shape[1])\nfreq_filter = scipy.signal.butter(25, [5, 23500], 'bandpass', fs = float(metadata.iloc[0, 0]), output='sos')\nsignals_fft = fft(signals, freq_filter = freq_filter, window = window)\n</code></pre>"},{"location":"signal_processing/#zoomed_fftsignals-f_min-f_max-desired_len-sampling_freq-freq_filter-none-window-none","title":"<code>zoomed_fft(signals, f_min, f_max, desired_len, sampling_freq, freq_filter = None, window = None)</code>","text":""},{"location":"signal_processing/#applying-the-zoomfft-algorithm-to-derive-a-fine-grained-frequency-representation-in-a-desired-frequency-range","title":"Applying the ZoomFFT algorithm to derive a fine-grained frequency representation in a desired frequency range","text":""},{"location":"signal_processing/#arguments_1","title":"Arguments:","text":"<ul> <li>signals: A <code>pandas.DataFrame</code> incuding signals in its rows.</li> <li>f_min: Minum of the desired frequency range.</li> <li>f_max: Maximum of the desired frequency range.</li> <li>desired_len: The desired length of the frequency domain representation.</li> <li>sampling_freq: The sampling frequency of the signals included in signals.</li> <li>freq_filter:\u00a0 A frequency filter object from <code>scipy.signal</code> module (e.g. <code>scipy.signal.butter</code>) to avoid aliasing.</li> <li>window: A window object from <code>scipy.signal.windows</code> module (e.g. <code>scipy.signal.windows.hann</code>) to encounter the leakage error.</li> </ul>"},{"location":"signal_processing/#return-value_2","title":"Return Value:","text":"<ul> <li>A <code>pandas.DataFrame</code> whose rows are the frequency representations of the inputted DataFrame, in the desired frequency range and with the chosen lenght.</li> </ul>"},{"location":"signal_processing/#descriptions_2","title":"Descriptions:","text":"<p>This function enables one to derive a frequency represenationin a desired frequency range and with the desired length, through the application of <code>sicpy.signal.ZoomFFT</code>. <code>freq_filter</code> and <code>window</code> are not mandatory arguments and a function call without them is valid, however, we recommend using them to avoid aliasing (and of course near-zero/DC filtering through band-pass filters) and leakage error. We encourage you to use frequency axis for the sake of visualization; this can be done using either of the followings: <code>numpy.linspace</code> or <code>damavand.utils.zoomed_fft_freq_axis</code>.</p>"},{"location":"signal_processing/#usage-example_2","title":"Usage example:","text":"<pre><code>from damavand.damavand.datasets.downloaders import read_addresses, ZipDatasetDownloader\nfrom damavand.damavand.datasets.digestors import MFPT\nfrom damavand.damavand.signal_processing.transformations import zoomed_fft\nimport scipy\n\n# Downloading the MFPT dataset\naddresses = read_addresses()\ndownloader = ZipDatasetDownloader(addresses['MFPT'])\ndownloader.download_extract('MFPT.zip', 'MFPT/')\n\nmfpt = MFPT('MFPT/MFPT Fault Data Sets/', [\n\u00a0 \u00a0 '1 - Three Baseline Conditions',\n\u00a0 \u00a0 '2 - Three Outer Race Fault Conditions',\n\u00a0 \u00a0 '3 - Seven More Outer Race Fault Conditions',\n\u00a0 \u00a0 '4 - Seven Inner Race Fault Conditions',\n])\n\n# Mining the dataset\nmining_params = {\n\u00a0 \u00a0 97656: {'win_len': 16671, 'hop_len': 2000},\n\u00a0 \u00a0 48828: {'win_len': 8337, 'hop_len': 1000},\n}\nmfpt.mine(mining_params)\n\n# Signal/Metadata split\ndf = pd.concat(mfpt.data[48828]).reset_index(drop = True)\nsignals, metadata = df.iloc[:, : - 4], df.iloc[:, - 4 :]\n\n# Frequency spectra extraction, through zoomed_FFT\nwindow = scipy.signal.windows.hann(signals_env.shape[1])\nfreq_filter = scipy.signal.butter(25, [5, 23500], 'bandpass', fs = float(metadata.iloc[0, 0]), output='sos')\nsignals_ZoomedFFT = zoomed_fft(signals_env, 0, 2500, 2500, float(metadata.iloc[0, 0]), freq_filter = freq_filter, window = window)\n</code></pre>"},{"location":"signal_processing/#stftsignals-window_len-hop_len-freq_filter-none-window-none","title":"<code>stft(signals, window_len, hop_len, freq_filter = None, window = None)</code>","text":""},{"location":"signal_processing/#application-of-short-time-fourier-transform-to-derive-time-frequency-representation-of-the-inputted-signals","title":"Application of Short-Time Fourier Transform to derive Time-Frequency representation of the inputted signals","text":""},{"location":"signal_processing/#arguemnts_1","title":"Arguemnts:","text":"<ul> <li>signals: A <code>pandas.DataFrame</code> incuding signals in its rows.</li> <li>window_len: Lenght of the desired time segments.</li> <li>hop_len: Length of the feed, used to get forward during the segmentation process.</li> <li>freq_filter:\u00a0 A frequency filter object from <code>scipy.signal</code> module (e.g. <code>scipy.signal.butter</code>) to avoid aliasing.</li> <li>window: A window object from <code>scipy.signal.windows</code> module (e.g. <code>scipy.signal.windows.hann</code>) to encounter the leakage error.</li> </ul>"},{"location":"signal_processing/#return-value_3","title":"Return Value:","text":"<ul> <li>A <code>numpy.array</code>, whose first dimension equals the number of rows included in the input <code>pandas.DataFrame</code>; it includes derived Time-Frequency representations of the inputted signals. <code>freq_filter</code> and <code>window</code> are not mandatory arguments and a function call without them is valid, however, we recommend using them to avoid aliasing (and of course near-zero/DC filtering through band-pass filters) and leakage error. Pay attention that unlike the case of <code>damavand.signal_processing.FFT</code> or <code>damavand.signal_processing.ZoomedFFT</code>, for this function you have to define freq_filter and window objects with a lenght that suits the segmented signals (equal to the <code>window_len</code> argument), instead of the original signals, presented in the inputted <code>pandas.DataFrame</code>.</li> </ul>"},{"location":"signal_processing/#descriptions_3","title":"Descriptions:","text":"<p>By the application of this function, one is able to derive Time-Frequency representation; this is done by first segmenting the original signals to a series of shorter signals and consecutively FFT is applied on each segmented signal to derive the corresponding frequency representation. Results are usually visualized as heatmaps.</p>"},{"location":"signal_processing/#usage-example_3","title":"Usage example:","text":"<pre><code>from damavand.damavand.datasets.downloaders import read_addresses, ZipDatasetDownloader\nfrom damavand.damavand.datasets.digestors import MFPT\nfrom damavand.damavand.signal_processing.transformations import stft\nimport scipy\n\n# Downloading the MFPT dataset\naddresses = read_addresses()\ndownloader = ZipDatasetDownloader(addresses['MFPT'])\ndownloader.download_extract('MFPT.zip', 'MFPT/')\n\nmfpt = MFPT('MFPT/MFPT Fault Data Sets/', [\n\u00a0 \u00a0 '1 - Three Baseline Conditions',\n\u00a0 \u00a0 '2 - Three Outer Race Fault Conditions',\n\u00a0 \u00a0 '3 - Seven More Outer Race Fault Conditions',\n\u00a0 \u00a0 '4 - Seven Inner Race Fault Conditions',\n])\n\n# Mining the dataset\nmining_params = {\n\u00a0 \u00a0 97656: {'win_len': 16671, 'hop_len': 2000},\n\u00a0 \u00a0 48828: {'win_len': 8337, 'hop_len': 1000},\n}\nmfpt.mine(mining_params)\n\n# Signal/Metadata split\ndf = pd.concat(mfpt.data[48828]).reset_index(drop = True)\nsignals, metadata = df.iloc[:, : - 4], df.iloc[:, - 4 :]\n\n# Time-Frequency spectrograms extraction, through stft\nSTFT_window = scipy.signal.windows.hann(2400)\nSTFT_freq_filter = scipy.signal.butter(25, [5, 23500], 'bandpass', fs = float(metadata.iloc[0, 0]), output='sos')\nsignal_STFT = stft(signals, 2400, 200, STFT_freq_filter, STFT_window)\n</code></pre>"},{"location":"signal_processing/#feature-extraction-submodule","title":"Feature Extraction Submodule","text":"<p>Hand-crafted features (from both time and frequency domains) are widely used for rotating machinery conidition monitoring. This submodule, facilitates the extraction of such features from raw (time and frequency) data. We first introduce <code>feature_extractor(signals, features)</code> function that enables one to extract various functions from raw data <code>pd.DataFrame</code>s, at once. Next, a comprehensive list of features to extract are introduced; last but not least, a code snippet to extract these features is provided.</p>"},{"location":"signal_processing/#feature_extractorsignals-features","title":"<code>feature_extractor(signals, features)</code>","text":""},{"location":"signal_processing/#extracting-a-number-of-features-from-the-inpuuted-signals","title":"Extracting a number of features from the inpuuted signals","text":""},{"location":"signal_processing/#arguments_2","title":"Arguments:","text":"<ul> <li>signals: A <code>pandas.DataFrame</code> incuding signals in its rows.</li> <li>features: A python <code>dict</code> where: \u00a0 - keys are feature names \u00a0 - values are tuples of (function, args, kwargs) where: \u00a0 \u00a0 * function: the feature extraction function \u00a0 \u00a0 * args: tuple of positional arguments (optional) \u00a0 \u00a0 * kwargs: dict of keyword arguments (optional)</li> </ul>"},{"location":"signal_processing/#return-value_4","title":"Return Value:","text":"<ul> <li>A <code>pandas.DataFrame</code>, including the feature values for the signals in the inputted <code>pandas.DataFrame</code>.</li> </ul>"},{"location":"signal_processing/#description","title":"Description:","text":"<p>To extract a set of features from the signals presented in a <code>pandas.DataFrame</code>, one can use this function. Features of interest are supposed to be passed as a python <code>dict</code> where: \u00a0 - keys are feature names \u00a0 - values are tuples of (function, args, kwargs) where:</p> <p>- function: the feature extraction function \u00a0 \u00a0 - args: tuple of positional arguments (optional) \u00a0 \u00a0 - kwargs: dict of keyword arguments (optional)</p>"},{"location":"signal_processing/#features-to-extract","title":"Features to extract","text":"<p>This study introduces 11 time-domain and 13 frequency-domain (24 in total) features for rotating machinery fault diagnosis. Detailed list of them alongside tips on how to extract them using <code>feature_extractor(signals, features)</code> is included in the table below. It is worth mentioning that \\(x(n)\\), \\(s(k)\\) and \\(f_k\\) correspond to time-domain signal, frequency spectrum and corresponding frequency axis; moreover, TS and FS in the Description column stand for time-series and frequency spectra. For detailed example on how to extract these features using <code>feature_extractor(signals, features)</code>, checkout the code snippet below the table.</p> Number Formula Description Implementation P1 \\(P_1 = \\frac{\\sum_{n=1}^{N} x(n)}{N}\\) TS Mean <code>np.mean</code> P2 \\(P_2 = \\sqrt{\\frac{\\sum_{n=1}^{N} (x(n)-P_1)^2}{N-1}}\\) TS Standard Deviation <code>np.std</code> P3 \\(P_3 = \\left(\\frac{\\sum_{n=1}^{N} \\sqrt{\\|x(n)\\|}}{N}\\right)^{2}\\) TS Squared Mean of Square Roots of Absolutes <code>damavand.damavand.signal_processing.feature_extraction.smsa</code> P4 \\(P_4 = \\sqrt{\\frac{\\sum_{n=1}^{N} (x(n))^2}{N}}\\) TS Root Mean Square <code>damavand.damavand.signal_processing.feature_extraction.rms</code> P5 \\(P_5 = \\max \\|x(n)\\|\\) TS Peak <code>damavand.damavand.signal_processing.feature_extraction.peak</code> P6 \\(P_6 = \\frac{\\sum_{n=1}^{N} (x(n)-P_1)^3}{(N-1)P_1^3}\\) TS Skewness <code>scipy.stats.skew</code> P7 \\(P_7 = \\sqrt{\\frac{\\sum_{n=1}^{N} (x(n)-P_1)^4}{(N-1)P_1^4}}\\) TS Kurtosis <code>scipy.stats.kurtosis</code> P8 \\(P_8 = \\frac{p_5}{p_4}\\) TS Crest Factor <code>damavand.damavand.signal_processing.feature_extraction.crest_factor</code> P9 \\(P_9 = \\frac{p_5}{p_3}\\) TS Clearance Factor <code>damavand.damavand.signal_processing.feature_extraction.clearance_factor</code> P10 \\(P_{10} = \\frac{P_4}{\\frac{1}{N}\\sum_{n=1}^{N}\\|x(n)\\|}\\) TS Shape Factor <code>damavand.damavand.signal_processing.feature_extraction.shape_factor</code> P11 \\(P_{11} = \\frac{P_5}{\\frac{1}{N}\\sum_{n=1}^{N}\\|x(n)\\|}\\) TS Impulse Factor <code>damavand.damavand.signal_processing.feature_extraction.impulse_factor</code> P12 \\(P_{12} = \\frac{\\sum_{k=1}^{K} s(k)}{K}\\) FS Mean <code>np.mean</code> P13 \\(P_{13} = \\sqrt{\\frac{\\sum_{k=1}^{K} (s(k)-P_{12})^2}{K-1}}\\) FS Variance <code>np.var</code> P14 \\(P_{14} = \\frac{\\sum_{k=1}^{K} (s(k)-P_{12})^3}{K(\\sqrt{P_{12}})^3}\\) FS Skewness <code>scipy.stats.skew</code> P15 \\(P_{15} = \\frac{\\sum_{k=1}^{K} (s(k)-P_{12})^4}{KP_{13}^2}\\) FS Kurtosis <code>scipy.stats.kurtosis</code> P16 \\(P_{16} = \\frac{\\sum_{k=1}^{K} f_k \\cdot s(k)}{\\sum_{k=1}^{K} s(k)}\\) FS Spectral Centroid <code>damavand.damavand.signal_processing.feature_extraction.spectral_centroid</code> P17 \\(P_{17} = \\sqrt{\\frac{\\sum_{k=1}^{K} (f_k - P_{16})^2 \\cdot s(k)}{K}}\\) <code>damavand.damavand.signal_processing.feature_extraction.P17</code> P18 \\(P_{18} = \\sqrt{\\frac{\\sum_{k=1}^{K} f_k^2 \\cdot s(k)}{\\sum_{k=1}^{K} s(k)}}\\) <code>damavand.damavand.signal_processing.feature_extraction.P18</code> P19 \\(P_{19} = \\sqrt{\\frac{\\sum_{k=1}^{K} f_k^4 \\cdot s(k)}{\\sum_{k=1}^{K} f_k^2 \\cdot s(k)}}\\) <code>damavand.damavand.signal_processing.feature_extraction.P19</code> P20 \\(P_{20} = \\frac{\\sum_{k=1}^{K} f_k^2 \\cdot s(k)}{\\sum_{k=1}^{K} s(k) \\sum_{k=1}^{K} f_k^4 \\cdot s(k)}\\) <code>damavand.damavand.signal_processing.feature_extraction.P20</code> P21 \\(P_{21} = \\frac{P_{17}}{P_{16}}\\) <code>damavand.damavand.signal_processing.feature_extraction.P21</code> P22 \\(P_{22} = \\frac{\\sum_{k=1}^{K} (f_k-P_{16})^3 \\cdot s(k)}{KP_{17}^3}\\) <code>damavand.damavand.signal_processing.feature_extraction.P22</code> P23 \\(P_{23} = \\frac{\\sum_{k=1}^{K} (f_k-P_{16})^4 \\cdot s(k)}{KP_{17}^4}\\) <code>damavand.damavand.signal_processing.feature_extraction.P23</code> P24 \\(P_{24} = \\frac{\\sum_{k=1}^{K} (f_k-P_{16})^{1/2} \\cdot s(k)}{K\\sqrt{P_{17}}}\\) <code>damavand.damavand.signal_processing.feature_extraction.P24</code>"},{"location":"signal_processing/#example-on-using-feature_extractorsignals-features-to-extract-features","title":"Example on using <code>feature_extractor(signals, features)</code> to extract features","text":"<p>The code snippet below, explains the feature extraction from both time and frequency domains, comprehensively.</p> <pre><code># Importings\nfrom damavand.damavand.datasets.downloaders import read_addresses, ZipDatasetDownloader\nfrom damavand.damavand.datasets.digestors import MFPT\nfrom damavand.damavand.signal_processing.feature_extraction import *\nfrom damavand.damavand.signal_processing.transformations import fft\nfrom damavand.damavand.utils import *\nfrom zipfile import ZipFile\nimport os\nimport pandas as pd\nimport numpy as np\nimport scipy\n\n# Downloading the MFPT dataset\naddresses = read_addresses()\ndownloader = ZipDatasetDownloader(addresses['MFPT'])\ndownloader.download_extract('MFPT.zip', 'MFPT/')\n\nmfpt = MFPT('MFPT/MFPT Fault Data Sets/', [\n\u00a0 \u00a0 '1 - Three Baseline Conditions',\n\u00a0 \u00a0 '2 - Three Outer Race Fault Conditions',\n\u00a0 \u00a0 '3 - Seven More Outer Race Fault Conditions',\n\u00a0 \u00a0 '4 - Seven Inner Race Fault Conditions',\n])\n\n# Mining the dataset\nmining_params = {\n\u00a0 \u00a0 97656: {'win_len': 16671, 'hop_len': 2000},\n\u00a0 \u00a0 48828: {'win_len': 8337, 'hop_len': 1000},\n}\nmfpt.mine(mining_params)\n\n# Signal/Metadata split\ndf = pd.concat(mfpt.data[48828]).reset_index(drop = True)\nsignals, metadata = df.iloc[:, : - 4], df.iloc[:, - 4 :]\n\n# Extracting time-domain features\ntime_features = {\n\u00a0 'mean': (np.mean, (), {}),\n\u00a0 'std': (np.std, (), {}),\n\u00a0 'smsa': (smsa, (), {}),\n\u00a0 'rms': (rms, (), {}),\n\u00a0 'peak': (peak, (), {}),\n\u00a0 'skew': (scipy.stats.skew, (), {}),\n\u00a0 'kurtosis': (scipy.stats.kurtosis, (), {}),\n\u00a0 'crest_factor': (crest_factor, (), {}),\n\u00a0 'clearance_factor': (clearance_factor, (), {}),\n\u00a0 'shape_factor': (shape_factor, (), {}),\n\u00a0 'impulse_factor': (impulse_factor, (), {}),\n}\ntime_features_df = feature_extractor(signals, time_features)\n\n# Applying the FFT to transform data into frequency-domain\nwindow = scipy.signal.windows.hann(signals.shape[1])\nfreq_filter = scipy.signal.butter(25, [5, 12500], 'bandpass', fs = 25600, output='sos')\nsignals_fft = fft(signals, freq_filter = freq_filter, window = window)\nfreq_axis = fft_freq_axis(8337, 48828)\n\n# Extracting frequency-domain features\nfreq_features = {\n\u00a0 'mean': (np.mean, (), {}),\n\u00a0 'var': (np.var, (), {}),\n\u00a0 'skew': (scipy.stats.skew, (), {}),\n\u00a0 'kurtosis': (scipy.stats.kurtosis, (), {}),\n\u00a0 'spectral_centroid': (spectral_centroid, (freq_axis,), {}),\n\u00a0 'P17': (P17, (freq_axis,), {}),\n\u00a0 'P18': (P18, (freq_axis,), {}),\n\u00a0 'P19': (P19, (freq_axis,), {}),\n\u00a0 'P20': (P20, (freq_axis,), {}),\n\u00a0 'P21': (P21, (freq_axis,), {}),\n\u00a0 'P22': (P22, (freq_axis,), {}),\n\u00a0 'P23': (P23, (freq_axis,), {}),\n\u00a0 'P24': (P24, (freq_axis,), {}),\n}\nfreq_features_df = feature_extractor(signals_fft, freq_features)\n</code></pre>"},{"location":"utils/","title":"Damavand Documention - Utilities Module API Reference","text":"<p>We prefer to include entities that are used in different modules in the utilities (utils for short) module.</p>"},{"location":"utils/#splitterarray-win_len-hop_len-return_df-true","title":"<code>splitter(array, win_len, hop_len, return_df = True)</code>","text":""},{"location":"utils/#splitting-raw-data-signals-into-series-of-segmented-signals","title":"Splitting raw data signals into series of segmented signals","text":""},{"location":"utils/#arguments","title":"Arguments:","text":"<ul> <li>array: A column vector <code>np.array</code> (the array should be one-dimensional) including the original signal.</li> <li>win_len: The desired length of the segmented signals.</li> <li>hop_len: The forward move of the window.</li> <li>return_df: A flag, determining the output type; the default value is <code>True</code>, meaning that returned type is a <code>pandas.DataFrame</code>. If set to <code>False</code>, the function returns a <code>np.array</code>. \u00a0\u00a0</li> </ul>"},{"location":"utils/#return-value","title":"Return Value:","text":"<ul> <li>A <code>pandas.DataFrame</code> including segmented signals, extracted from the original raw signal.</li> </ul>"},{"location":"utils/#descriptions","title":"Descriptions:","text":"<p>This function helps one to transform the raw signals into a series of segmented signals.</p> <pre><code># Importings\nimport numpy as np\nfrom damavand.damavand.utils import splitter\n\n# Creating a dummy np.array with the size of (10000)\narray = np.random.rand(10000)\n\n# Declaring win_len and hop_len\nwin_len, hop_len = 1000, 250\n\n# Splitting the original array and returning a Pandas.DataFrame\ndf = splitter(array, win_len, hop_len)\n\n# Splitting the original array and returning a numpy.array\nsegmented_array = splitter(array, win_len, hop_len, return_df = False)\n</code></pre>"},{"location":"utils/#usage-example","title":"Usage example:","text":""},{"location":"utils/#fft_freq_axistime_len-sampling_freq","title":"<code>fft_freq_axis(time_len, sampling_freq)</code>","text":""},{"location":"utils/#generating-a-frequency-axis-to-be-used-with-a-frequncy-spectrum-extracted-by-signal_processingtransformationsfft","title":"Generating a frequency axis to be used with a frequncy spectrum extracted by signal_processing.transformations.fft","text":""},{"location":"utils/#arguments_1","title":"Arguments:","text":"<ul> <li>time_len is the length of time-domain signals, FFT is applied on.</li> <li>sampling_freq is the sampling frequency. \u00a0\u00a0</li> </ul>"},{"location":"utils/#return-value_1","title":"Return Value:","text":"<ul> <li>A <code>numpy.array</code> that includes the frequency bins.</li> </ul>"},{"location":"utils/#descriptions_1","title":"Descriptions:","text":"<p>This function enables one to generate a frequency axis, corresponding to a Discrete Fourier Transform frequency spectrum.</p>"},{"location":"utils/#usage-example_1","title":"Usage example:","text":"<pre><code># Importing\nfrom damavand.damavand.utils import fft_freq_axis\n\n# Generating a frequency axis for frequency spectrum, extracted from time-series with the length of 10000 and sampling frequency of 5000\nfreq_axis = fft_freq_axis(10000, 5000)\n</code></pre>"},{"location":"utils/#zoomed_fft_freq_axisf_min-f_max-desired_len","title":"<code>zoomed_fft_freq_axis(f_min, f_max, desired_len)</code>","text":""},{"location":"utils/#generating-a-frequency-axis-to-be-used-with-a-frequncy-spectrum-extracted-by-signal_processingtransformationszoomed_fft","title":"Generating a frequency axis to be used with a frequncy spectrum extracted by signal_processing.transformations.zoomed_fft","text":""},{"location":"utils/#arguments_2","title":"Arguments:","text":"<ul> <li>f_min is the minumum frequency covered in the spectrum.</li> <li>f_max is the maximum frequency covered in the spectrum.</li> <li>desired_len is the length of frequency spectrum, extracted by <code>zoomed_fft_freq_axis</code>. \u00a0\u00a0</li> </ul>"},{"location":"utils/#return-value_2","title":"Return Value:","text":"<ul> <li>A <code>numpy.array</code> that includes the frequency bins.</li> </ul>"},{"location":"utils/#descriptions_2","title":"Descriptions:","text":"<p>This function enables one to generate a frequency axis, corresponding to a ZoomedFFT frequency spectrum.</p>"},{"location":"utils/#usage-example_2","title":"Usage example:","text":"<pre><code># Importing\nfrom damavand.damavand.utils import zoomed_fft_freq_axis\n\n# Generating a frequency axis for zoomed frequency spectrum, covering from 0 Hz to 2500 Hz using 2500 points\nzoomed_freq_axis = zoomed_fft_freq_axis(0, 2500, 2500)\n</code></pre>"},{"location":"notebooks/dataset_demos/CWRU_demo/","title":"Cloning the damavand repository","text":"In\u00a0[1]: Copied! <pre>!git clone https://github.com/amirberenji1995/damavand\n</pre> !git clone https://github.com/amirberenji1995/damavand <pre>Cloning into 'damavand'...\nremote: Enumerating objects: 263, done.\nremote: Counting objects: 100% (263/263), done.\nremote: Compressing objects: 100% (197/197), done.\nremote: Total 263 (delta 132), reused 196 (delta 65), pack-reused 0 (from 0)\nReceiving objects: 100% (263/263), 6.62 MiB | 14.71 MiB/s, done.\nResolving deltas: 100% (132/132), done.\n</pre> In\u00a0[2]: Copied! <pre>!pip install -r damavand/requirements.txt\n</pre> !pip install -r damavand/requirements.txt <pre>Collecting certifi==2024.7.4 (from -r damavand/requirements.txt (line 1))\n  Downloading certifi-2024.7.4-py3-none-any.whl.metadata (2.2 kB)\nCollecting charset-normalizer==3.3.2 (from -r damavand/requirements.txt (line 2))\n  Downloading charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\nCollecting idna==3.7 (from -r damavand/requirements.txt (line 3))\n  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\nCollecting numpy==1.26.4 (from -r damavand/requirements.txt (line 4))\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 61.0/61.0 kB 1.8 MB/s eta 0:00:00\nCollecting pandas==2.1.4 (from -r damavand/requirements.txt (line 5))\n  Downloading pandas-2.1.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\nRequirement already satisfied: python-dateutil==2.9.0.post0 in /usr/local/lib/python3.11/dist-packages (from -r damavand/requirements.txt (line 6)) (2.9.0.post0)\nCollecting pytz==2024.1 (from -r damavand/requirements.txt (line 7))\n  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\nCollecting rarfile==4.2 (from -r damavand/requirements.txt (line 8))\n  Downloading rarfile-4.2-py3-none-any.whl.metadata (4.4 kB)\nRequirement already satisfied: requests==2.32.3 in /usr/local/lib/python3.11/dist-packages (from -r damavand/requirements.txt (line 9)) (2.32.3)\nCollecting scipy==1.13.1 (from -r damavand/requirements.txt (line 10))\n  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 60.6/60.6 kB 2.4 MB/s eta 0:00:00\nCollecting six==1.16.0 (from -r damavand/requirements.txt (line 11))\n  Downloading six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\nCollecting tzdata==2024.1 (from -r damavand/requirements.txt (line 12))\n  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\nCollecting urllib3==2.2.2 (from -r damavand/requirements.txt (line 13))\n  Downloading urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\nDownloading certifi-2024.7.4-py3-none-any.whl (162 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 163.0/163.0 kB 1.6 MB/s eta 0:00:00\nDownloading charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (140 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 140.3/140.3 kB 5.0 MB/s eta 0:00:00\nDownloading idna-3.7-py3-none-any.whl (66 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 66.8/66.8 kB 3.9 MB/s eta 0:00:00\nDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 35.5 MB/s eta 0:00:00\nDownloading pandas-2.1.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 12.2/12.2 MB 35.9 MB/s eta 0:00:00\nDownloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 505.5/505.5 kB 34.4 MB/s eta 0:00:00\nDownloading rarfile-4.2-py3-none-any.whl (29 kB)\nDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 38.6/38.6 MB 16.6 MB/s eta 0:00:00\nDownloading six-1.16.0-py2.py3-none-any.whl (11 kB)\nDownloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 345.4/345.4 kB 26.0 MB/s eta 0:00:00\nDownloading urllib3-2.2.2-py3-none-any.whl (121 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 121.4/121.4 kB 9.6 MB/s eta 0:00:00\nInstalling collected packages: pytz, urllib3, tzdata, six, rarfile, numpy, idna, charset-normalizer, certifi, scipy, pandas\n  Attempting uninstall: pytz\n    Found existing installation: pytz 2025.2\n    Uninstalling pytz-2025.2:\n      Successfully uninstalled pytz-2025.2\n  Attempting uninstall: urllib3\n    Found existing installation: urllib3 2.5.0\n    Uninstalling urllib3-2.5.0:\n      Successfully uninstalled urllib3-2.5.0\n  Attempting uninstall: tzdata\n    Found existing installation: tzdata 2025.2\n    Uninstalling tzdata-2025.2:\n      Successfully uninstalled tzdata-2025.2\n  Attempting uninstall: six\n    Found existing installation: six 1.17.0\n    Uninstalling six-1.17.0:\n      Successfully uninstalled six-1.17.0\n  Attempting uninstall: numpy\n    Found existing installation: numpy 2.0.2\n    Uninstalling numpy-2.0.2:\n      Successfully uninstalled numpy-2.0.2\n  Attempting uninstall: idna\n    Found existing installation: idna 3.10\n    Uninstalling idna-3.10:\n      Successfully uninstalled idna-3.10\n  Attempting uninstall: charset-normalizer\n    Found existing installation: charset-normalizer 3.4.2\n    Uninstalling charset-normalizer-3.4.2:\n      Successfully uninstalled charset-normalizer-3.4.2\n  Attempting uninstall: certifi\n    Found existing installation: certifi 2025.7.14\n    Uninstalling certifi-2025.7.14:\n      Successfully uninstalled certifi-2025.7.14\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.16.0\n    Uninstalling scipy-1.16.0:\n      Successfully uninstalled scipy-1.16.0\n  Attempting uninstall: pandas\n    Found existing installation: pandas 2.2.2\n    Uninstalling pandas-2.2.2:\n      Successfully uninstalled pandas-2.2.2\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.1.4 which is incompatible.\nopencv-python 4.12.0.88 requires numpy&lt;2.3.0,&gt;=2; python_version &gt;= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy&lt;2.3.0,&gt;=2; python_version &gt;= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy&lt;2.3.0,&gt;=2; python_version &gt;= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nplotnine 0.14.5 requires pandas&gt;=2.2.0, but you have pandas 2.1.4 which is incompatible.\ntsfresh 0.21.0 requires scipy&gt;=1.14.0; python_version &gt;= \"3.10\", but you have scipy 1.13.1 which is incompatible.\nthinc 8.3.6 requires numpy&lt;3.0.0,&gt;=2.0.0, but you have numpy 1.26.4 which is incompatible.\nxarray 2025.7.1 requires pandas&gt;=2.2, but you have pandas 2.1.4 which is incompatible.\nmizani 0.13.5 requires pandas&gt;=2.2.0, but you have pandas 2.1.4 which is incompatible.\nSuccessfully installed certifi-2024.7.4 charset-normalizer-3.3.2 idna-3.7 numpy-1.26.4 pandas-2.1.4 pytz-2024.1 rarfile-4.2 scipy-1.13.1 six-1.16.0 tzdata-2024.1 urllib3-2.2.2\n</pre> In\u00a0[1]: Copied! <pre>from damavand.damavand.datasets.downloaders import read_addresses, CwruDownloader\nfrom damavand.damavand.datasets.digestors import CWRU\nfrom damavand.damavand.signal_processing.transformations import *\nfrom damavand.damavand.signal_processing.feature_extraction import *\nfrom damavand.damavand.utils import *\nimport pandas as pd\nimport numpy as np\nimport scipy\n</pre> from damavand.damavand.datasets.downloaders import read_addresses, CwruDownloader from damavand.damavand.datasets.digestors import CWRU from damavand.damavand.signal_processing.transformations import * from damavand.damavand.signal_processing.feature_extraction import * from damavand.damavand.utils import * import pandas as pd import numpy as np import scipy In\u00a0[2]: Copied! <pre>addresses = read_addresses()\ndownloader = CwruDownloader(addresses['CWRU'])\ndownloader.download('CWRU/')\nwhile len(list(downloader.undownloaded.keys())) &gt; 0:\n  downloader.redownload()\n  print(downloader.undownloaded)\n</pre> addresses = read_addresses() downloader = CwruDownloader(addresses['CWRU']) downloader.download('CWRU/') while len(list(downloader.undownloaded.keys())) &gt; 0:   downloader.redownload()   print(downloader.undownloaded) <pre>Downloading: normal_1797_12K.mat\nDownloaded: normal_1797_12K.mat\nDownloading: normal_1772_12K.mat\nDownloaded: normal_1772_12K.mat\nDownloading: normal_1750_12K.mat\nDownloaded: normal_1750_12K.mat\nDownloading: normal_1730_12K.mat\nDownloaded: normal_1730_12K.mat\nDownloading: DE_IR_007_1797_12K.mat\nDownloaded: DE_IR_007_1797_12K.mat\nDownloading: DE_IR_007_1772_12K.mat\nDownloaded: DE_IR_007_1772_12K.mat\nDownloading: DE_IR_007_1750_12K.mat\nDownloaded: DE_IR_007_1750_12K.mat\nDownloading: DE_IR_007_1730_12K.mat\nDownloaded: DE_IR_007_1730_12K.mat\nDownloading: DE_IR_014_1797_12K.mat\nDownloaded: DE_IR_014_1797_12K.mat\nDownloading: DE_IR_014_1772_12K.mat\nDownloaded: DE_IR_014_1772_12K.mat\nDownloading: DE_IR_014_1750_12K.mat\nDownloaded: DE_IR_014_1750_12K.mat\nDownloading: DE_IR_014_1730_12K.mat\nDownloaded: DE_IR_014_1730_12K.mat\nDownloading: DE_IR_021_1797_12K.mat\nDownloaded: DE_IR_021_1797_12K.mat\nDownloading: DE_IR_021_1772_12K.mat\nDownloaded: DE_IR_021_1772_12K.mat\nDownloading: DE_IR_021_1750_12K.mat\nDownloaded: DE_IR_021_1750_12K.mat\nDownloading: DE_IR_021_1730_12K.mat\nDownloaded: DE_IR_021_1730_12K.mat\nDownloading: DE_IR_028_1797_12K.mat\nDownloaded: DE_IR_028_1797_12K.mat\nDownloading: DE_IR_028_1772_12K.mat\nDownloaded: DE_IR_028_1772_12K.mat\nDownloading: DE_IR_028_1750_12K.mat\nDownloaded: DE_IR_028_1750_12K.mat\nDownloading: DE_IR_028_1730_12K.mat\nDownloaded: DE_IR_028_1730_12K.mat\nDownloading: DE_Ball_007_1797_12K.mat\nDownloaded: DE_Ball_007_1797_12K.mat\nDownloading: DE_Ball_007_1772_12K.mat\nDownloaded: DE_Ball_007_1772_12K.mat\nDownloading: DE_Ball_007_1750_12K.mat\nDownloaded: DE_Ball_007_1750_12K.mat\nDownloading: DE_Ball_007_1730_12K.mat\nDownloaded: DE_Ball_007_1730_12K.mat\nDownloading: DE_Ball_014_1797_12K.mat\nDownloaded: DE_Ball_014_1797_12K.mat\nDownloading: DE_Ball_014_1772_12K.mat\nDownloaded: DE_Ball_014_1772_12K.mat\nDownloading: DE_Ball_014_1750_12K.mat\nDownloaded: DE_Ball_014_1750_12K.mat\nDownloading: DE_Ball_014_1730_12K.mat\nDownloaded: DE_Ball_014_1730_12K.mat\nDownloading: DE_Ball_021_1797_12K.mat\nDownloaded: DE_Ball_021_1797_12K.mat\nDownloading: DE_Ball_021_1772_12K.mat\nDownloaded: DE_Ball_021_1772_12K.mat\nDownloading: DE_Ball_021_1750_12K.mat\nDownloaded: DE_Ball_021_1750_12K.mat\nDownloading: DE_Ball_021_1730_12K.mat\nDownloaded: DE_Ball_021_1730_12K.mat\nDownloading: DE_Ball_028_1797_12K.mat\nDownloaded: DE_Ball_028_1797_12K.mat\nDownloading: DE_Ball_028_1772_12K.mat\nDownloaded: DE_Ball_028_1772_12K.mat\nDownloading: DE_Ball_028_1750_12K.mat\nDownloaded: DE_Ball_028_1750_12K.mat\nDownloading: DE_Ball_028_1730_12K.mat\nDownloaded: DE_Ball_028_1730_12K.mat\nDownloading: DE_OR@6_007_1797_12K.mat\nDownloaded: DE_OR@6_007_1797_12K.mat\nDownloading: DE_OR@6_007_1772_12K.mat\nDownloaded: DE_OR@6_007_1772_12K.mat\nDownloading: DE_OR@6_007_1750_12K.mat\nDownloaded: DE_OR@6_007_1750_12K.mat\nDownloading: DE_OR@6_007_1730_12K.mat\nDownloaded: DE_OR@6_007_1730_12K.mat\nDownloading: DE_OR@6_014_1797_12K.mat\nDownloaded: DE_OR@6_014_1797_12K.mat\nDownloading: DE_OR@6_014_1772_12K.mat\nDownloaded: DE_OR@6_014_1772_12K.mat\nDownloading: DE_OR@6_014_1750_12K.mat\nDownloaded: DE_OR@6_014_1750_12K.mat\nDownloading: DE_OR@6_014_1730_12K.mat\nDownloaded: DE_OR@6_014_1730_12K.mat\nDownloading: DE_OR@6_021_1797_12K.mat\nDownloaded: DE_OR@6_021_1797_12K.mat\nDownloading: DE_OR@6_021_1772_12K.mat\nDownloaded: DE_OR@6_021_1772_12K.mat\nDownloading: DE_OR@6_021_1750_12K.mat\nDownloaded: DE_OR@6_021_1750_12K.mat\nDownloading: DE_OR@6_021_1730_12K.mat\nDownloaded: DE_OR@6_021_1730_12K.mat\nDownloading: DE_OR@3_007_1797_12K.mat\nDownloaded: DE_OR@3_007_1797_12K.mat\nDownloading: DE_OR@3_007_1772_12K.mat\nDownloaded: DE_OR@3_007_1772_12K.mat\nDownloading: DE_OR@3_007_1750_12K.mat\nDownloaded: DE_OR@3_007_1750_12K.mat\nDownloading: DE_OR@3_007_1730_12K.mat\nDownloaded: DE_OR@3_007_1730_12K.mat\nDownloading: DE_OR@3_021_1797_12K.mat\nDownloaded: DE_OR@3_021_1797_12K.mat\nDownloading: DE_OR@3_021_1772_12K.mat\nDownloaded: DE_OR@3_021_1772_12K.mat\nDownloading: DE_OR@3_021_1750_12K.mat\nDownloaded: DE_OR@3_021_1750_12K.mat\nDownloading: DE_OR@3_021_1730_12K.mat\nDownloaded: DE_OR@3_021_1730_12K.mat\nDownloading: DE_OR@12_007_1797_12K.mat\nDownloaded: DE_OR@12_007_1797_12K.mat\nDownloading: DE_OR@12_007_1772_12K.mat\nDownloaded: DE_OR@12_007_1772_12K.mat\nDownloading: DE_OR@12_007_1750_12K.mat\nDownloaded: DE_OR@12_007_1750_12K.mat\nDownloading: DE_OR@12_007_1730_12K.mat\nDownloaded: DE_OR@12_007_1730_12K.mat\nDownloading: DE_OR@12_021_1797_12K.mat\nDownloaded: DE_OR@12_021_1797_12K.mat\nDownloading: DE_OR@12_021_1772_12K.mat\nDownloaded: DE_OR@12_021_1772_12K.mat\nDownloading: DE_OR@12_021_1750_12K.mat\nDownloaded: DE_OR@12_021_1750_12K.mat\nDownloading: DE_OR@12_021_1730_12K.mat\nDownloaded: DE_OR@12_021_1730_12K.mat\nDownloading: DE_IR_007_1797_48K.mat\nDownloaded: DE_IR_007_1797_48K.mat\nDownloading: DE_IR_007_1772_48K.mat\nDownloaded: DE_IR_007_1772_48K.mat\nDownloading: DE_IR_007_1750_48K.mat\nDownloaded: DE_IR_007_1750_48K.mat\nDownloading: DE_IR_007_1730_48K.mat\nDownloaded: DE_IR_007_1730_48K.mat\nDownloading: DE_IR_014_1797_48K.mat\nDownloaded: DE_IR_014_1797_48K.mat\nDownloading: DE_IR_014_1772_48K.mat\nDownloaded: DE_IR_014_1772_48K.mat\nDownloading: DE_IR_014_1750_48K.mat\nDownloaded: DE_IR_014_1750_48K.mat\nDownloading: DE_IR_014_1730_48K.mat\nDownloaded: DE_IR_014_1730_48K.mat\nDownloading: DE_IR_021_1797_48K.mat\nDownloaded: DE_IR_021_1797_48K.mat\nDownloading: DE_IR_021_1772_48K.mat\nDownloaded: DE_IR_021_1772_48K.mat\nDownloading: DE_IR_021_1750_48K.mat\nDownloaded: DE_IR_021_1750_48K.mat\nDownloading: DE_IR_021_1730_48K.mat\nDownloaded: DE_IR_021_1730_48K.mat\nDownloading: DE_Ball_007_1797_48K.mat\nDownloaded: DE_Ball_007_1797_48K.mat\nDownloading: DE_Ball_007_1772_48K.mat\nDownloaded: DE_Ball_007_1772_48K.mat\nDownloading: DE_Ball_007_1750_48K.mat\nDownloaded: DE_Ball_007_1750_48K.mat\nDownloading: DE_Ball_007_1730_48K.mat\nDownloaded: DE_Ball_007_1730_48K.mat\nDownloading: DE_Ball_014_1797_48K.mat\nDownloaded: DE_Ball_014_1797_48K.mat\nDownloading: DE_Ball_014_1772_48K.mat\nDownloading: DE_Ball_014_1750_48K.mat\nDownloaded: DE_Ball_014_1750_48K.mat\nDownloading: DE_Ball_014_1730_48K.mat\nDownloaded: DE_Ball_014_1730_48K.mat\nDownloading: DE_Ball_021_1797_48K.mat\nDownloaded: DE_Ball_021_1797_48K.mat\nDownloading: DE_Ball_021_1772_48K.mat\nDownloaded: DE_Ball_021_1772_48K.mat\nDownloading: DE_Ball_021_1750_48K.mat\nDownloaded: DE_Ball_021_1750_48K.mat\nDownloading: DE_Ball_021_1730_48K.mat\nDownloaded: DE_Ball_021_1730_48K.mat\nDownloading: DE_OR@6_007_1797_48K.mat\nDownloaded: DE_OR@6_007_1797_48K.mat\nDownloading: DE_OR@6_007_1772_48K.mat\nDownloaded: DE_OR@6_007_1772_48K.mat\nDownloading: DE_OR@6_007_1750_48K.mat\nDownloaded: DE_OR@6_007_1750_48K.mat\nDownloading: DE_OR@6_007_1730_48K.mat\nDownloaded: DE_OR@6_007_1730_48K.mat\nDownloading: DE_OR@6_014_1797_48K.mat\nDownloaded: DE_OR@6_014_1797_48K.mat\nDownloading: DE_OR@6_014_1772_48K.mat\nDownloaded: DE_OR@6_014_1772_48K.mat\nDownloading: DE_OR@6_014_1750_48K.mat\nDownloaded: DE_OR@6_014_1750_48K.mat\nDownloading: DE_OR@6_014_1730_48K.mat\nDownloaded: DE_OR@6_014_1730_48K.mat\nDownloading: DE_OR@6_021_1797_48K.mat\nDownloaded: DE_OR@6_021_1797_48K.mat\nDownloading: DE_OR@6_021_1772_48K.mat\nDownloaded: DE_OR@6_021_1772_48K.mat\nDownloading: DE_OR@6_021_1750_48K.mat\nDownloaded: DE_OR@6_021_1750_48K.mat\nDownloading: DE_OR@6_021_1730_48K.mat\nDownloaded: DE_OR@6_021_1730_48K.mat\nDownloading: DE_OR@3_007_1797_48K.mat\nDownloaded: DE_OR@3_007_1797_48K.mat\nDownloading: DE_OR@3_007_1772_48K.mat\nDownloaded: DE_OR@3_007_1772_48K.mat\nDownloading: DE_OR@3_007_1750_48K.mat\nDownloaded: DE_OR@3_007_1750_48K.mat\nDownloading: DE_OR@3_007_1730_48K.mat\nDownloaded: DE_OR@3_007_1730_48K.mat\nDownloading: DE_OR@3_021_1797_48K.mat\nDownloaded: DE_OR@3_021_1797_48K.mat\nDownloading: DE_OR@3_021_1772_48K.mat\nDownloaded: DE_OR@3_021_1772_48K.mat\nDownloading: DE_OR@3_021_1750_48K.mat\nDownloaded: DE_OR@3_021_1750_48K.mat\nDownloading: DE_OR@3_021_1730_48K.mat\nDownloaded: DE_OR@3_021_1730_48K.mat\nDownloading: DE_OR@12_007_1797_48K.mat\nDownloaded: DE_OR@12_007_1797_48K.mat\nDownloading: DE_OR@12_007_1772_48K.mat\nDownloaded: DE_OR@12_007_1772_48K.mat\nDownloading: DE_OR@12_007_1750_48K.mat\nDownloaded: DE_OR@12_007_1750_48K.mat\nDownloading: DE_OR@12_007_1730_48K.mat\nDownloaded: DE_OR@12_007_1730_48K.mat\nDownloading: DE_OR@12_021_1797_48K.mat\nDownloaded: DE_OR@12_021_1797_48K.mat\nDownloading: DE_OR@12_021_1772_48K.mat\nDownloaded: DE_OR@12_021_1772_48K.mat\nDownloading: DE_OR@12_021_1750_48K.mat\nDownloaded: DE_OR@12_021_1750_48K.mat\nDownloading: DE_OR@12_021_1730_48K.mat\nDownloaded: DE_OR@12_021_1730_48K.mat\nDownloading: FE_IR_007_1797_12K.mat\nDownloaded: FE_IR_007_1797_12K.mat\nDownloading: FE_IR_007_1772_12K.mat\nDownloaded: FE_IR_007_1772_12K.mat\nDownloading: FE_IR_007_1750_12K.mat\nDownloaded: FE_IR_007_1750_12K.mat\nDownloading: FE_IR_007_1730_12K.mat\nDownloaded: FE_IR_007_1730_12K.mat\nDownloading: FE_IR_014_1797_12K.mat\nDownloaded: FE_IR_014_1797_12K.mat\nDownloading: FE_IR_014_1772_12K.mat\nDownloaded: FE_IR_014_1772_12K.mat\nDownloading: FE_IR_014_1750_12K.mat\nDownloaded: FE_IR_014_1750_12K.mat\nDownloading: FE_IR_014_1730_12K.mat\nDownloaded: FE_IR_014_1730_12K.mat\nDownloading: FE_IR_021_1797_12K.mat\nDownloaded: FE_IR_021_1797_12K.mat\nDownloading: FE_IR_021_1772_12K.mat\nDownloaded: FE_IR_021_1772_12K.mat\nDownloading: FE_IR_021_1750_12K.mat\nDownloaded: FE_IR_021_1750_12K.mat\nDownloading: FE_IR_021_1730_12K.mat\nDownloaded: FE_IR_021_1730_12K.mat\nDownloading: FE_Ball_007_1797_12K.mat\nDownloaded: FE_Ball_007_1797_12K.mat\nDownloading: FE_Ball_007_1772_12K.mat\nDownloaded: FE_Ball_007_1772_12K.mat\nDownloading: FE_Ball_007_1750_12K.mat\nDownloaded: FE_Ball_007_1750_12K.mat\nDownloading: FE_Ball_007_1730_12K.mat\nDownloaded: FE_Ball_007_1730_12K.mat\nDownloading: FE_Ball_014_1797_12K.mat\nDownloaded: FE_Ball_014_1797_12K.mat\nDownloading: FE_Ball_014_1772_12K.mat\nDownloaded: FE_Ball_014_1772_12K.mat\nDownloading: FE_Ball_014_1750_12K.mat\nDownloaded: FE_Ball_014_1750_12K.mat\nDownloading: FE_Ball_014_1730_12K.mat\nDownloaded: FE_Ball_014_1730_12K.mat\nDownloading: FE_Ball_021_1797_12K.mat\nDownloaded: FE_Ball_021_1797_12K.mat\nDownloading: FE_Ball_021_1772_12K.mat\nDownloaded: FE_Ball_021_1772_12K.mat\nDownloading: FE_Ball_021_1750_12K.mat\nDownloaded: FE_Ball_021_1750_12K.mat\nDownloading: FE_Ball_021_1730_12K.mat\nDownloaded: FE_Ball_021_1730_12K.mat\nDownloading: FE_OR@6_007_1797_12K.mat\nDownloaded: FE_OR@6_007_1797_12K.mat\nDownloading: FE_OR@6_007_1772_12K.mat\nDownloaded: FE_OR@6_007_1772_12K.mat\nDownloading: FE_OR@6_007_1750_12K.mat\nDownloaded: FE_OR@6_007_1750_12K.mat\nDownloading: FE_OR@6_007_1730_12K.mat\nDownloaded: FE_OR@6_007_1730_12K.mat\nDownloading: FE_OR@6_014_1797_12K.mat\nDownloaded: FE_OR@6_014_1797_12K.mat\nDownloading: FE_OR@6_021_1797_12K.mat\nDownloaded: FE_OR@6_021_1797_12K.mat\nDownloading: FE_OR@3_007_1797_12K.mat\nDownloaded: FE_OR@3_007_1797_12K.mat\nDownloading: FE_OR@3_007_1772_12K.mat\nDownloaded: FE_OR@3_007_1772_12K.mat\nDownloading: FE_OR@3_007_1750_12K.mat\nDownloaded: FE_OR@3_007_1750_12K.mat\nDownloading: FE_OR@3_007_1730_12K.mat\nDownloaded: FE_OR@3_007_1730_12K.mat\nDownloading: FE_OR@3_014_1797_12K.mat\nDownloaded: FE_OR@3_014_1797_12K.mat\nDownloading: FE_OR@3_014_1772_12K.mat\nDownloaded: FE_OR@3_014_1772_12K.mat\nDownloading: FE_OR@3_014_1750_12K.mat\nDownloaded: FE_OR@3_014_1750_12K.mat\nDownloading: FE_OR@3_014_1730_12K.mat\nDownloaded: FE_OR@3_014_1730_12K.mat\nDownloading: FE_OR@3_021_1772_12K.mat\nDownloaded: FE_OR@3_021_1772_12K.mat\nDownloading: FE_OR@3_021_1750_12K.mat\nDownloaded: FE_OR@3_021_1750_12K.mat\nDownloading: FE_OR@3_021_1730_12K.mat\nDownloaded: FE_OR@3_021_1730_12K.mat\nDownloading: FE_OR@12_007_1797_12K.mat\nDownloaded: FE_OR@12_007_1797_12K.mat\nDownloading: FE_OR@12_007_1772_12K.mat\nDownloaded: FE_OR@12_007_1772_12K.mat\nDownloading: FE_OR@12_007_1750_12K.mat\nDownloaded: FE_OR@12_007_1750_12K.mat\nDownloading: FE_OR@12_007_1730_12K.mat\nDownloaded: FE_OR@12_007_1730_12K.mat\n</pre> In\u00a0[3]: Copied! <pre>mining_params = {\n    '12K': {'win_len': 12000, 'hop_len': 3000},\n    '48K': {'win_len': 48000, 'hop_len': 16000},\n}\n\ncwru = CWRU('CWRU/')\ncwru.mine(mining_params, synchronous_only = True)\n</pre> mining_params = {     '12K': {'win_len': 12000, 'hop_len': 3000},     '48K': {'win_len': 48000, 'hop_len': 16000}, }  cwru = CWRU('CWRU/') cwru.mine(mining_params, synchronous_only = True) In\u00a0[4]: Copied! <pre>cwru.data.keys()\n</pre> cwru.data.keys() Out[4]: <pre>dict_keys(['FE', 'DE'])</pre> In\u00a0[5]: Copied! <pre>cwru.data['FE'].keys()\n</pre> cwru.data['FE'].keys() Out[5]: <pre>dict_keys(['12K', '48K'])</pre> In\u00a0[6]: Copied! <pre>print(len(cwru.data['FE']['12K']), ' - ', len(cwru.data['FE']['48K']), '\\n',\n      len(cwru.data['DE']['12K']), ' - ', len(cwru.data['DE']['48K']))\n</pre> print(len(cwru.data['FE']['12K']), ' - ', len(cwru.data['FE']['48K']), '\\n',       len(cwru.data['DE']['12K']), ' - ', len(cwru.data['DE']['48K'])) <pre>101  -  52 \n 101  -  52\n</pre> In\u00a0[7]: Copied! <pre>df = pd.concat(cwru.data['DE']['12K']).reset_index(drop = True)\ndf\n</pre> df = pd.concat(cwru.data['DE']['12K']).reset_index(drop = True) df Out[7]: 0 1 2 3 4 5 6 7 8 9 ... 11995 11996 11997 11998 11999 state defected_bearing severity rot_speed fs 0 1.189431 -0.177866 -0.774816 0.501518 0.993697 -0.348017 -0.811363 0.424362 0.988012 0.089339 ... -0.520605 -0.530757 0.244465 0.147410 -0.780907 IR DE 021 1797 12K 1 -0.068629 0.294414 0.136852 -0.341114 -0.238780 0.232282 0.006903 -0.444260 0.123857 0.719994 ... -0.023147 -0.305784 -0.127512 0.177866 0.084466 IR DE 021 1797 12K 2 0.770755 -0.434920 -0.674918 0.679791 1.179685 -0.180303 -0.758978 0.390656 0.934408 -0.152283 ... 0.437763 -0.450351 -0.537254 0.128730 -0.076751 IR DE 021 1797 12K 3 0.034517 -0.164466 0.175024 0.291977 -0.102740 -0.330555 -0.026802 0.242841 -0.049137 -0.370758 ... 0.015431 -0.360606 -0.196140 0.165684 -0.100710 IR DE 021 1797 12K 4 -0.418677 0.707811 0.378068 -0.569741 -0.200201 1.194710 0.927911 -0.470250 -0.257866 0.823140 ... 0.858064 0.634309 -0.640400 -0.449133 0.775628 IR DE 021 1797 12K ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 4136 -0.022532 -0.014940 0.090820 0.108805 -0.032479 -0.060330 0.016889 0.028866 -0.029800 -0.066176 ... 0.065242 0.048231 0.085988 0.052819 0.029312 OR@3 FE 007 1797 12K 4137 0.008851 -0.059640 -0.084851 0.049368 0.038650 -0.020178 0.076204 0.141162 0.019650 -0.014940 ... -0.003694 0.164182 -0.046567 -0.244283 0.038082 OR@3 FE 007 1797 12K 4138 0.006171 -0.011571 -0.031667 0.037960 0.053550 -0.010596 -0.019893 0.131581 0.111850 -0.109048 ... -0.041208 0.034225 0.133489 0.028176 -0.085339 OR@3 FE 007 1797 12K 4139 0.004385 -0.025983 0.018066 0.015752 -0.043197 -0.122730 -0.086881 -0.064146 -0.095691 -0.063537 ... -0.210627 0.015022 0.143436 0.007633 -0.112824 OR@3 FE 007 1797 12K 4140 0.008729 -0.043116 -0.015062 -0.029272 -0.063821 -0.007470 0.037310 0.038082 -0.003491 -0.010190 ... -0.039909 -0.067394 0.090860 0.116762 -0.100929 OR@3 FE 007 1797 12K <p>4141 rows \u00d7 12005 columns</p> In\u00a0[8]: Copied! <pre>signals, metadata = df.iloc[:, : -5], df.iloc[:, -5 :]\nsignals\n</pre> signals, metadata = df.iloc[:, : -5], df.iloc[:, -5 :] signals Out[8]: 0 1 2 3 4 5 6 7 8 9 ... 11990 11991 11992 11993 11994 11995 11996 11997 11998 11999 0 1.189431 -0.177866 -0.774816 0.501518 0.993697 -0.348017 -0.811363 0.424362 0.988012 0.089339 ... 0.673294 -0.183552 -0.681415 0.108425 0.295632 -0.520605 -0.530757 0.244465 0.147410 -0.780907 1 -0.068629 0.294414 0.136852 -0.341114 -0.238780 0.232282 0.006903 -0.444260 0.123857 0.719994 ... 0.040609 0.207105 -0.270048 -0.426392 -0.032081 -0.023147 -0.305784 -0.127512 0.177866 0.084466 2 0.770755 -0.434920 -0.674918 0.679791 1.179685 -0.180303 -0.758978 0.390656 0.934408 -0.152283 ... 0.262333 0.704968 -0.175836 -0.538879 0.260708 0.437763 -0.450351 -0.537254 0.128730 -0.076751 3 0.034517 -0.164466 0.175024 0.291977 -0.102740 -0.330555 -0.026802 0.242841 -0.049137 -0.370758 ... 0.046294 0.249744 -0.306190 -0.383347 0.082030 0.015431 -0.360606 -0.196140 0.165684 -0.100710 4 -0.418677 0.707811 0.378068 -0.569741 -0.200201 1.194710 0.927911 -0.470250 -0.257866 0.823140 ... 0.104771 0.671669 0.063756 -0.820297 -0.407306 0.858064 0.634309 -0.640400 -0.449133 0.775628 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 4136 -0.022532 -0.014940 0.090820 0.108805 -0.032479 -0.060330 0.016889 0.028866 -0.029800 -0.066176 ... 0.007633 -0.045186 0.006496 0.002111 0.030246 0.065242 0.048231 0.085988 0.052819 0.029312 4137 0.008851 -0.059640 -0.084851 0.049368 0.038650 -0.020178 0.076204 0.141162 0.019650 -0.014940 ... -0.068815 -0.074702 0.162720 0.086638 -0.153098 -0.003694 0.164182 -0.046567 -0.244283 0.038082 4138 0.006171 -0.011571 -0.031667 0.037960 0.053550 -0.010596 -0.019893 0.131581 0.111850 -0.109048 ... -0.081076 -0.023750 -0.060452 -0.018310 0.035240 -0.041208 0.034225 0.133489 0.028176 -0.085339 4139 0.004385 -0.025983 0.018066 0.015752 -0.043197 -0.122730 -0.086881 -0.064146 -0.095691 -0.063537 ... 0.072347 -0.177092 -0.102553 0.174291 0.009825 -0.210627 0.015022 0.143436 0.007633 -0.112824 4140 0.008729 -0.043116 -0.015062 -0.029272 -0.063821 -0.007470 0.037310 0.038082 -0.003491 -0.010190 ... 0.012098 0.009216 0.012626 0.070764 0.056067 -0.039909 -0.067394 0.090860 0.116762 -0.100929 <p>4141 rows \u00d7 12000 columns</p> In\u00a0[9]: Copied! <pre>metadata\n</pre> metadata Out[9]: state defected_bearing severity rot_speed fs 0 IR DE 021 1797 12K 1 IR DE 021 1797 12K 2 IR DE 021 1797 12K 3 IR DE 021 1797 12K 4 IR DE 021 1797 12K ... ... ... ... ... ... 4136 OR@3 FE 007 1797 12K 4137 OR@3 FE 007 1797 12K 4138 OR@3 FE 007 1797 12K 4139 OR@3 FE 007 1797 12K 4140 OR@3 FE 007 1797 12K <p>4141 rows \u00d7 5 columns</p> In\u00a0[10]: Copied! <pre>metadata['comb'] = metadata['state'] + '_' + metadata['defected_bearing'] + '_' + metadata['severity'] + '_' + metadata['rot_speed']\nmetadata['comb'].value_counts()\n</pre> metadata['comb'] = metadata['state'] + '_' + metadata['defected_bearing'] + '_' + metadata['severity'] + '_' + metadata['rot_speed'] metadata['comb'].value_counts() Out[10]: count comb normal_-_-_1750 158 normal_-_-_1730 158 normal_-_-_1772 158 normal_-_-_1797 78 IR_DE_021_1797 37 ... ... Ball_FE_021_1730 37 IR_DE_014_1750 37 OR@6_DE_021_1772 37 Ball_FE_014_1750 37 OR@3_FE_007_1797 37 <p>101 rows \u00d7 1 columns</p> dtype: int64 In\u00a0[11]: Copied! <pre>signals_env = env(signals)\nsignals_env.shape\n</pre> signals_env = env(signals) signals_env.shape Out[11]: <pre>(4141, 12000)</pre> In\u00a0[12]: Copied! <pre>window = scipy.signal.windows.hann(signals_env.shape[1])\nfreq_filter = scipy.signal.butter(25, [5, 5500], 'bandpass', fs = 12000, output='sos')\n</pre> window = scipy.signal.windows.hann(signals_env.shape[1]) freq_filter = scipy.signal.butter(25, [5, 5500], 'bandpass', fs = 12000, output='sos') In\u00a0[13]: Copied! <pre>signals_env_fft = fft(signals_env, freq_filter = freq_filter, window = window)\nsignals_env_fft.shape\n</pre> signals_env_fft = fft(signals_env, freq_filter = freq_filter, window = window) signals_env_fft.shape Out[13]: <pre>(4141, 6000)</pre> In\u00a0[14]: Copied! <pre>signals_env_ZoomedFFT = zoomed_fft(signals_env, 0, 1000, 2000, 12000, freq_filter = freq_filter, window = window)\nsignals_env_ZoomedFFT.shape\n</pre> signals_env_ZoomedFFT = zoomed_fft(signals_env, 0, 1000, 2000, 12000, freq_filter = freq_filter, window = window) signals_env_ZoomedFFT.shape Out[14]: <pre>(4141, 2000)</pre> In\u00a0[15]: Copied! <pre>STFT_window = scipy.signal.windows.hann(1024)\nSTFT_freq_filter = scipy.signal.butter(25, [5, 5500], 'bandpass', fs = 12000, output='sos')\nsignals_env_STFT = stft(signals_env, 1024, 200, STFT_freq_filter, STFT_window)\nsignals_env_STFT.shape\n</pre> STFT_window = scipy.signal.windows.hann(1024) STFT_freq_filter = scipy.signal.butter(25, [5, 5500], 'bandpass', fs = 12000, output='sos') signals_env_STFT = stft(signals_env, 1024, 200, STFT_freq_filter, STFT_window) signals_env_STFT.shape Out[15]: <pre>(4141, 55, 512)</pre> In\u00a0[16]: Copied! <pre># Defining the feature-set to be extracted\nfeatures = {'mean': (np.mean, (), {}), 'var': (np.var, (), {}), 'rms': (rms, (), {})}\n</pre> # Defining the feature-set to be extracted features = {'mean': (np.mean, (), {}), 'var': (np.var, (), {}), 'rms': (rms, (), {})} In\u00a0[17]: Copied! <pre>features_df = feature_extractor(signals, features)\nfeatures_df\n</pre> features_df = feature_extractor(signals, features) features_df Out[17]: mean var rms 0 0.019769 0.267842 0.517912 1 0.019584 0.261590 0.511833 2 0.019419 0.260970 0.511221 3 0.019047 0.258746 0.509027 4 0.018798 0.254876 0.505202 ... ... ... ... 4136 0.002500 0.007554 0.086951 4137 0.002516 0.007271 0.085306 4138 0.002493 0.007095 0.084266 4139 0.002426 0.007193 0.084844 4140 0.002341 0.007462 0.086413 <p>4141 rows \u00d7 3 columns</p> In\u00a0[18]: Copied! <pre>import seaborn as sns\nfrom matplotlib import pyplot as plt\nsns.set()\n\nfig, axes = plt.subplots(4, 1, figsize = (16, 10))\n\nsns.lineplot(ax=axes[0], x=range(len(signals.iloc[0,:])), y = signals.iloc[0,:])\naxes[0].set_title(\"Original Time Signal\")\naxes[0].set_ylabel(\"Amplitude\")\naxes[0].set_xlabel(\"sample\")\naxes[0].set_xlim(0, 12000)\n\nsns.lineplot(ax=axes[1], x=range(len(signals_env.iloc[0,:])), y = signals_env.iloc[0, :])\naxes[1].set_title(\"Envelope\")\naxes[1].set_ylabel(\"Amplitude\")\naxes[1].set_xlabel(\"sample\")\naxes[1].set_xlim(0, 12000)\n\n\nsns.lineplot(ax=axes[2], x = fft_freq_axis(12000, 12000), y = signals_env_fft.iloc[0, :])\naxes[2].set_title(\"FFT\")\naxes[2].set_ylabel(\"Amplitude\")\naxes[2].set_xlabel(\"Frequency (Hz)\")\naxes[2].set_xlim(0, 6000)\n\n\nsns.lineplot(ax=axes[3], x = zoomed_fft_freq_axis(0, 1000, 2000), y = signals_env_ZoomedFFT.iloc[0, :])\naxes[3].set_title(\"Zoomed FFT\")\naxes[3].set_ylabel(\"Amplitude\")\naxes[3].set_xlabel(\"Frequency (Hz)\")\naxes[3].set_xlim(0, 1000)\n\n\nplt.subplots_adjust(hspace = 0.75)\nfig.show()\n</pre> import seaborn as sns from matplotlib import pyplot as plt sns.set()  fig, axes = plt.subplots(4, 1, figsize = (16, 10))  sns.lineplot(ax=axes[0], x=range(len(signals.iloc[0,:])), y = signals.iloc[0,:]) axes[0].set_title(\"Original Time Signal\") axes[0].set_ylabel(\"Amplitude\") axes[0].set_xlabel(\"sample\") axes[0].set_xlim(0, 12000)  sns.lineplot(ax=axes[1], x=range(len(signals_env.iloc[0,:])), y = signals_env.iloc[0, :]) axes[1].set_title(\"Envelope\") axes[1].set_ylabel(\"Amplitude\") axes[1].set_xlabel(\"sample\") axes[1].set_xlim(0, 12000)   sns.lineplot(ax=axes[2], x = fft_freq_axis(12000, 12000), y = signals_env_fft.iloc[0, :]) axes[2].set_title(\"FFT\") axes[2].set_ylabel(\"Amplitude\") axes[2].set_xlabel(\"Frequency (Hz)\") axes[2].set_xlim(0, 6000)   sns.lineplot(ax=axes[3], x = zoomed_fft_freq_axis(0, 1000, 2000), y = signals_env_ZoomedFFT.iloc[0, :]) axes[3].set_title(\"Zoomed FFT\") axes[3].set_ylabel(\"Amplitude\") axes[3].set_xlabel(\"Frequency (Hz)\") axes[3].set_xlim(0, 1000)   plt.subplots_adjust(hspace = 0.75) fig.show() In\u00a0[19]: Copied! <pre>t = np.linspace(0, 1, 55)\nf = fft_freq_axis(1024, 12000)\n\nfig, ax = plt.subplots(figsize = (16, 8))\n\nax = sns.heatmap(signals_env_STFT[0, :, :], xticklabels = np.round(f, decimals = 2), yticklabels = np.round(t, decimals = 2), annot = False, cbar = False)\nax.set(xlabel = 'Frequency (Hz)', ylabel = 'Time (sec)')\nax.set_title('STFT')\nax.set_xticks(ax.get_xticks()[::20])\nax.set_yticks(ax.get_yticks()[::5])\n\n\nfig.show()\n</pre> t = np.linspace(0, 1, 55) f = fft_freq_axis(1024, 12000)  fig, ax = plt.subplots(figsize = (16, 8))  ax = sns.heatmap(signals_env_STFT[0, :, :], xticklabels = np.round(f, decimals = 2), yticklabels = np.round(t, decimals = 2), annot = False, cbar = False) ax.set(xlabel = 'Frequency (Hz)', ylabel = 'Time (sec)') ax.set_title('STFT') ax.set_xticks(ax.get_xticks()[::20]) ax.set_yticks(ax.get_yticks()[::5])   fig.show() In\u00a0[19]: Copied! <pre>\n</pre>"},{"location":"notebooks/dataset_demos/CWRU_demo/#cloning-the-damavand-repository","title":"Cloning the damavand repository\u00b6","text":""},{"location":"notebooks/dataset_demos/CWRU_demo/#importings","title":"Importings\u00b6","text":""},{"location":"notebooks/dataset_demos/CWRU_demo/#instantiating-a-downloader-object","title":"Instantiating a downloader object\u00b6","text":""},{"location":"notebooks/dataset_demos/CWRU_demo/#instantiating-a-digestor-object","title":"Instantiating a digestor object\u00b6","text":""},{"location":"notebooks/dataset_demos/CWRU_demo/#aggregating-drive-end-data-recorded-at-12-khz","title":"Aggregating Drive-End data recorded at 12 kHz\u00b6","text":""},{"location":"notebooks/dataset_demos/CWRU_demo/#signals-metadata-declaration","title":"Signals-Metadata declaration\u00b6","text":""},{"location":"notebooks/dataset_demos/CWRU_demo/#signal-processing","title":"Signal Processing\u00b6","text":""},{"location":"notebooks/dataset_demos/CWRU_demo/#envelope-extraction","title":"Envelope Extraction\u00b6","text":""},{"location":"notebooks/dataset_demos/CWRU_demo/#fft","title":"FFT\u00b6","text":""},{"location":"notebooks/dataset_demos/CWRU_demo/#zoomed-fft","title":"Zoomed FFT\u00b6","text":""},{"location":"notebooks/dataset_demos/CWRU_demo/#stft","title":"STFT\u00b6","text":""},{"location":"notebooks/dataset_demos/CWRU_demo/#statistical-features","title":"Statistical Features\u00b6","text":""},{"location":"notebooks/dataset_demos/CWRU_demo/#visualization","title":"Visualization\u00b6","text":""},{"location":"notebooks/dataset_demos/KAIST_demo/","title":"Cloning the damavand repository","text":"In\u00a0[1]: Copied! <pre>!git clone https://github.com/amirberenji1995/damavand\n</pre> !git clone https://github.com/amirberenji1995/damavand <pre>Cloning into 'damavand'...\nremote: Enumerating objects: 263, done.\nremote: Counting objects: 100% (263/263), done.\nremote: Compressing objects: 100% (197/197), done.\nremote: Total 263 (delta 132), reused 196 (delta 65), pack-reused 0 (from 0)\nReceiving objects: 100% (263/263), 6.62 MiB | 9.53 MiB/s, done.\nResolving deltas: 100% (132/132), done.\n</pre> In\u00a0[2]: Copied! <pre>!pip install -r damavand/requirements.txt\n</pre> !pip install -r damavand/requirements.txt <pre>Collecting certifi==2024.7.4 (from -r damavand/requirements.txt (line 1))\n  Downloading certifi-2024.7.4-py3-none-any.whl.metadata (2.2 kB)\nCollecting charset-normalizer==3.3.2 (from -r damavand/requirements.txt (line 2))\n  Downloading charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\nCollecting idna==3.7 (from -r damavand/requirements.txt (line 3))\n  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\nCollecting numpy==1.26.4 (from -r damavand/requirements.txt (line 4))\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 61.0/61.0 kB 3.9 MB/s eta 0:00:00\nCollecting pandas==2.1.4 (from -r damavand/requirements.txt (line 5))\n  Downloading pandas-2.1.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\nRequirement already satisfied: python-dateutil==2.9.0.post0 in /usr/local/lib/python3.11/dist-packages (from -r damavand/requirements.txt (line 6)) (2.9.0.post0)\nCollecting pytz==2024.1 (from -r damavand/requirements.txt (line 7))\n  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\nCollecting rarfile==4.2 (from -r damavand/requirements.txt (line 8))\n  Downloading rarfile-4.2-py3-none-any.whl.metadata (4.4 kB)\nRequirement already satisfied: requests==2.32.3 in /usr/local/lib/python3.11/dist-packages (from -r damavand/requirements.txt (line 9)) (2.32.3)\nCollecting scipy==1.13.1 (from -r damavand/requirements.txt (line 10))\n  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 60.6/60.6 kB 5.0 MB/s eta 0:00:00\nCollecting six==1.16.0 (from -r damavand/requirements.txt (line 11))\n  Downloading six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\nCollecting tzdata==2024.1 (from -r damavand/requirements.txt (line 12))\n  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\nCollecting urllib3==2.2.2 (from -r damavand/requirements.txt (line 13))\n  Downloading urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\nDownloading certifi-2024.7.4-py3-none-any.whl (162 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 163.0/163.0 kB 11.9 MB/s eta 0:00:00\nDownloading charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (140 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 140.3/140.3 kB 12.6 MB/s eta 0:00:00\nDownloading idna-3.7-py3-none-any.whl (66 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 66.8/66.8 kB 5.9 MB/s eta 0:00:00\nDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 88.7 MB/s eta 0:00:00\nDownloading pandas-2.1.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 12.2/12.2 MB 104.0 MB/s eta 0:00:00\nDownloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 505.5/505.5 kB 35.7 MB/s eta 0:00:00\nDownloading rarfile-4.2-py3-none-any.whl (29 kB)\nDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 38.6/38.6 MB 15.3 MB/s eta 0:00:00\nDownloading six-1.16.0-py2.py3-none-any.whl (11 kB)\nDownloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 345.4/345.4 kB 27.6 MB/s eta 0:00:00\nDownloading urllib3-2.2.2-py3-none-any.whl (121 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 121.4/121.4 kB 9.2 MB/s eta 0:00:00\nInstalling collected packages: pytz, urllib3, tzdata, six, rarfile, numpy, idna, charset-normalizer, certifi, scipy, pandas\n  Attempting uninstall: pytz\n    Found existing installation: pytz 2025.2\n    Uninstalling pytz-2025.2:\n      Successfully uninstalled pytz-2025.2\n  Attempting uninstall: urllib3\n    Found existing installation: urllib3 2.5.0\n    Uninstalling urllib3-2.5.0:\n      Successfully uninstalled urllib3-2.5.0\n  Attempting uninstall: tzdata\n    Found existing installation: tzdata 2025.2\n    Uninstalling tzdata-2025.2:\n      Successfully uninstalled tzdata-2025.2\n  Attempting uninstall: six\n    Found existing installation: six 1.17.0\n    Uninstalling six-1.17.0:\n      Successfully uninstalled six-1.17.0\n  Attempting uninstall: numpy\n    Found existing installation: numpy 2.0.2\n    Uninstalling numpy-2.0.2:\n      Successfully uninstalled numpy-2.0.2\n  Attempting uninstall: idna\n    Found existing installation: idna 3.10\n    Uninstalling idna-3.10:\n      Successfully uninstalled idna-3.10\n  Attempting uninstall: charset-normalizer\n    Found existing installation: charset-normalizer 3.4.2\n    Uninstalling charset-normalizer-3.4.2:\n      Successfully uninstalled charset-normalizer-3.4.2\n  Attempting uninstall: certifi\n    Found existing installation: certifi 2025.7.14\n    Uninstalling certifi-2025.7.14:\n      Successfully uninstalled certifi-2025.7.14\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.16.0\n    Uninstalling scipy-1.16.0:\n      Successfully uninstalled scipy-1.16.0\n  Attempting uninstall: pandas\n    Found existing installation: pandas 2.2.2\n    Uninstalling pandas-2.2.2:\n      Successfully uninstalled pandas-2.2.2\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.1.4 which is incompatible.\nopencv-python 4.12.0.88 requires numpy&lt;2.3.0,&gt;=2; python_version &gt;= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy&lt;2.3.0,&gt;=2; python_version &gt;= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy&lt;2.3.0,&gt;=2; python_version &gt;= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nplotnine 0.14.5 requires pandas&gt;=2.2.0, but you have pandas 2.1.4 which is incompatible.\ntsfresh 0.21.0 requires scipy&gt;=1.14.0; python_version &gt;= \"3.10\", but you have scipy 1.13.1 which is incompatible.\nthinc 8.3.6 requires numpy&lt;3.0.0,&gt;=2.0.0, but you have numpy 1.26.4 which is incompatible.\nxarray 2025.7.1 requires pandas&gt;=2.2, but you have pandas 2.1.4 which is incompatible.\nmizani 0.13.5 requires pandas&gt;=2.2.0, but you have pandas 2.1.4 which is incompatible.\nSuccessfully installed certifi-2024.7.4 charset-normalizer-3.3.2 idna-3.7 numpy-1.26.4 pandas-2.1.4 pytz-2024.1 rarfile-4.2 scipy-1.13.1 six-1.16.0 tzdata-2024.1 urllib3-2.2.2\n</pre> In\u00a0[1]: Copied! <pre>from damavand.damavand.datasets.downloaders import read_addresses, ZipDatasetDownloader\nfrom damavand.damavand.datasets.digestors import KAIST\nfrom damavand.damavand.signal_processing.transformations import *\nfrom damavand.damavand.signal_processing.feature_extraction import *\nfrom damavand.damavand.utils import *\nimport os\nimport pandas as pd\nimport numpy as np\nimport scipy\n</pre> from damavand.damavand.datasets.downloaders import read_addresses, ZipDatasetDownloader from damavand.damavand.datasets.digestors import KAIST from damavand.damavand.signal_processing.transformations import * from damavand.damavand.signal_processing.feature_extraction import * from damavand.damavand.utils import * import os import pandas as pd import numpy as np import scipy In\u00a0[2]: Copied! <pre>addresses = read_addresses()\ndownloader = ZipDatasetDownloader(addresses['KAIST'])\ndownloader.download_extract('KAIST.zip', 'KAIST/')\n</pre> addresses = read_addresses() downloader = ZipDatasetDownloader(addresses['KAIST']) downloader.download_extract('KAIST.zip', 'KAIST/') In\u00a0[3]: Copied! <pre># using only two channels out of four available ones to avoid RAM oveflow\nkaist = KAIST('KAIST/', os.listdir('KAIST/'), [0, 1])\nmining_params = {\n    'win_len': 20000,\n    'hop_len': 20000,\n}\nkaist.mine(mining_params)\n</pre> # using only two channels out of four available ones to avoid RAM oveflow kaist = KAIST('KAIST/', os.listdir('KAIST/'), [0, 1]) mining_params = {     'win_len': 20000,     'hop_len': 20000, } kaist.mine(mining_params) In\u00a0[4]: Copied! <pre>df = pd.concat(kaist.data[0]).reset_index(drop = True)\ndf\n</pre> df = pd.concat(kaist.data[0]).reset_index(drop = True) df Out[4]: 0 1 2 3 4 5 6 7 8 9 ... 19993 19994 19995 19996 19997 19998 19999 load state severity 0 -1.944207 -1.869208 -0.700953 0.622949 1.509236 2.044446 2.168603 1.390728 1.435319 1.199144 ... 0.760928 0.598310 0.039182 -1.434117 -2.607060 -3.098760 -3.737334 0 Unbalance 1751mg 1 -3.483491 -2.891311 -1.911275 0.165863 1.687359 2.457542 3.103087 3.639138 2.771841 2.732539 ... -0.646987 -0.931479 -0.819702 -0.619944 -0.537013 -0.696746 -0.201440 0 Unbalance 1751mg 2 -1.211644 -0.885927 -0.350116 -0.703717 -0.046273 0.956839 0.804317 1.513203 2.280862 0.980156 ... -0.457927 -1.216331 -1.920289 -2.103460 -0.397712 -0.361173 -0.157210 0 Unbalance 1751mg 3 0.037139 -0.502879 -0.406245 0.688573 0.127522 0.873908 0.577036 -1.399622 -1.025468 0.224156 ... 0.620305 -0.664295 0.557926 0.104205 -0.254805 -1.375945 -1.342291 0 Unbalance 1751mg 4 -1.445535 -1.056838 0.697708 1.074386 0.055047 0.572469 0.648790 -0.162378 0.219108 0.835326 ... 0.436052 0.254324 0.942416 1.173784 0.757202 0.885446 0.511292 0 Unbalance 1751mg ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 5725 11.029553 9.238710 2.267160 -5.447049 -4.526868 -4.117498 -9.404333 -6.117354 -0.513335 -0.738573 ... -12.613789 -2.490475 4.834797 10.946742 22.474607 20.914770 7.714329 2 BPFI 10 5726 2.758740 -6.649919 -21.230511 -22.532659 -16.486819 -13.761852 -13.621950 3.862814 11.202989 9.215032 ... -20.135813 -20.691696 -17.724304 11.103351 29.171400 40.736885 27.580194 2 BPFI 10 5727 29.922353 17.379957 -6.027451 -10.810926 -24.749339 -33.270028 -26.654243 -18.569245 -2.376173 13.680843 ... -1.945048 0.506725 6.842345 6.426364 4.083003 7.537048 8.481748 2 BPFI 10 5728 -0.829317 -2.394081 -2.869556 -7.333324 -7.723223 -4.031201 -2.693116 -0.980757 0.632204 5.181908 ... 3.109698 -1.915962 -12.598885 -13.542744 -7.100995 -5.017727 -6.237304 2 BPFI 10 5729 -1.093737 2.293722 -0.871985 -4.953305 -1.556231 -5.444405 -13.618464 -6.238866 -0.510931 -2.013437 ... -1.669090 5.386834 10.794941 1.546616 -9.350367 -1.623177 -6.441388 2 BPFI 10 <p>5730 rows \u00d7 20003 columns</p> In\u00a0[5]: Copied! <pre>signals, metadata = df.iloc[:, : - 3], df.iloc[:, - 3 :]\nsignals\n</pre> signals, metadata = df.iloc[:, : - 3], df.iloc[:, - 3 :] signals Out[5]: 0 1 2 3 4 5 6 7 8 9 ... 19990 19991 19992 19993 19994 19995 19996 19997 19998 19999 0 -1.944207 -1.869208 -0.700953 0.622949 1.509236 2.044446 2.168603 1.390728 1.435319 1.199144 ... 1.392050 2.207545 1.707672 0.760928 0.598310 0.039182 -1.434117 -2.607060 -3.098760 -3.737334 1 -3.483491 -2.891311 -1.911275 0.165863 1.687359 2.457542 3.103087 3.639138 2.771841 2.732539 ... 1.688441 1.155154 -0.698068 -0.646987 -0.931479 -0.819702 -0.619944 -0.537013 -0.696746 -0.201440 2 -1.211644 -0.885927 -0.350116 -0.703717 -0.046273 0.956839 0.804317 1.513203 2.280862 0.980156 ... 0.878355 1.734955 0.886047 -0.457927 -1.216331 -1.920289 -2.103460 -0.397712 -0.361173 -0.157210 3 0.037139 -0.502879 -0.406245 0.688573 0.127522 0.873908 0.577036 -1.399622 -1.025468 0.224156 ... 1.915602 0.994098 1.689403 0.620305 -0.664295 0.557926 0.104205 -0.254805 -1.375945 -1.342291 4 -1.445535 -1.056838 0.697708 1.074386 0.055047 0.572469 0.648790 -0.162378 0.219108 0.835326 ... -0.680641 -0.403120 0.002284 0.436052 0.254324 0.942416 1.173784 0.757202 0.885446 0.511292 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 5725 11.029553 9.238710 2.267160 -5.447049 -4.526868 -4.117498 -9.404333 -6.117354 -0.513335 -0.738573 ... -5.380103 -13.323276 -15.883461 -12.613789 -2.490475 4.834797 10.946742 22.474607 20.914770 7.714329 5726 2.758740 -6.649919 -21.230511 -22.532659 -16.486819 -13.761852 -13.621950 3.862814 11.202989 9.215032 ... -11.592527 -30.372708 -32.979407 -20.135813 -20.691696 -17.724304 11.103351 29.171400 40.736885 27.580194 5727 29.922353 17.379957 -6.027451 -10.810926 -24.749339 -33.270028 -26.654243 -18.569245 -2.376173 13.680843 ... -8.085358 -3.086140 -1.875097 -1.945048 0.506725 6.842345 6.426364 4.083003 7.537048 8.481748 5728 -0.829317 -2.394081 -2.869556 -7.333324 -7.723223 -4.031201 -2.693116 -0.980757 0.632204 5.181908 ... 24.585999 19.187387 7.661205 3.109698 -1.915962 -12.598885 -13.542744 -7.100995 -5.017727 -6.237304 5729 -1.093737 2.293722 -0.871985 -4.953305 -1.556231 -5.444405 -13.618464 -6.238866 -0.510931 -2.013437 ... -18.894842 5.014963 10.367302 -1.669090 5.386834 10.794941 1.546616 -9.350367 -1.623177 -6.441388 <p>5730 rows \u00d7 20000 columns</p> In\u00a0[6]: Copied! <pre>metadata\n</pre> metadata Out[6]: load state severity 0 0 Unbalance 1751mg 1 0 Unbalance 1751mg 2 0 Unbalance 1751mg 3 0 Unbalance 1751mg 4 0 Unbalance 1751mg ... ... ... ... 5725 2 BPFI 10 5726 2 BPFI 10 5727 2 BPFI 10 5728 2 BPFI 10 5729 2 BPFI 10 <p>5730 rows \u00d7 3 columns</p> In\u00a0[7]: Copied! <pre>signals_env = env(signals)\nsignals_env.shape\n</pre> signals_env = env(signals) signals_env.shape Out[7]: <pre>(5730, 20000)</pre> In\u00a0[8]: Copied! <pre># Defenition of a window (to avoid leakage error) and a bandpass frequency filter (to both avoid aliasing and DC-component removal)\nwindow = scipy.signal.windows.hann(signals_env.shape[1])\nfreq_filter = scipy.signal.butter(25, [5, 12500], 'bandpass', fs = 25600, output='sos')\n</pre> # Defenition of a window (to avoid leakage error) and a bandpass frequency filter (to both avoid aliasing and DC-component removal) window = scipy.signal.windows.hann(signals_env.shape[1]) freq_filter = scipy.signal.butter(25, [5, 12500], 'bandpass', fs = 25600, output='sos') In\u00a0[9]: Copied! <pre>signals_env_fft = fft(signals_env, freq_filter = freq_filter, window = window)\nsignals_env_fft.shape\n</pre> signals_env_fft = fft(signals_env, freq_filter = freq_filter, window = window) signals_env_fft.shape Out[9]: <pre>(5730, 10000)</pre> In\u00a0[10]: Copied! <pre>signals_env_ZoomedFFT = zoomed_fft(signals_env, 0, 2500, 2500, 25600, freq_filter = freq_filter, window = window)\nsignals_env_ZoomedFFT.shape\n</pre> signals_env_ZoomedFFT = zoomed_fft(signals_env, 0, 2500, 2500, 25600, freq_filter = freq_filter, window = window) signals_env_ZoomedFFT.shape Out[10]: <pre>(5730, 2500)</pre> In\u00a0[11]: Copied! <pre># Defenition of a window (to avoid leakage error) and a bandpass frequency filter (to both avoid aliasing and DC-component removal)\nSTFT_window = scipy.signal.windows.hann(4000) # the length of the window must match the `window_len` argument from the stft function\nSTFT_freq_filter = scipy.signal.butter(25, [5, 12500], 'bandpass', fs = 25600, output='sos')\n</pre> # Defenition of a window (to avoid leakage error) and a bandpass frequency filter (to both avoid aliasing and DC-component removal) STFT_window = scipy.signal.windows.hann(4000) # the length of the window must match the `window_len` argument from the stft function STFT_freq_filter = scipy.signal.butter(25, [5, 12500], 'bandpass', fs = 25600, output='sos') In\u00a0[12]: Copied! <pre>signals_env_STFT = stft(signals_env, 4000, 2000, STFT_freq_filter, STFT_window)\nsignals_env_STFT.shape\n</pre> signals_env_STFT = stft(signals_env, 4000, 2000, STFT_freq_filter, STFT_window) signals_env_STFT.shape Out[12]: <pre>(5730, 9, 2000)</pre> In\u00a0[13]: Copied! <pre># Defining the feature-set to be extracted\nfeatures = {'mean': (np.mean, (), {}), 'var': (np.var, (), {}), 'rms': (rms, (), {})}\n</pre> # Defining the feature-set to be extracted features = {'mean': (np.mean, (), {}), 'var': (np.var, (), {}), 'rms': (rms, (), {})} In\u00a0[14]: Copied! <pre># Extracting the desired feature-set from time-domain signals\nfeatures_df = feature_extractor(signals, features)\nfeatures_df\n</pre> # Extracting the desired feature-set from time-domain signals features_df = feature_extractor(signals, features) features_df Out[14]: mean var rms 0 -0.004402 0.988075 0.994029 1 -0.002861 0.997338 0.998672 2 0.000540 0.995953 0.997975 3 -0.001185 0.968905 0.984331 4 -0.000524 1.015189 1.007566 ... ... ... ... 5725 -0.002470 183.735440 13.554905 5726 -0.009952 183.644658 13.551559 5727 0.009894 182.985433 13.527214 5728 0.006107 181.833258 13.484558 5729 -0.013400 182.282390 13.501206 <p>5730 rows \u00d7 3 columns</p> In\u00a0[14]: Copied! <pre>\n</pre> In\u00a0[15]: Copied! <pre>import seaborn as sns\nfrom matplotlib import pyplot as plt\nsns.set()\n\nfig, axes = plt.subplots(4, 1, figsize = (16, 10))\n\nsns.lineplot(ax=axes[0], x=range(len(signals.iloc[0,:])), y = signals.iloc[0,:])\naxes[0].set_title(\"Original Time Signal\")\naxes[0].set_ylabel(\"Amplitude\")\naxes[0].set_xlabel(\"sample\")\naxes[0].set_xlim(0, 20000)\n\nsns.lineplot(ax=axes[1], x=range(len(signals_env.iloc[0,:])), y = signals_env.iloc[0, :])\naxes[1].set_title(\"Envelope\")\naxes[1].set_ylabel(\"Amplitude\")\naxes[1].set_xlabel(\"sample\")\naxes[1].set_xlim(0, 20000)\n\n\nsns.lineplot(ax=axes[2], x = fft_freq_axis(20000, 25600), y = signals_env_fft.iloc[0, :])\naxes[2].set_title(\"FFT\")\naxes[2].set_ylabel(\"Amplitude\")\naxes[2].set_xlabel(\"Frequency (Hz)\")\naxes[2].set_xlim(0, 12800)\n\n\nsns.lineplot(ax=axes[3], x = zoomed_fft_freq_axis(0, 2500, 2500), y = signals_env_ZoomedFFT.iloc[0, :])\naxes[3].set_title(\"Zoomed FFT\")\naxes[3].set_ylabel(\"Amplitude\")\naxes[3].set_xlabel(\"Frequency (Hz)\")\naxes[3].set_xlim(0, 2500)\n\n\nplt.subplots_adjust(hspace = 0.75)\nfig.show()\n</pre> import seaborn as sns from matplotlib import pyplot as plt sns.set()  fig, axes = plt.subplots(4, 1, figsize = (16, 10))  sns.lineplot(ax=axes[0], x=range(len(signals.iloc[0,:])), y = signals.iloc[0,:]) axes[0].set_title(\"Original Time Signal\") axes[0].set_ylabel(\"Amplitude\") axes[0].set_xlabel(\"sample\") axes[0].set_xlim(0, 20000)  sns.lineplot(ax=axes[1], x=range(len(signals_env.iloc[0,:])), y = signals_env.iloc[0, :]) axes[1].set_title(\"Envelope\") axes[1].set_ylabel(\"Amplitude\") axes[1].set_xlabel(\"sample\") axes[1].set_xlim(0, 20000)   sns.lineplot(ax=axes[2], x = fft_freq_axis(20000, 25600), y = signals_env_fft.iloc[0, :]) axes[2].set_title(\"FFT\") axes[2].set_ylabel(\"Amplitude\") axes[2].set_xlabel(\"Frequency (Hz)\") axes[2].set_xlim(0, 12800)   sns.lineplot(ax=axes[3], x = zoomed_fft_freq_axis(0, 2500, 2500), y = signals_env_ZoomedFFT.iloc[0, :]) axes[3].set_title(\"Zoomed FFT\") axes[3].set_ylabel(\"Amplitude\") axes[3].set_xlabel(\"Frequency (Hz)\") axes[3].set_xlim(0, 2500)   plt.subplots_adjust(hspace = 0.75) fig.show() In\u00a0[16]: Copied! <pre>t = np.linspace(0, 0.222, 8)\nf = fft_freq_axis(4000, 25600)\n\nfig, ax = plt.subplots(figsize = (16, 8))\n\nax = sns.heatmap(signals_env_STFT[0, :, :], xticklabels = np.round(f, decimals = 2), yticklabels = np.round(t, decimals = 2), annot = False, cbar = False)\nax.set(xlabel = 'Frequency (Hz)', ylabel = 'Time (sec)')\nax.set_title('STFT')\nax.set_xticks(ax.get_xticks()[::50])\nax.set_yticks(ax.get_yticks()[::2])\n\n\nfig.show()\n</pre> t = np.linspace(0, 0.222, 8) f = fft_freq_axis(4000, 25600)  fig, ax = plt.subplots(figsize = (16, 8))  ax = sns.heatmap(signals_env_STFT[0, :, :], xticklabels = np.round(f, decimals = 2), yticklabels = np.round(t, decimals = 2), annot = False, cbar = False) ax.set(xlabel = 'Frequency (Hz)', ylabel = 'Time (sec)') ax.set_title('STFT') ax.set_xticks(ax.get_xticks()[::50]) ax.set_yticks(ax.get_yticks()[::2])   fig.show() In\u00a0[16]: Copied! <pre>\n</pre>"},{"location":"notebooks/dataset_demos/KAIST_demo/#cloning-the-damavand-repository","title":"Cloning the damavand repository\u00b6","text":""},{"location":"notebooks/dataset_demos/KAIST_demo/#importings","title":"Importings\u00b6","text":""},{"location":"notebooks/dataset_demos/KAIST_demo/#instantiating-a-downloader-object","title":"Instantiating a downloader object\u00b6","text":""},{"location":"notebooks/dataset_demos/KAIST_demo/#instantiating-a-digestor-object","title":"Instantiating a digestor object\u00b6","text":""},{"location":"notebooks/dataset_demos/KAIST_demo/#aggregating-data-over-the-first-channel","title":"Aggregating data over the first channel\u00b6","text":""},{"location":"notebooks/dataset_demos/KAIST_demo/#signals-metadata-declaration","title":"Signals-Metadata declaration\u00b6","text":""},{"location":"notebooks/dataset_demos/KAIST_demo/#signal-processing","title":"Signal Processing\u00b6","text":""},{"location":"notebooks/dataset_demos/KAIST_demo/#envelope-extraction","title":"Envelope Extraction\u00b6","text":""},{"location":"notebooks/dataset_demos/KAIST_demo/#fft","title":"FFT\u00b6","text":""},{"location":"notebooks/dataset_demos/KAIST_demo/#zoomedfft","title":"ZoomedFFT\u00b6","text":""},{"location":"notebooks/dataset_demos/KAIST_demo/#stft","title":"STFT\u00b6","text":""},{"location":"notebooks/dataset_demos/KAIST_demo/#statistical-features","title":"Statistical Features\u00b6","text":""},{"location":"notebooks/dataset_demos/KAIST_demo/#visualization","title":"Visualization\u00b6","text":""},{"location":"notebooks/dataset_demos/MEUT_demo/","title":"Cloning the damavand repository","text":"In\u00a0[1]: Copied! <pre>!git clone https://github.com/amirberenji1995/damavand\n</pre> !git clone https://github.com/amirberenji1995/damavand <pre>Cloning into 'damavand'...\nremote: Enumerating objects: 263, done.\nremote: Counting objects: 100% (263/263), done.\nremote: Compressing objects: 100% (197/197), done.\nremote: Total 263 (delta 132), reused 196 (delta 65), pack-reused 0 (from 0)\nReceiving objects: 100% (263/263), 6.62 MiB | 12.18 MiB/s, done.\nResolving deltas: 100% (132/132), done.\n</pre> In\u00a0[2]: Copied! <pre>!pip install -r damavand/requirements.txt\n</pre> !pip install -r damavand/requirements.txt <pre>Collecting certifi==2024.7.4 (from -r damavand/requirements.txt (line 1))\n  Downloading certifi-2024.7.4-py3-none-any.whl.metadata (2.2 kB)\nCollecting charset-normalizer==3.3.2 (from -r damavand/requirements.txt (line 2))\n  Downloading charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\nCollecting idna==3.7 (from -r damavand/requirements.txt (line 3))\n  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\nCollecting numpy==1.26.4 (from -r damavand/requirements.txt (line 4))\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 61.0/61.0 kB 3.8 MB/s eta 0:00:00\nCollecting pandas==2.1.4 (from -r damavand/requirements.txt (line 5))\n  Downloading pandas-2.1.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\nRequirement already satisfied: python-dateutil==2.9.0.post0 in /usr/local/lib/python3.11/dist-packages (from -r damavand/requirements.txt (line 6)) (2.9.0.post0)\nCollecting pytz==2024.1 (from -r damavand/requirements.txt (line 7))\n  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\nCollecting rarfile==4.2 (from -r damavand/requirements.txt (line 8))\n  Downloading rarfile-4.2-py3-none-any.whl.metadata (4.4 kB)\nRequirement already satisfied: requests==2.32.3 in /usr/local/lib/python3.11/dist-packages (from -r damavand/requirements.txt (line 9)) (2.32.3)\nCollecting scipy==1.13.1 (from -r damavand/requirements.txt (line 10))\n  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 60.6/60.6 kB 4.1 MB/s eta 0:00:00\nCollecting six==1.16.0 (from -r damavand/requirements.txt (line 11))\n  Downloading six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\nCollecting tzdata==2024.1 (from -r damavand/requirements.txt (line 12))\n  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\nCollecting urllib3==2.2.2 (from -r damavand/requirements.txt (line 13))\n  Downloading urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\nDownloading certifi-2024.7.4-py3-none-any.whl (162 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 163.0/163.0 kB 7.4 MB/s eta 0:00:00\nDownloading charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (140 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 140.3/140.3 kB 12.2 MB/s eta 0:00:00\nDownloading idna-3.7-py3-none-any.whl (66 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 66.8/66.8 kB 4.3 MB/s eta 0:00:00\nDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 91.6 MB/s eta 0:00:00\nDownloading pandas-2.1.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 12.2/12.2 MB 107.4 MB/s eta 0:00:00\nDownloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 505.5/505.5 kB 31.6 MB/s eta 0:00:00\nDownloading rarfile-4.2-py3-none-any.whl (29 kB)\nDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 38.6/38.6 MB 19.6 MB/s eta 0:00:00\nDownloading six-1.16.0-py2.py3-none-any.whl (11 kB)\nDownloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 345.4/345.4 kB 24.4 MB/s eta 0:00:00\nDownloading urllib3-2.2.2-py3-none-any.whl (121 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 121.4/121.4 kB 10.1 MB/s eta 0:00:00\nInstalling collected packages: pytz, urllib3, tzdata, six, rarfile, numpy, idna, charset-normalizer, certifi, scipy, pandas\n  Attempting uninstall: pytz\n    Found existing installation: pytz 2025.2\n    Uninstalling pytz-2025.2:\n      Successfully uninstalled pytz-2025.2\n  Attempting uninstall: urllib3\n    Found existing installation: urllib3 2.5.0\n    Uninstalling urllib3-2.5.0:\n      Successfully uninstalled urllib3-2.5.0\n  Attempting uninstall: tzdata\n    Found existing installation: tzdata 2025.2\n    Uninstalling tzdata-2025.2:\n      Successfully uninstalled tzdata-2025.2\n  Attempting uninstall: six\n    Found existing installation: six 1.17.0\n    Uninstalling six-1.17.0:\n      Successfully uninstalled six-1.17.0\n  Attempting uninstall: numpy\n    Found existing installation: numpy 2.0.2\n    Uninstalling numpy-2.0.2:\n      Successfully uninstalled numpy-2.0.2\n  Attempting uninstall: idna\n    Found existing installation: idna 3.10\n    Uninstalling idna-3.10:\n      Successfully uninstalled idna-3.10\n  Attempting uninstall: charset-normalizer\n    Found existing installation: charset-normalizer 3.4.2\n    Uninstalling charset-normalizer-3.4.2:\n      Successfully uninstalled charset-normalizer-3.4.2\n  Attempting uninstall: certifi\n    Found existing installation: certifi 2025.7.14\n    Uninstalling certifi-2025.7.14:\n      Successfully uninstalled certifi-2025.7.14\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.16.0\n    Uninstalling scipy-1.16.0:\n      Successfully uninstalled scipy-1.16.0\n  Attempting uninstall: pandas\n    Found existing installation: pandas 2.2.2\n    Uninstalling pandas-2.2.2:\n      Successfully uninstalled pandas-2.2.2\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.1.4 which is incompatible.\nopencv-python 4.12.0.88 requires numpy&lt;2.3.0,&gt;=2; python_version &gt;= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy&lt;2.3.0,&gt;=2; python_version &gt;= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy&lt;2.3.0,&gt;=2; python_version &gt;= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nplotnine 0.14.5 requires pandas&gt;=2.2.0, but you have pandas 2.1.4 which is incompatible.\ntsfresh 0.21.0 requires scipy&gt;=1.14.0; python_version &gt;= \"3.10\", but you have scipy 1.13.1 which is incompatible.\nthinc 8.3.6 requires numpy&lt;3.0.0,&gt;=2.0.0, but you have numpy 1.26.4 which is incompatible.\nxarray 2025.7.1 requires pandas&gt;=2.2, but you have pandas 2.1.4 which is incompatible.\nmizani 0.13.5 requires pandas&gt;=2.2.0, but you have pandas 2.1.4 which is incompatible.\nSuccessfully installed certifi-2024.7.4 charset-normalizer-3.3.2 idna-3.7 numpy-1.26.4 pandas-2.1.4 pytz-2024.1 rarfile-4.2 scipy-1.13.1 six-1.16.0 tzdata-2024.1 urllib3-2.2.2\n</pre> In\u00a0[1]: Copied! <pre>from damavand.damavand.datasets.downloaders import read_addresses, ZipDatasetDownloader\nfrom damavand.damavand.datasets.digestors import MUET\nfrom damavand.damavand.signal_processing.transformations import *\nfrom damavand.damavand.signal_processing.feature_extraction import *\nfrom damavand.damavand.utils import *\nimport os\nimport pandas as pd\nimport numpy as np\nimport scipy\n</pre> from damavand.damavand.datasets.downloaders import read_addresses, ZipDatasetDownloader from damavand.damavand.datasets.digestors import MUET from damavand.damavand.signal_processing.transformations import * from damavand.damavand.signal_processing.feature_extraction import * from damavand.damavand.utils import * import os import pandas as pd import numpy as np import scipy In\u00a0[2]: Copied! <pre>addresses = read_addresses()\ndownloader = ZipDatasetDownloader(addresses['MEUT'])\ndownloader.download_extract('MEUT.zip', 'MEUT/')\n</pre> addresses = read_addresses() downloader = ZipDatasetDownloader(addresses['MEUT']) downloader.download_extract('MEUT.zip', 'MEUT/') In\u00a0[3]: Copied! <pre>dataset = MUET('MEUT/fm6xzxnf36-2/', os.listdir('MEUT/fm6xzxnf36-2/'), [3])\nmining_params = {\n    'win_len': 10000,\n    'hop_len': 5000\n}\ndataset.mine(mining_params)\n</pre> dataset = MUET('MEUT/fm6xzxnf36-2/', os.listdir('MEUT/fm6xzxnf36-2/'), [3]) mining_params = {     'win_len': 10000,     'hop_len': 5000 } dataset.mine(mining_params) In\u00a0[4]: Copied! <pre>df = pd.concat(dataset.data[3]).reset_index(drop = True)\ndf\n</pre> df = pd.concat(dataset.data[3]).reset_index(drop = True) df Out[4]: 0 1 2 3 4 5 6 7 8 9 ... 9993 9994 9995 9996 9997 9998 9999 state severity load 0 1.2666 1.0863 1.2619 1.0262 0.5688 0.8018 1.0815 0.5893 1.3947 1.2990 ... 0.8280 0.9742 0.9013 1.2043 1.1234 1.1803 1.4774 inner 1.7mm 300watt 1 0.7622 0.7220 1.1902 0.9127 0.9632 1.2018 0.8078 1.0471 0.8268 0.8127 ... 1.0598 0.9710 0.8199 1.4032 0.5488 1.3034 1.1842 inner 1.7mm 300watt 2 1.1204 0.8002 0.6241 0.7893 1.0753 0.8071 0.9605 0.8527 0.7562 1.3141 ... 0.6383 1.1305 1.2806 1.0464 0.9988 0.8893 0.7472 inner 1.7mm 300watt 3 0.5021 0.9887 1.1369 0.9777 1.1457 1.2852 0.9475 1.0292 1.2071 1.2955 ... 1.0351 1.1757 0.8629 0.8174 1.1683 0.9307 0.9282 inner 1.7mm 300watt 4 0.4603 1.1289 0.7353 1.1372 0.7812 1.1996 0.9217 0.4311 1.2474 0.7464 ... 1.1843 1.1004 0.5805 1.1865 0.5676 0.9766 0.9430 inner 1.7mm 300watt ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 948 1.1333 1.0647 1.0057 0.9308 0.9584 0.7640 0.7204 0.8193 0.8335 0.9000 ... 1.1683 1.1828 1.1728 1.1462 1.1098 1.0133 0.9573 healthy - with with 949 1.1124 1.0830 0.9493 0.9067 0.9229 0.7486 0.7436 0.7156 0.7114 0.7514 ... 1.1864 1.1791 1.1298 0.9603 0.9554 0.8857 0.8053 healthy - with with 950 0.9498 0.8349 0.8612 0.9097 0.9825 1.0573 1.1616 1.1726 1.1897 1.1852 ... 1.1295 1.1739 1.1008 0.9265 0.8579 0.7646 0.7966 healthy - with with 951 0.8021 0.6988 0.7541 0.8113 0.8752 0.9022 1.1178 1.1399 1.1692 1.1779 ... 1.1629 1.1838 1.1569 1.0788 0.9513 0.9549 0.8471 healthy - with with 952 0.7843 0.7991 0.7984 0.8943 0.9371 1.0001 1.1332 1.1315 1.2031 1.1875 ... 0.9645 1.0088 1.0618 1.1734 1.1739 1.1378 1.0438 healthy - with with <p>953 rows \u00d7 10003 columns</p> In\u00a0[5]: Copied! <pre>signals, metadata = df.iloc[:, : -3], df.iloc[:, -3 :]\nsignals\n</pre> signals, metadata = df.iloc[:, : -3], df.iloc[:, -3 :] signals Out[5]: 0 1 2 3 4 5 6 7 8 9 ... 9990 9991 9992 9993 9994 9995 9996 9997 9998 9999 0 1.2666 1.0863 1.2619 1.0262 0.5688 0.8018 1.0815 0.5893 1.3947 1.2990 ... 1.1624 1.1464 0.8923 0.8280 0.9742 0.9013 1.2043 1.1234 1.1803 1.4774 1 0.7622 0.7220 1.1902 0.9127 0.9632 1.2018 0.8078 1.0471 0.8268 0.8127 ... 1.1883 0.7536 0.5153 1.0598 0.9710 0.8199 1.4032 0.5488 1.3034 1.1842 2 1.1204 0.8002 0.6241 0.7893 1.0753 0.8071 0.9605 0.8527 0.7562 1.3141 ... 0.6905 0.7385 0.7172 0.6383 1.1305 1.2806 1.0464 0.9988 0.8893 0.7472 3 0.5021 0.9887 1.1369 0.9777 1.1457 1.2852 0.9475 1.0292 1.2071 1.2955 ... 0.9602 1.1475 1.1205 1.0351 1.1757 0.8629 0.8174 1.1683 0.9307 0.9282 4 0.4603 1.1289 0.7353 1.1372 0.7812 1.1996 0.9217 0.4311 1.2474 0.7464 ... 1.3979 0.8986 0.6804 1.1843 1.1004 0.5805 1.1865 0.5676 0.9766 0.9430 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 948 1.1333 1.0647 1.0057 0.9308 0.9584 0.7640 0.7204 0.8193 0.8335 0.9000 ... 0.9696 1.1463 1.1494 1.1683 1.1828 1.1728 1.1462 1.1098 1.0133 0.9573 949 1.1124 1.0830 0.9493 0.9067 0.9229 0.7486 0.7436 0.7156 0.7114 0.7514 ... 1.1253 1.1974 1.1655 1.1864 1.1791 1.1298 0.9603 0.9554 0.8857 0.8053 950 0.9498 0.8349 0.8612 0.9097 0.9825 1.0573 1.1616 1.1726 1.1897 1.1852 ... 1.1352 1.1830 1.1776 1.1295 1.1739 1.1008 0.9265 0.8579 0.7646 0.7966 951 0.8021 0.6988 0.7541 0.8113 0.8752 0.9022 1.1178 1.1399 1.1692 1.1779 ... 1.0087 1.1645 1.1937 1.1629 1.1838 1.1569 1.0788 0.9513 0.9549 0.8471 952 0.7843 0.7991 0.7984 0.8943 0.9371 1.0001 1.1332 1.1315 1.2031 1.1875 ... 0.9143 0.8392 0.8605 0.9645 1.0088 1.0618 1.1734 1.1739 1.1378 1.0438 <p>953 rows \u00d7 10000 columns</p> In\u00a0[6]: Copied! <pre>metadata\n</pre> metadata Out[6]: state severity load 0 inner 1.7mm 300watt 1 inner 1.7mm 300watt 2 inner 1.7mm 300watt 3 inner 1.7mm 300watt 4 inner 1.7mm 300watt ... ... ... ... 948 healthy - with with 949 healthy - with with 950 healthy - with with 951 healthy - with with 952 healthy - with with <p>953 rows \u00d7 3 columns</p> In\u00a0[7]: Copied! <pre>metadata['comb'] = metadata['state'] + '_' + metadata['severity'] + '_' + metadata['load']\nmetadata['comb'].value_counts()\n</pre> metadata['comb'] = metadata['state'] + '_' + metadata['severity'] + '_' + metadata['load'] metadata['comb'].value_counts() Out[7]: count comb outer_1.1mm_200watt 29 inner_0.9mm_100watt 29 inner_1.1mm_300watt 28 inner_1.5mm_200watt 28 inner_0.7mm_100watt 27 outer_0.9mm_100watt 27 outer_0.9mm_200watt 27 inner_1.3mm_100watt 27 outer_1.1mm_300watt 26 outer_1.1mm_100watt 26 inner_1.1mm_200watt 26 inner_1.1mm_100watt 26 inner_1.7mm_200watt 26 inner_0.9mm_200watt 26 outer_1.3mm_200watt 26 inner_1.7mm_100watt 26 outer_1.7mm_100watt 26 outer_1.7mm_300watt 26 outer_1.5mm_300watt 26 inner_1.3mm_200watt 25 outer_1.3mm_100watt 25 inner_0.9mm_300watt 25 outer_0.7mm_100watt 25 inner_1.5mm_100watt 25 outer_1.5mm_100watt 25 outer_0.7mm_200watt 25 inner_1.5mm_300watt 25 outer_1.7mm_200watt 25 outer_1.5mm_200watt 24 inner_1.7mm_300watt 24 inner_1.3mm_300watt 24 outer_1.3mm_300watt 24 inner_0.7mm_200watt 24 outer_0.9mm_300watt 23 healthy_-_without without 22 healthy_-_with with 22 inner_0.7mm_300watt 21 outer_0.7mm_300watt 12 dtype: int64 In\u00a0[8]: Copied! <pre>signals_env = env(signals)\nsignals_env.shape\n</pre> signals_env = env(signals) signals_env.shape Out[8]: <pre>(953, 10000)</pre> In\u00a0[9]: Copied! <pre># Defenition of a window (to avoid leakage error) and a bandpass frequency filter (to both avoid aliasing and DC-component removal)\nwindow = scipy.signal.windows.hann(signals_env.shape[1])\nfreq_filter = scipy.signal.butter(25, [5, 4500], 'bandpass', fs = 10000, output='sos')\n</pre> # Defenition of a window (to avoid leakage error) and a bandpass frequency filter (to both avoid aliasing and DC-component removal) window = scipy.signal.windows.hann(signals_env.shape[1]) freq_filter = scipy.signal.butter(25, [5, 4500], 'bandpass', fs = 10000, output='sos') In\u00a0[10]: Copied! <pre>signals_env_fft = fft(signals_env, freq_filter = freq_filter, window = window)\nsignals_env_fft.shape\n</pre> signals_env_fft = fft(signals_env, freq_filter = freq_filter, window = window) signals_env_fft.shape Out[10]: <pre>(953, 5000)</pre> In\u00a0[11]: Copied! <pre>signals_env_ZoomedFFT = zoomed_fft(signals_env, 0, 1000, 2000, 10000, freq_filter = freq_filter, window = window)\nsignals_env_ZoomedFFT.shape\n</pre> signals_env_ZoomedFFT = zoomed_fft(signals_env, 0, 1000, 2000, 10000, freq_filter = freq_filter, window = window) signals_env_ZoomedFFT.shape Out[11]: <pre>(953, 2000)</pre> In\u00a0[12]: Copied! <pre># Defenition of a window (to avoid leakage error) and a bandpass frequency filter (to both avoid aliasing and DC-component removal)\nSTFT_window = scipy.signal.windows.hann(1024) # the length of the window must match the `window_len` argument from the stft function\nSTFT_freq_filter = scipy.signal.butter(25, [5, 4500], 'bandpass', fs = 10000, output='sos')\nsignals_env_STFT = stft(signals_env, 1024, 200, STFT_freq_filter, STFT_window)\nsignals_env_STFT.shape\n</pre> # Defenition of a window (to avoid leakage error) and a bandpass frequency filter (to both avoid aliasing and DC-component removal) STFT_window = scipy.signal.windows.hann(1024) # the length of the window must match the `window_len` argument from the stft function STFT_freq_filter = scipy.signal.butter(25, [5, 4500], 'bandpass', fs = 10000, output='sos') signals_env_STFT = stft(signals_env, 1024, 200, STFT_freq_filter, STFT_window) signals_env_STFT.shape Out[12]: <pre>(953, 45, 512)</pre> In\u00a0[13]: Copied! <pre># Defining the feature-set to be extracted\nfeatures = {'mean': (np.mean, (), {}), 'var': (np.var, (), {}), 'rms': (rms, (), {})}\n</pre> # Defining the feature-set to be extracted features = {'mean': (np.mean, (), {}), 'var': (np.var, (), {}), 'rms': (rms, (), {})} In\u00a0[14]: Copied! <pre># Extracting the desired feature-set from time-domain signals\nfeatures_df = feature_extractor(signals, features)\nfeatures_df\n</pre> # Extracting the desired feature-set from time-domain signals features_df = feature_extractor(signals, features) features_df Out[14]: mean var rms 0 0.960899 0.058191 0.990716 1 0.962156 0.059195 0.992441 2 0.964804 0.061620 0.996227 3 0.965389 0.059221 0.995589 4 0.964893 0.054064 0.992514 ... ... ... ... 948 0.982739 0.027171 0.996467 949 0.982324 0.027039 0.995992 950 0.980701 0.027028 0.994386 951 0.981535 0.027043 0.995215 952 0.981407 0.026704 0.994919 <p>953 rows \u00d7 3 columns</p> In\u00a0[15]: Copied! <pre>import seaborn as sns\nfrom matplotlib import pyplot as plt\nsns.set()\n\nfig, axes = plt.subplots(4, 1, figsize = (16, 10))\n\nsns.lineplot(ax=axes[0], x=range(len(signals.iloc[0,:])), y = signals.iloc[0,:])\naxes[0].set_title(\"Original Time Signal\")\naxes[0].set_ylabel(\"Amplitude\")\naxes[0].set_xlabel(\"sample\")\naxes[0].set_xlim(0, 10000)\n\nsns.lineplot(ax=axes[1], x=range(len(signals_env.iloc[0,:])), y = signals_env.iloc[0, :])\naxes[1].set_title(\"Envelope\")\naxes[1].set_ylabel(\"Amplitude\")\naxes[1].set_xlabel(\"sample\")\naxes[1].set_xlim(0, 10000)\n\n\nsns.lineplot(ax=axes[2], x = fft_freq_axis(10000, 10000), y = signals_env_fft.iloc[0, :])\naxes[2].set_title(\"FFT\")\naxes[2].set_ylabel(\"Amplitude\")\naxes[2].set_xlabel(\"Frequency (Hz)\")\naxes[2].set_xlim(0, 5000)\n\n\nsns.lineplot(ax=axes[3], x = zoomed_fft_freq_axis(0, 1000, 2000), y = signals_env_ZoomedFFT.iloc[0, :])\naxes[3].set_title(\"Zoomed FFT\")\naxes[3].set_ylabel(\"Amplitude\")\naxes[3].set_xlabel(\"Frequency (Hz)\")\naxes[3].set_xlim(0, 1000)\n\n\nplt.subplots_adjust(hspace = 0.75)\nfig.show()\n</pre> import seaborn as sns from matplotlib import pyplot as plt sns.set()  fig, axes = plt.subplots(4, 1, figsize = (16, 10))  sns.lineplot(ax=axes[0], x=range(len(signals.iloc[0,:])), y = signals.iloc[0,:]) axes[0].set_title(\"Original Time Signal\") axes[0].set_ylabel(\"Amplitude\") axes[0].set_xlabel(\"sample\") axes[0].set_xlim(0, 10000)  sns.lineplot(ax=axes[1], x=range(len(signals_env.iloc[0,:])), y = signals_env.iloc[0, :]) axes[1].set_title(\"Envelope\") axes[1].set_ylabel(\"Amplitude\") axes[1].set_xlabel(\"sample\") axes[1].set_xlim(0, 10000)   sns.lineplot(ax=axes[2], x = fft_freq_axis(10000, 10000), y = signals_env_fft.iloc[0, :]) axes[2].set_title(\"FFT\") axes[2].set_ylabel(\"Amplitude\") axes[2].set_xlabel(\"Frequency (Hz)\") axes[2].set_xlim(0, 5000)   sns.lineplot(ax=axes[3], x = zoomed_fft_freq_axis(0, 1000, 2000), y = signals_env_ZoomedFFT.iloc[0, :]) axes[3].set_title(\"Zoomed FFT\") axes[3].set_ylabel(\"Amplitude\") axes[3].set_xlabel(\"Frequency (Hz)\") axes[3].set_xlim(0, 1000)   plt.subplots_adjust(hspace = 0.75) fig.show() In\u00a0[16]: Copied! <pre>t = np.linspace(0, 1, 45)\nf = fft_freq_axis(1024, 10000)\n\nfig, ax = plt.subplots(figsize = (16, 8))\n\nax = sns.heatmap(signals_env_STFT[0, :, :], xticklabels = np.round(f, decimals = 2), yticklabels = np.round(t, decimals = 2), annot = False, cbar = False)\nax.set(xlabel = 'Frequency (Hz)', ylabel = 'Time (sec)')\nax.set_title('STFT')\nax.set_xticks(ax.get_xticks()[::20])\nax.set_yticks(ax.get_yticks()[::5])\n\nfig.show()\n</pre> t = np.linspace(0, 1, 45) f = fft_freq_axis(1024, 10000)  fig, ax = plt.subplots(figsize = (16, 8))  ax = sns.heatmap(signals_env_STFT[0, :, :], xticklabels = np.round(f, decimals = 2), yticklabels = np.round(t, decimals = 2), annot = False, cbar = False) ax.set(xlabel = 'Frequency (Hz)', ylabel = 'Time (sec)') ax.set_title('STFT') ax.set_xticks(ax.get_xticks()[::20]) ax.set_yticks(ax.get_yticks()[::5])  fig.show() In\u00a0[16]: Copied! <pre>\n</pre>"},{"location":"notebooks/dataset_demos/MEUT_demo/#cloning-the-damavand-repository","title":"Cloning the damavand repository\u00b6","text":""},{"location":"notebooks/dataset_demos/MEUT_demo/#importings","title":"Importings\u00b6","text":""},{"location":"notebooks/dataset_demos/MEUT_demo/#instantiating-a-downloader-object","title":"Instantiating a downloader object\u00b6","text":""},{"location":"notebooks/dataset_demos/MEUT_demo/#instantiating-a-digestor-object","title":"Instantiating a digestor object\u00b6","text":""},{"location":"notebooks/dataset_demos/MEUT_demo/#aggregating-data-over-the-third-channel","title":"Aggregating data over the third channel\u00b6","text":""},{"location":"notebooks/dataset_demos/MEUT_demo/#signal-processing","title":"Signal Processing\u00b6","text":""},{"location":"notebooks/dataset_demos/MEUT_demo/#envelope-extraction","title":"Envelope Extraction\u00b6","text":""},{"location":"notebooks/dataset_demos/MEUT_demo/#fft","title":"FFT\u00b6","text":""},{"location":"notebooks/dataset_demos/MEUT_demo/#zoomedfft","title":"ZoomedFFT\u00b6","text":""},{"location":"notebooks/dataset_demos/MEUT_demo/#stft","title":"STFT\u00b6","text":""},{"location":"notebooks/dataset_demos/MEUT_demo/#statistical-features","title":"Statistical Features\u00b6","text":""},{"location":"notebooks/dataset_demos/MEUT_demo/#visualization","title":"Visualization\u00b6","text":""},{"location":"notebooks/dataset_demos/MFPT_demo/","title":"Cloning the damavand repository","text":"In\u00a0[1]: Copied! <pre>!git clone https://github.com/amirberenji1995/damavand\n</pre> !git clone https://github.com/amirberenji1995/damavand <pre>Cloning into 'damavand'...\nremote: Enumerating objects: 263, done.\nremote: Counting objects: 100% (263/263), done.\nremote: Compressing objects: 100% (197/197), done.\nremote: Total 263 (delta 132), reused 196 (delta 65), pack-reused 0 (from 0)\nReceiving objects: 100% (263/263), 6.62 MiB | 11.75 MiB/s, done.\nResolving deltas: 100% (132/132), done.\n</pre> In\u00a0[2]: Copied! <pre>!pip install -r damavand/requirements.txt\n</pre> !pip install -r damavand/requirements.txt <pre>Collecting certifi==2024.7.4 (from -r damavand/requirements.txt (line 1))\n  Downloading certifi-2024.7.4-py3-none-any.whl.metadata (2.2 kB)\nCollecting charset-normalizer==3.3.2 (from -r damavand/requirements.txt (line 2))\n  Downloading charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\nCollecting idna==3.7 (from -r damavand/requirements.txt (line 3))\n  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\nCollecting numpy==1.26.4 (from -r damavand/requirements.txt (line 4))\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 61.0/61.0 kB 3.8 MB/s eta 0:00:00\nCollecting pandas==2.1.4 (from -r damavand/requirements.txt (line 5))\n  Downloading pandas-2.1.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\nRequirement already satisfied: python-dateutil==2.9.0.post0 in /usr/local/lib/python3.11/dist-packages (from -r damavand/requirements.txt (line 6)) (2.9.0.post0)\nCollecting pytz==2024.1 (from -r damavand/requirements.txt (line 7))\n  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\nCollecting rarfile==4.2 (from -r damavand/requirements.txt (line 8))\n  Downloading rarfile-4.2-py3-none-any.whl.metadata (4.4 kB)\nRequirement already satisfied: requests==2.32.3 in /usr/local/lib/python3.11/dist-packages (from -r damavand/requirements.txt (line 9)) (2.32.3)\nCollecting scipy==1.13.1 (from -r damavand/requirements.txt (line 10))\n  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 60.6/60.6 kB 4.1 MB/s eta 0:00:00\nCollecting six==1.16.0 (from -r damavand/requirements.txt (line 11))\n  Downloading six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\nCollecting tzdata==2024.1 (from -r damavand/requirements.txt (line 12))\n  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\nCollecting urllib3==2.2.2 (from -r damavand/requirements.txt (line 13))\n  Downloading urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\nDownloading certifi-2024.7.4-py3-none-any.whl (162 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 163.0/163.0 kB 10.5 MB/s eta 0:00:00\nDownloading charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (140 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 140.3/140.3 kB 11.6 MB/s eta 0:00:00\nDownloading idna-3.7-py3-none-any.whl (66 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 66.8/66.8 kB 4.9 MB/s eta 0:00:00\nDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 58.0 MB/s eta 0:00:00\nDownloading pandas-2.1.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 12.2/12.2 MB 78.9 MB/s eta 0:00:00\nDownloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 505.5/505.5 kB 33.0 MB/s eta 0:00:00\nDownloading rarfile-4.2-py3-none-any.whl (29 kB)\nDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 38.6/38.6 MB 17.3 MB/s eta 0:00:00\nDownloading six-1.16.0-py2.py3-none-any.whl (11 kB)\nDownloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 345.4/345.4 kB 22.8 MB/s eta 0:00:00\nDownloading urllib3-2.2.2-py3-none-any.whl (121 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 121.4/121.4 kB 9.4 MB/s eta 0:00:00\nInstalling collected packages: pytz, urllib3, tzdata, six, rarfile, numpy, idna, charset-normalizer, certifi, scipy, pandas\n  Attempting uninstall: pytz\n    Found existing installation: pytz 2025.2\n    Uninstalling pytz-2025.2:\n      Successfully uninstalled pytz-2025.2\n  Attempting uninstall: urllib3\n    Found existing installation: urllib3 2.5.0\n    Uninstalling urllib3-2.5.0:\n      Successfully uninstalled urllib3-2.5.0\n  Attempting uninstall: tzdata\n    Found existing installation: tzdata 2025.2\n    Uninstalling tzdata-2025.2:\n      Successfully uninstalled tzdata-2025.2\n  Attempting uninstall: six\n    Found existing installation: six 1.17.0\n    Uninstalling six-1.17.0:\n      Successfully uninstalled six-1.17.0\n  Attempting uninstall: numpy\n    Found existing installation: numpy 2.0.2\n    Uninstalling numpy-2.0.2:\n      Successfully uninstalled numpy-2.0.2\n  Attempting uninstall: idna\n    Found existing installation: idna 3.10\n    Uninstalling idna-3.10:\n      Successfully uninstalled idna-3.10\n  Attempting uninstall: charset-normalizer\n    Found existing installation: charset-normalizer 3.4.2\n    Uninstalling charset-normalizer-3.4.2:\n      Successfully uninstalled charset-normalizer-3.4.2\n  Attempting uninstall: certifi\n    Found existing installation: certifi 2025.7.14\n    Uninstalling certifi-2025.7.14:\n      Successfully uninstalled certifi-2025.7.14\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.16.0\n    Uninstalling scipy-1.16.0:\n      Successfully uninstalled scipy-1.16.0\n  Attempting uninstall: pandas\n    Found existing installation: pandas 2.2.2\n    Uninstalling pandas-2.2.2:\n      Successfully uninstalled pandas-2.2.2\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.1.4 which is incompatible.\nopencv-python 4.12.0.88 requires numpy&lt;2.3.0,&gt;=2; python_version &gt;= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy&lt;2.3.0,&gt;=2; python_version &gt;= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy&lt;2.3.0,&gt;=2; python_version &gt;= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nplotnine 0.14.5 requires pandas&gt;=2.2.0, but you have pandas 2.1.4 which is incompatible.\ntsfresh 0.21.0 requires scipy&gt;=1.14.0; python_version &gt;= \"3.10\", but you have scipy 1.13.1 which is incompatible.\nthinc 8.3.6 requires numpy&lt;3.0.0,&gt;=2.0.0, but you have numpy 1.26.4 which is incompatible.\nxarray 2025.7.1 requires pandas&gt;=2.2, but you have pandas 2.1.4 which is incompatible.\nmizani 0.13.5 requires pandas&gt;=2.2.0, but you have pandas 2.1.4 which is incompatible.\nSuccessfully installed certifi-2024.7.4 charset-normalizer-3.3.2 idna-3.7 numpy-1.26.4 pandas-2.1.4 pytz-2024.1 rarfile-4.2 scipy-1.13.1 six-1.16.0 tzdata-2024.1 urllib3-2.2.2\n</pre> In\u00a0[8]: Copied! <pre>from damavand.damavand.datasets.downloaders import read_addresses, ZipDatasetDownloader\nfrom damavand.damavand.datasets.digestors import MFPT\nfrom damavand.damavand.signal_processing.transformations import *\nfrom damavand.damavand.signal_processing.feature_extraction import *\nfrom damavand.damavand.utils import *\nimport os\nimport pandas as pd\nimport numpy as np\nimport scipy\n</pre> from damavand.damavand.datasets.downloaders import read_addresses, ZipDatasetDownloader from damavand.damavand.datasets.digestors import MFPT from damavand.damavand.signal_processing.transformations import * from damavand.damavand.signal_processing.feature_extraction import * from damavand.damavand.utils import * import os import pandas as pd import numpy as np import scipy In\u00a0[2]: Copied! <pre>addresses = read_addresses()\ndownloader = ZipDatasetDownloader(addresses['MFPT'])\ndownloader.download_extract('MFPT.zip', 'MFPT/')\n</pre> addresses = read_addresses() downloader = ZipDatasetDownloader(addresses['MFPT']) downloader.download_extract('MFPT.zip', 'MFPT/') In\u00a0[3]: Copied! <pre>mfpt = MFPT('MFPT/MFPT Fault Data Sets/', [\n    '1 - Three Baseline Conditions',\n    '2 - Three Outer Race Fault Conditions',\n    '3 - Seven More Outer Race Fault Conditions',\n    '4 - Seven Inner Race Fault Conditions',\n])\nmining_params = {\n    97656: {'win_len': 16671, 'hop_len': 2000},\n    48828: {'win_len': 8337, 'hop_len': 1000},\n}\nmfpt.mine(mining_params)\n</pre> mfpt = MFPT('MFPT/MFPT Fault Data Sets/', [     '1 - Three Baseline Conditions',     '2 - Three Outer Race Fault Conditions',     '3 - Seven More Outer Race Fault Conditions',     '4 - Seven Inner Race Fault Conditions', ]) mining_params = {     97656: {'win_len': 16671, 'hop_len': 2000},     48828: {'win_len': 8337, 'hop_len': 1000}, } mfpt.mine(mining_params) In\u00a0[4]: Copied! <pre>df = pd.concat(mfpt.data[48828]).reset_index(drop = True)\ndf\n</pre> df = pd.concat(mfpt.data[48828]).reset_index(drop = True) df Out[4]: 0 1 2 3 4 5 6 7 8 9 ... 8331 8332 8333 8334 8335 8336 Fs load rot_speed state 0 -0.190937 0.378588 -0.490422 -1.442238 -0.051721 -0.428069 -0.656600 0.154819 -0.165624 0.138256 ... 0.477637 0.310549 -0.486780 1.008681 1.973270 0.014804 48828 5 25 OR 1 0.985570 1.528132 1.166798 1.804840 0.136606 -0.444940 0.634646 0.592880 -0.172613 -0.709712 ... -0.555015 -0.437741 -0.217456 0.394339 1.813208 -0.340981 48828 5 25 OR 2 -0.710302 -0.726792 0.444288 -0.230426 0.743183 2.364421 -0.180058 -0.472887 -0.975517 -0.169131 ... -0.284190 0.323076 -0.155004 0.493425 0.662359 -0.422113 48828 5 25 OR 3 -0.489143 0.019271 -0.348365 0.377997 1.174083 0.532484 -0.967210 -0.215117 -0.132693 0.017302 ... -2.915880 -1.967240 -1.461447 -1.681819 -3.130149 -1.986240 48828 5 25 OR 4 0.938070 0.278812 1.789827 1.254427 2.114565 0.990776 0.126996 0.384187 0.611524 1.394369 ... -0.194616 -0.122824 0.054416 0.301516 -0.707878 -1.143872 48828 5 25 OR ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 1941 -0.230820 -0.790745 0.025854 0.201064 -0.548063 -0.909767 -0.839439 -0.679033 -0.491099 -0.703546 ... 0.509361 0.721611 0.855363 0.290441 -0.004898 0.272807 48828 3 25 IR 1942 -0.284608 -0.134428 -0.334139 1.513390 0.178323 -0.398191 -1.127591 -0.356363 0.749201 0.402916 ... -0.213912 -0.462882 0.138403 0.439809 -0.295757 0.332970 48828 3 25 IR 1943 0.931031 1.508049 1.777522 0.967493 0.836941 0.520904 -0.444128 -0.654865 -0.066033 -0.035638 ... -1.742745 -0.446306 -0.275613 -1.514215 -1.291886 -0.547804 48828 3 25 IR 1944 -0.799814 -0.954929 -0.645598 1.089000 0.379178 -2.049443 -1.156781 -1.620229 -0.118172 -0.314733 ... -1.741047 0.355268 -0.063067 1.540943 -0.970865 -3.165983 48828 3 25 IR 1945 1.035470 0.888933 1.426499 1.277414 2.548528 0.588647 0.596621 0.003606 0.368644 0.038259 ... -1.019165 0.174644 -0.980685 -0.502556 0.272979 0.495246 48828 3 25 IR <p>1946 rows \u00d7 8341 columns</p> In\u00a0[5]: Copied! <pre>signals, metadata = df.iloc[:, : - 4], df.iloc[:, - 4 :]\nsignals\n</pre> signals, metadata = df.iloc[:, : - 4], df.iloc[:, - 4 :] signals Out[5]: 0 1 2 3 4 5 6 7 8 9 ... 8327 8328 8329 8330 8331 8332 8333 8334 8335 8336 0 -0.190937 0.378588 -0.490422 -1.442238 -0.051721 -0.428069 -0.656600 0.154819 -0.165624 0.138256 ... -1.544105 -0.186346 0.480972 0.054982 0.477637 0.310549 -0.486780 1.008681 1.973270 0.014804 1 0.985570 1.528132 1.166798 1.804840 0.136606 -0.444940 0.634646 0.592880 -0.172613 -0.709712 ... -1.792633 0.365347 0.498470 -0.398966 -0.555015 -0.437741 -0.217456 0.394339 1.813208 -0.340981 2 -0.710302 -0.726792 0.444288 -0.230426 0.743183 2.364421 -0.180058 -0.472887 -0.975517 -0.169131 ... 0.010177 -1.666413 -0.462193 -0.095431 -0.284190 0.323076 -0.155004 0.493425 0.662359 -0.422113 3 -0.489143 0.019271 -0.348365 0.377997 1.174083 0.532484 -0.967210 -0.215117 -0.132693 0.017302 ... -1.868412 -0.944457 -1.729467 -2.656622 -2.915880 -1.967240 -1.461447 -1.681819 -3.130149 -1.986240 4 0.938070 0.278812 1.789827 1.254427 2.114565 0.990776 0.126996 0.384187 0.611524 1.394369 ... -0.996436 -0.205396 -0.512019 -0.346519 -0.194616 -0.122824 0.054416 0.301516 -0.707878 -1.143872 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 1941 -0.230820 -0.790745 0.025854 0.201064 -0.548063 -0.909767 -0.839439 -0.679033 -0.491099 -0.703546 ... 0.085144 0.338052 0.070881 0.367426 0.509361 0.721611 0.855363 0.290441 -0.004898 0.272807 1942 -0.284608 -0.134428 -0.334139 1.513390 0.178323 -0.398191 -1.127591 -0.356363 0.749201 0.402916 ... -0.506888 -0.956246 -0.803839 -0.162990 -0.213912 -0.462882 0.138403 0.439809 -0.295757 0.332970 1943 0.931031 1.508049 1.777522 0.967493 0.836941 0.520904 -0.444128 -0.654865 -0.066033 -0.035638 ... -0.495529 0.493770 0.659627 -0.834037 -1.742745 -0.446306 -0.275613 -1.514215 -1.291886 -0.547804 1944 -0.799814 -0.954929 -0.645598 1.089000 0.379178 -2.049443 -1.156781 -1.620229 -0.118172 -0.314733 ... -0.995907 -2.718988 1.922668 0.230709 -1.741047 0.355268 -0.063067 1.540943 -0.970865 -3.165983 1945 1.035470 0.888933 1.426499 1.277414 2.548528 0.588647 0.596621 0.003606 0.368644 0.038259 ... 0.596695 -0.867976 0.016121 0.215585 -1.019165 0.174644 -0.980685 -0.502556 0.272979 0.495246 <p>1946 rows \u00d7 8337 columns</p> In\u00a0[6]: Copied! <pre>metadata\n</pre> metadata Out[6]: Fs load rot_speed state 0 48828 5 25 OR 1 48828 5 25 OR 2 48828 5 25 OR 3 48828 5 25 OR 4 48828 5 25 OR ... ... ... ... ... 1941 48828 3 25 IR 1942 48828 3 25 IR 1943 48828 3 25 IR 1944 48828 3 25 IR 1945 48828 3 25 IR <p>1946 rows \u00d7 4 columns</p> In\u00a0[9]: Copied! <pre>signals_env = env(signals)\nsignals_env.shape\n</pre> signals_env = env(signals) signals_env.shape Out[9]: <pre>(1946, 8337)</pre> In\u00a0[10]: Copied! <pre>window = scipy.signal.windows.hann(signals_env.shape[1])\nfreq_filter = scipy.signal.butter(25, [5, 23500], 'bandpass', fs = float(metadata.iloc[0, 0]), output='sos')\n</pre> window = scipy.signal.windows.hann(signals_env.shape[1]) freq_filter = scipy.signal.butter(25, [5, 23500], 'bandpass', fs = float(metadata.iloc[0, 0]), output='sos') In\u00a0[11]: Copied! <pre>signals_env_fft = fft(signals_env, freq_filter = freq_filter, window = window)\nsignals_env_fft.shape\n</pre> signals_env_fft = fft(signals_env, freq_filter = freq_filter, window = window) signals_env_fft.shape Out[11]: <pre>(1946, 4168)</pre> In\u00a0[12]: Copied! <pre>signals_env_ZoomedFFT = zoomed_fft(signals_env, 0, 2500, 2500, float(metadata.iloc[0, 0]), freq_filter = freq_filter, window = window)\nsignals_env_ZoomedFFT.shape\n</pre> signals_env_ZoomedFFT = zoomed_fft(signals_env, 0, 2500, 2500, float(metadata.iloc[0, 0]), freq_filter = freq_filter, window = window) signals_env_ZoomedFFT.shape Out[12]: <pre>(1946, 2500)</pre> In\u00a0[13]: Copied! <pre>STFT_window = scipy.signal.windows.hann(2400)\nSTFT_freq_filter = scipy.signal.butter(25, [5, 23500], 'bandpass', fs = float(metadata.iloc[0, 0]), output='sos')\nsignals_env_STFT = stft(signals_env, 2400, 200, STFT_freq_filter, STFT_window)\nsignals_env_STFT.shape\n</pre> STFT_window = scipy.signal.windows.hann(2400) STFT_freq_filter = scipy.signal.butter(25, [5, 23500], 'bandpass', fs = float(metadata.iloc[0, 0]), output='sos') signals_env_STFT = stft(signals_env, 2400, 200, STFT_freq_filter, STFT_window) signals_env_STFT.shape Out[13]: <pre>(1946, 30, 1200)</pre> In\u00a0[15]: Copied! <pre># Defining the feature-set to be extracted\nfeatures = {'mean': (np.mean, (), {}), 'var': (np.var, (), {}), 'rms': (rms, (), {})}\n</pre> # Defining the feature-set to be extracted features = {'mean': (np.mean, (), {}), 'var': (np.var, (), {}), 'rms': (rms, (), {})} In\u00a0[16]: Copied! <pre># Extracting the desired feature-set from time-domain signals\nfeatures_df = feature_extractor(signals, features)\nfeatures_df\n</pre> # Extracting the desired feature-set from time-domain signals features_df = feature_extractor(signals, features) features_df Out[16]: mean var rms 0 -0.191650 0.914853 0.975491 1 -0.187638 0.934313 0.984643 2 -0.186269 0.949628 0.992131 3 -0.188875 0.920510 0.977847 4 -0.187438 0.954540 0.994823 ... ... ... ... 1941 -0.186669 3.676619 1.926516 1942 -0.184197 3.627021 1.913361 1943 -0.183948 3.623858 1.912510 1944 -0.189699 3.634298 1.915799 1945 -0.188605 3.639548 1.917060 <p>1946 rows \u00d7 3 columns</p> In\u00a0[17]: Copied! <pre>import seaborn as sns\nfrom matplotlib import pyplot as plt\nsns.set()\n\nfig, axes = plt.subplots(4, 1, figsize = (16, 10))\n\nsns.lineplot(ax=axes[0], x=range(len(signals.iloc[0,:])), y = signals.iloc[0,:])\naxes[0].set_title(\"Original Time Signal\")\naxes[0].set_ylabel(\"Amplitude\")\naxes[0].set_xlabel(\"sample\")\naxes[0].set_xlim(0, 8337)\n\nsns.lineplot(ax=axes[1], x=range(len(signals_env.iloc[0,:])), y = signals_env.iloc[0, :])\naxes[1].set_title(\"Envelope\")\naxes[1].set_ylabel(\"Amplitude\")\naxes[1].set_xlabel(\"sample\")\naxes[1].set_xlim(0, 8337)\n\n\nsns.lineplot(ax=axes[2], x = fft_freq_axis(8337, 48828), y = signals_env_fft.iloc[0, :])\naxes[2].set_title(\"FFT\")\naxes[2].set_ylabel(\"Amplitude\")\naxes[2].set_xlabel(\"Frequency (Hz)\")\naxes[2].set_xlim(0, 24424)\n\n\nsns.lineplot(ax=axes[3], x = zoomed_fft_freq_axis(0, 2500, 2500), y = signals_env_ZoomedFFT.iloc[0, :])\naxes[3].set_title(\"Zoomed FFT\")\naxes[3].set_ylabel(\"Amplitude\")\naxes[3].set_xlabel(\"Frequency (Hz)\")\naxes[3].set_xlim(0, 2500)\n\n\nplt.subplots_adjust(hspace = 0.75)\nfig.show()\n</pre> import seaborn as sns from matplotlib import pyplot as plt sns.set()  fig, axes = plt.subplots(4, 1, figsize = (16, 10))  sns.lineplot(ax=axes[0], x=range(len(signals.iloc[0,:])), y = signals.iloc[0,:]) axes[0].set_title(\"Original Time Signal\") axes[0].set_ylabel(\"Amplitude\") axes[0].set_xlabel(\"sample\") axes[0].set_xlim(0, 8337)  sns.lineplot(ax=axes[1], x=range(len(signals_env.iloc[0,:])), y = signals_env.iloc[0, :]) axes[1].set_title(\"Envelope\") axes[1].set_ylabel(\"Amplitude\") axes[1].set_xlabel(\"sample\") axes[1].set_xlim(0, 8337)   sns.lineplot(ax=axes[2], x = fft_freq_axis(8337, 48828), y = signals_env_fft.iloc[0, :]) axes[2].set_title(\"FFT\") axes[2].set_ylabel(\"Amplitude\") axes[2].set_xlabel(\"Frequency (Hz)\") axes[2].set_xlim(0, 24424)   sns.lineplot(ax=axes[3], x = zoomed_fft_freq_axis(0, 2500, 2500), y = signals_env_ZoomedFFT.iloc[0, :]) axes[3].set_title(\"Zoomed FFT\") axes[3].set_ylabel(\"Amplitude\") axes[3].set_xlabel(\"Frequency (Hz)\") axes[3].set_xlim(0, 2500)   plt.subplots_adjust(hspace = 0.75) fig.show() In\u00a0[18]: Copied! <pre>t = np.linspace(0, 0.1707, 30)\nf = fft_freq_axis(2400, 48828)\n\nfig, ax = plt.subplots(figsize = (16, 8))\n\nax = sns.heatmap(signals_env_STFT[0, :, :], xticklabels = np.round(f, decimals = 2), yticklabels = np.round(t, decimals = 2), annot = False, cbar = False)\nax.set(xlabel = 'Frequency (Hz)', ylabel = 'Time (sec)')\nax.set_title('STFT')\nax.set_xticks(ax.get_xticks()[::30])\nax.set_yticks(ax.get_yticks()[::2])\n\n\nfig.show()\n</pre> t = np.linspace(0, 0.1707, 30) f = fft_freq_axis(2400, 48828)  fig, ax = plt.subplots(figsize = (16, 8))  ax = sns.heatmap(signals_env_STFT[0, :, :], xticklabels = np.round(f, decimals = 2), yticklabels = np.round(t, decimals = 2), annot = False, cbar = False) ax.set(xlabel = 'Frequency (Hz)', ylabel = 'Time (sec)') ax.set_title('STFT') ax.set_xticks(ax.get_xticks()[::30]) ax.set_yticks(ax.get_yticks()[::2])   fig.show() In\u00a0[18]: Copied! <pre>\n</pre>"},{"location":"notebooks/dataset_demos/MFPT_demo/#cloning-the-damavand-repository","title":"Cloning the damavand repository\u00b6","text":""},{"location":"notebooks/dataset_demos/MFPT_demo/#importings","title":"Importings\u00b6","text":""},{"location":"notebooks/dataset_demos/MFPT_demo/#instantiating-a-downloader-object","title":"Instantiating a downloader object\u00b6","text":""},{"location":"notebooks/dataset_demos/MFPT_demo/#instantiating-a-digestor-object","title":"Instantiating a digestor object\u00b6","text":""},{"location":"notebooks/dataset_demos/MFPT_demo/#aggregating-the-data-recorded-at-48828-hz","title":"Aggregating the data recorded at 48828 Hz\u00b6","text":""},{"location":"notebooks/dataset_demos/MFPT_demo/#signal-processing","title":"Signal Processing\u00b6","text":""},{"location":"notebooks/dataset_demos/MFPT_demo/#envelope-extraction","title":"Envelope Extraction\u00b6","text":""},{"location":"notebooks/dataset_demos/MFPT_demo/#fft","title":"FFT\u00b6","text":""},{"location":"notebooks/dataset_demos/MFPT_demo/#zoomedfft","title":"ZoomedFFT\u00b6","text":""},{"location":"notebooks/dataset_demos/MFPT_demo/#stft","title":"STFT\u00b6","text":""},{"location":"notebooks/dataset_demos/MFPT_demo/#statistical-features","title":"Statistical Features\u00b6","text":""},{"location":"notebooks/dataset_demos/MFPT_demo/#visualization","title":"Visualization\u00b6","text":""},{"location":"notebooks/dataset_demos/MaFaulDa_demo/","title":"Cloning the damavand repository","text":"In\u00a0[1]: Copied! <pre>!git clone https://github.com/amirberenji1995/damavand\n</pre> !git clone https://github.com/amirberenji1995/damavand <pre>Cloning into 'damavand'...\nremote: Enumerating objects: 263, done.\nremote: Counting objects: 100% (263/263), done.\nremote: Compressing objects: 100% (197/197), done.\nremote: Total 263 (delta 132), reused 196 (delta 65), pack-reused 0 (from 0)\nReceiving objects: 100% (263/263), 6.62 MiB | 13.62 MiB/s, done.\nResolving deltas: 100% (132/132), done.\n</pre> In\u00a0[2]: Copied! <pre>!pip install -r damavand/requirements.txt\n</pre> !pip install -r damavand/requirements.txt <pre>Collecting certifi==2024.7.4 (from -r damavand/requirements.txt (line 1))\n  Downloading certifi-2024.7.4-py3-none-any.whl.metadata (2.2 kB)\nCollecting charset-normalizer==3.3.2 (from -r damavand/requirements.txt (line 2))\n  Downloading charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\nCollecting idna==3.7 (from -r damavand/requirements.txt (line 3))\n  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\nCollecting numpy==1.26.4 (from -r damavand/requirements.txt (line 4))\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 61.0/61.0 kB 2.7 MB/s eta 0:00:00\nCollecting pandas==2.1.4 (from -r damavand/requirements.txt (line 5))\n  Downloading pandas-2.1.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\nRequirement already satisfied: python-dateutil==2.9.0.post0 in /usr/local/lib/python3.11/dist-packages (from -r damavand/requirements.txt (line 6)) (2.9.0.post0)\nCollecting pytz==2024.1 (from -r damavand/requirements.txt (line 7))\n  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\nCollecting rarfile==4.2 (from -r damavand/requirements.txt (line 8))\n  Downloading rarfile-4.2-py3-none-any.whl.metadata (4.4 kB)\nRequirement already satisfied: requests==2.32.3 in /usr/local/lib/python3.11/dist-packages (from -r damavand/requirements.txt (line 9)) (2.32.3)\nCollecting scipy==1.13.1 (from -r damavand/requirements.txt (line 10))\n  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 60.6/60.6 kB 2.9 MB/s eta 0:00:00\nCollecting six==1.16.0 (from -r damavand/requirements.txt (line 11))\n  Downloading six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\nCollecting tzdata==2024.1 (from -r damavand/requirements.txt (line 12))\n  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\nCollecting urllib3==2.2.2 (from -r damavand/requirements.txt (line 13))\n  Downloading urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\nDownloading certifi-2024.7.4-py3-none-any.whl (162 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 163.0/163.0 kB 6.0 MB/s eta 0:00:00\nDownloading charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (140 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 140.3/140.3 kB 4.6 MB/s eta 0:00:00\nDownloading idna-3.7-py3-none-any.whl (66 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 66.8/66.8 kB 4.7 MB/s eta 0:00:00\nDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 88.8 MB/s eta 0:00:00\nDownloading pandas-2.1.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 12.2/12.2 MB 102.0 MB/s eta 0:00:00\nDownloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 505.5/505.5 kB 31.1 MB/s eta 0:00:00\nDownloading rarfile-4.2-py3-none-any.whl (29 kB)\nDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 38.6/38.6 MB 17.9 MB/s eta 0:00:00\nDownloading six-1.16.0-py2.py3-none-any.whl (11 kB)\nDownloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 345.4/345.4 kB 17.1 MB/s eta 0:00:00\nDownloading urllib3-2.2.2-py3-none-any.whl (121 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 121.4/121.4 kB 9.2 MB/s eta 0:00:00\nInstalling collected packages: pytz, urllib3, tzdata, six, rarfile, numpy, idna, charset-normalizer, certifi, scipy, pandas\n  Attempting uninstall: pytz\n    Found existing installation: pytz 2025.2\n    Uninstalling pytz-2025.2:\n      Successfully uninstalled pytz-2025.2\n  Attempting uninstall: urllib3\n    Found existing installation: urllib3 2.5.0\n    Uninstalling urllib3-2.5.0:\n      Successfully uninstalled urllib3-2.5.0\n  Attempting uninstall: tzdata\n    Found existing installation: tzdata 2025.2\n    Uninstalling tzdata-2025.2:\n      Successfully uninstalled tzdata-2025.2\n  Attempting uninstall: six\n    Found existing installation: six 1.17.0\n    Uninstalling six-1.17.0:\n      Successfully uninstalled six-1.17.0\n  Attempting uninstall: numpy\n    Found existing installation: numpy 2.0.2\n    Uninstalling numpy-2.0.2:\n      Successfully uninstalled numpy-2.0.2\n  Attempting uninstall: idna\n    Found existing installation: idna 3.10\n    Uninstalling idna-3.10:\n      Successfully uninstalled idna-3.10\n  Attempting uninstall: charset-normalizer\n    Found existing installation: charset-normalizer 3.4.2\n    Uninstalling charset-normalizer-3.4.2:\n      Successfully uninstalled charset-normalizer-3.4.2\n  Attempting uninstall: certifi\n    Found existing installation: certifi 2025.7.14\n    Uninstalling certifi-2025.7.14:\n      Successfully uninstalled certifi-2025.7.14\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.16.0\n    Uninstalling scipy-1.16.0:\n      Successfully uninstalled scipy-1.16.0\n  Attempting uninstall: pandas\n    Found existing installation: pandas 2.2.2\n    Uninstalling pandas-2.2.2:\n      Successfully uninstalled pandas-2.2.2\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.1.4 which is incompatible.\nopencv-python 4.12.0.88 requires numpy&lt;2.3.0,&gt;=2; python_version &gt;= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy&lt;2.3.0,&gt;=2; python_version &gt;= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy&lt;2.3.0,&gt;=2; python_version &gt;= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nplotnine 0.14.5 requires pandas&gt;=2.2.0, but you have pandas 2.1.4 which is incompatible.\ntsfresh 0.21.0 requires scipy&gt;=1.14.0; python_version &gt;= \"3.10\", but you have scipy 1.13.1 which is incompatible.\nthinc 8.3.6 requires numpy&lt;3.0.0,&gt;=2.0.0, but you have numpy 1.26.4 which is incompatible.\nxarray 2025.7.1 requires pandas&gt;=2.2, but you have pandas 2.1.4 which is incompatible.\nmizani 0.13.5 requires pandas&gt;=2.2.0, but you have pandas 2.1.4 which is incompatible.\nSuccessfully installed certifi-2024.7.4 charset-normalizer-3.3.2 idna-3.7 numpy-1.26.4 pandas-2.1.4 pytz-2024.1 rarfile-4.2 scipy-1.13.1 six-1.16.0 tzdata-2024.1 urllib3-2.2.2\n</pre> In\u00a0[1]: Copied! <pre>from damavand.damavand.datasets.downloaders import read_addresses, MaFaulDaDownloader\nfrom damavand.damavand.datasets.digestors import MaFauldDa\nfrom damavand.damavand.signal_processing.transformations import *\nfrom damavand.damavand.signal_processing.feature_extraction import *\nfrom damavand.damavand.utils import *\nimport os\nimport pandas as pd\nimport numpy as np\n</pre> from damavand.damavand.datasets.downloaders import read_addresses, MaFaulDaDownloader from damavand.damavand.datasets.digestors import MaFauldDa from damavand.damavand.signal_processing.transformations import * from damavand.damavand.signal_processing.feature_extraction import * from damavand.damavand.utils import * import os import pandas as pd import numpy as np In\u00a0[2]: Copied! <pre>addresses = read_addresses()\ndownloader = MaFaulDaDownloader({key: addresses['MaFaulDa'][key] for key in ['normal.zip', 'imbalance.zip']})\ndownloader.download_extract('mafaulda_zip_files/', 'mafaulda/')\n</pre> addresses = read_addresses() downloader = MaFaulDaDownloader({key: addresses['MaFaulDa'][key] for key in ['normal.zip', 'imbalance.zip']}) downloader.download_extract('mafaulda_zip_files/', 'mafaulda/') <pre>Downloading:  normal.zip\nDownloading:  imbalance.zip\nExtracting:  normal.zip\nExtracting:  imbalance.zip\n</pre> In\u00a0[3]: Copied! <pre>mafaulda = MaFauldDa('mafaulda/', os.listdir('mafaulda/'), channels = [2])\nmining_params = {\n    'win_len': 50000,\n    'hop_len': 50000\n}\nmafaulda.mine(mining_params)\n</pre> mafaulda = MaFauldDa('mafaulda/', os.listdir('mafaulda/'), channels = [2]) mining_params = {     'win_len': 50000,     'hop_len': 50000 } mafaulda.mine(mining_params) In\u00a0[4]: Copied! <pre>df = pd.concat(mafaulda.data[2]).reset_index(drop = True)\ndf\n</pre> df = pd.concat(mafaulda.data[2]).reset_index(drop = True) df Out[4]: 0 1 2 3 4 5 6 7 8 9 ... 49992 49993 49994 49995 49996 49997 49998 49999 state severity 0 -0.769800 0.148490 -0.444640 -0.173230 -0.061578 -0.561010 0.245860 -0.681590 0.283750 -0.558160 ... -0.22701 -0.679750 0.081710 -0.79798 0.16689 -0.780670 -0.096885 -0.505490 imbalance 25g 1 -0.396390 -0.084910 -0.706400 0.098014 -0.841570 0.102120 -0.591070 -0.107300 -0.308920 -0.391270 ... 0.15224 0.782750 0.366870 0.45279 0.63565 0.035932 0.801350 -0.068741 imbalance 25g 2 0.722450 -0.069506 0.381250 0.151030 0.075812 0.530700 -0.086285 0.769020 -0.080516 0.756980 ... -1.20120 -0.258320 -1.213800 -0.41387 -0.99852 -0.755430 -0.634810 -1.101700 imbalance 25g 3 -0.359160 -1.285900 -0.362660 -1.271400 -0.643180 -0.767570 -0.732520 -0.547600 -1.075200 -0.298390 ... 0.70043 1.234100 0.529770 1.34420 0.42482 1.233200 0.657640 0.931680 imbalance 25g 4 1.061400 0.727540 1.370700 0.536410 1.378800 0.673500 1.267900 0.823470 0.929870 1.101900 ... -0.69956 -0.993960 -0.940630 -0.66817 -1.35880 -0.389490 -1.312700 -0.372960 imbalance 25g ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 1905 -0.029689 0.148640 -0.349120 0.342740 -0.351850 0.340670 -0.284620 0.096903 0.110810 -0.026946 ... -0.18006 0.186760 0.040866 -0.10206 0.31797 -0.285560 0.445120 -0.277080 normal _ 1906 0.358070 -0.107990 0.142550 0.164280 -0.129200 0.408630 -0.306600 0.481290 -0.260050 0.322450 ... -0.42628 0.417400 -0.364720 0.26404 -0.12011 -0.061874 0.209820 -0.326420 normal _ 1907 0.408730 -0.454570 0.397690 -0.295990 0.192650 -0.014766 -0.134890 0.289990 -0.393890 0.454840 ... -0.27756 0.095386 -0.642190 0.29521 -0.71886 0.263110 -0.439650 0.039742 normal _ 1908 -0.113390 -0.273610 0.240340 -0.373970 0.405140 -0.375480 0.324690 -0.205440 0.054960 0.064024 ... 0.16796 -0.095137 -0.121100 0.22238 -0.31794 0.381740 -0.369100 0.364300 normal _ 1909 -0.188880 0.155550 0.032046 -0.168030 0.299780 -0.286990 0.388050 -0.355930 0.284460 -0.112620 ... 0.39862 -0.323430 0.202300 -0.02139 -0.02897 0.245700 -0.351000 0.417440 normal _ <p>1910 rows \u00d7 50002 columns</p> In\u00a0[5]: Copied! <pre>signals, metadata = df.iloc[:, : -2], df.iloc[:, -2 :]\nsignals\n</pre> signals, metadata = df.iloc[:, : -2], df.iloc[:, -2 :] signals Out[5]: 0 1 2 3 4 5 6 7 8 9 ... 49990 49991 49992 49993 49994 49995 49996 49997 49998 49999 0 -0.769800 0.148490 -0.444640 -0.173230 -0.061578 -0.561010 0.245860 -0.681590 0.283750 -0.558160 ... -0.53685 -0.28361 -0.22701 -0.679750 0.081710 -0.79798 0.16689 -0.780670 -0.096885 -0.505490 1 -0.396390 -0.084910 -0.706400 0.098014 -0.841570 0.102120 -0.591070 -0.107300 -0.308920 -0.391270 ... 0.15249 0.97625 0.15224 0.782750 0.366870 0.45279 0.63565 0.035932 0.801350 -0.068741 2 0.722450 -0.069506 0.381250 0.151030 0.075812 0.530700 -0.086285 0.769020 -0.080516 0.756980 ... -0.94616 -0.37918 -1.20120 -0.258320 -1.213800 -0.41387 -0.99852 -0.755430 -0.634810 -1.101700 3 -0.359160 -1.285900 -0.362660 -1.271400 -0.643180 -0.767570 -0.732520 -0.547600 -1.075200 -0.298390 ... 1.09670 0.96135 0.70043 1.234100 0.529770 1.34420 0.42482 1.233200 0.657640 0.931680 4 1.061400 0.727540 1.370700 0.536410 1.378800 0.673500 1.267900 0.823470 0.929870 1.101900 ... -0.49750 -1.44260 -0.69956 -0.993960 -0.940630 -0.66817 -1.35880 -0.389490 -1.312700 -0.372960 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 1905 -0.029689 0.148640 -0.349120 0.342740 -0.351850 0.340670 -0.284620 0.096903 0.110810 -0.026946 ... -0.31529 0.40178 -0.18006 0.186760 0.040866 -0.10206 0.31797 -0.285560 0.445120 -0.277080 1906 0.358070 -0.107990 0.142550 0.164280 -0.129200 0.408630 -0.306600 0.481290 -0.260050 0.322450 ... -0.18523 0.36920 -0.42628 0.417400 -0.364720 0.26404 -0.12011 -0.061874 0.209820 -0.326420 1907 0.408730 -0.454570 0.397690 -0.295990 0.192650 -0.014766 -0.134890 0.289990 -0.393890 0.454840 ... 0.14742 -0.28190 -0.27756 0.095386 -0.642190 0.29521 -0.71886 0.263110 -0.439650 0.039742 1908 -0.113390 -0.273610 0.240340 -0.373970 0.405140 -0.375480 0.324690 -0.205440 0.054960 0.064024 ... 0.34270 -0.35081 0.16796 -0.095137 -0.121100 0.22238 -0.31794 0.381740 -0.369100 0.364300 1909 -0.188880 0.155550 0.032046 -0.168030 0.299780 -0.286990 0.388050 -0.355930 0.284460 -0.112620 ... 0.38898 -0.48163 0.39862 -0.323430 0.202300 -0.02139 -0.02897 0.245700 -0.351000 0.417440 <p>1910 rows \u00d7 50000 columns</p> In\u00a0[6]: Copied! <pre>metadata\n</pre> metadata Out[6]: state severity 0 imbalance 25g 1 imbalance 25g 2 imbalance 25g 3 imbalance 25g 4 imbalance 25g ... ... ... 1905 normal _ 1906 normal _ 1907 normal _ 1908 normal _ 1909 normal _ <p>1910 rows \u00d7 2 columns</p> In\u00a0[7]: Copied! <pre>metadata['comb'] = metadata['state'] + '_' + metadata['severity']\nmetadata['comb'].value_counts()\n</pre> metadata['comb'] = metadata['state'] + '_' + metadata['severity'] metadata['comb'].value_counts() Out[7]: count comb imbalance_6g 245 imbalance_20g 245 normal__ 245 imbalance_15g 240 imbalance_10g 240 imbalance_25g 235 imbalance_30g 235 imbalance_35g 225 dtype: int64 In\u00a0[8]: Copied! <pre>signals_env = env(signals)\nsignals_env.shape\n</pre> signals_env = env(signals) signals_env.shape Out[8]: <pre>(1910, 50000)</pre> In\u00a0[9]: Copied! <pre># Defenition of a window (to avoid leakage error) and a bandpass frequency filter (to both avoid aliasing and DC-component removal)\nwindow = scipy.signal.windows.hann(signals_env.shape[1])\nfreq_filter = scipy.signal.butter(25, [5, 24500], 'bandpass', fs = 51200, output='sos')\n</pre> # Defenition of a window (to avoid leakage error) and a bandpass frequency filter (to both avoid aliasing and DC-component removal) window = scipy.signal.windows.hann(signals_env.shape[1]) freq_filter = scipy.signal.butter(25, [5, 24500], 'bandpass', fs = 51200, output='sos') In\u00a0[10]: Copied! <pre>signals_env_fft = fft(signals_env, freq_filter = freq_filter, window = window)\nsignals_env_fft.shape\n</pre> signals_env_fft = fft(signals_env, freq_filter = freq_filter, window = window) signals_env_fft.shape Out[10]: <pre>(1910, 25000)</pre> In\u00a0[11]: Copied! <pre>signals_env_ZoomedFFT = zoomed_fft(signals_env, 0, 1000, 2000, 51200, freq_filter = freq_filter, window = window)\nsignals_env_ZoomedFFT.shape\n</pre> signals_env_ZoomedFFT = zoomed_fft(signals_env, 0, 1000, 2000, 51200, freq_filter = freq_filter, window = window) signals_env_ZoomedFFT.shape Out[11]: <pre>(1910, 2000)</pre> In\u00a0[12]: Copied! <pre># Defenition of a window (to avoid leakage error) and a bandpass frequency filter (to both avoid aliasing and DC-component removal)\nSTFT_window = scipy.signal.windows.hann(2048)\nSTFT_freq_filter = scipy.signal.butter(25, [5, 24500], 'bandpass', fs = 51200, output='sos')\n</pre> # Defenition of a window (to avoid leakage error) and a bandpass frequency filter (to both avoid aliasing and DC-component removal) STFT_window = scipy.signal.windows.hann(2048) STFT_freq_filter = scipy.signal.butter(25, [5, 24500], 'bandpass', fs = 51200, output='sos') In\u00a0[13]: Copied! <pre>signals_env_STFT = stft(signals_env, 2048, 1000, STFT_freq_filter, STFT_window)\nsignals_env_STFT.shape\n</pre> signals_env_STFT = stft(signals_env, 2048, 1000, STFT_freq_filter, STFT_window) signals_env_STFT.shape Out[13]: <pre>(1910, 48, 1024)</pre> In\u00a0[14]: Copied! <pre># Defining the feature-set to be extracted\nfeatures = {'mean': (np.mean, (), {}), 'var': (np.var, (), {}), 'rms': (rms, (), {})}\n</pre> # Defining the feature-set to be extracted features = {'mean': (np.mean, (), {}), 'var': (np.var, (), {}), 'rms': (rms, (), {})} In\u00a0[14]: Copied! <pre>\n</pre> In\u00a0[15]: Copied! <pre># Extracting the desired feature-set from time-domain signals\nfeatures_df = feature_extractor(signals, features)\nfeatures_df\n</pre> # Extracting the desired feature-set from time-domain signals features_df = feature_extractor(signals, features) features_df Out[15]: mean var rms 0 -0.012679 0.527956 0.726716 1 0.011876 0.529248 0.727591 2 -0.009493 0.528195 0.726832 3 0.005920 0.526064 0.725327 4 -0.001124 0.519390 0.720688 ... ... ... ... 1905 -0.001377 0.096043 0.309910 1906 0.000114 0.095341 0.308773 1907 -0.001045 0.096605 0.310816 1908 0.000556 0.094486 0.307386 1909 -0.000644 0.096400 0.310484 <p>1910 rows \u00d7 3 columns</p> In\u00a0[16]: Copied! <pre>import seaborn as sns\nfrom matplotlib import pyplot as plt\nsns.set()\n\nfig, axes = plt.subplots(4, 1, figsize = (16, 10))\n\nsns.lineplot(ax=axes[0], x=range(len(signals.iloc[0,:])), y = signals.iloc[0,:])\naxes[0].set_title(\"Original Time Signal\")\naxes[0].set_ylabel(\"Amplitude\")\naxes[0].set_xlabel(\"sample\")\naxes[0].set_xlim(0, 12000)\n\nsns.lineplot(ax=axes[1], x=range(len(signals_env.iloc[0,:])), y = signals_env.iloc[0, :])\naxes[1].set_title(\"Envelope\")\naxes[1].set_ylabel(\"Amplitude\")\naxes[1].set_xlabel(\"sample\")\naxes[1].set_xlim(0, 12000)\n\n\nsns.lineplot(ax=axes[2], x = fft_freq_axis(50000, 51200), y = signals_env_fft.iloc[0, :])\naxes[2].set_title(\"FFT\")\naxes[2].set_ylabel(\"Amplitude\")\naxes[2].set_xlabel(\"Frequency (Hz)\")\naxes[2].set_xlim(0, 6000)\n\n\nsns.lineplot(ax=axes[3], x = zoomed_fft_freq_axis(0, 1000, 2000), y = signals_env_ZoomedFFT.iloc[0, :])\naxes[3].set_title(\"Zoomed FFT\")\naxes[3].set_ylabel(\"Amplitude\")\naxes[3].set_xlabel(\"Frequency (Hz)\")\naxes[3].set_xlim(0, 1000)\n\n\nplt.subplots_adjust(hspace = 0.75)\nfig.show()\n</pre> import seaborn as sns from matplotlib import pyplot as plt sns.set()  fig, axes = plt.subplots(4, 1, figsize = (16, 10))  sns.lineplot(ax=axes[0], x=range(len(signals.iloc[0,:])), y = signals.iloc[0,:]) axes[0].set_title(\"Original Time Signal\") axes[0].set_ylabel(\"Amplitude\") axes[0].set_xlabel(\"sample\") axes[0].set_xlim(0, 12000)  sns.lineplot(ax=axes[1], x=range(len(signals_env.iloc[0,:])), y = signals_env.iloc[0, :]) axes[1].set_title(\"Envelope\") axes[1].set_ylabel(\"Amplitude\") axes[1].set_xlabel(\"sample\") axes[1].set_xlim(0, 12000)   sns.lineplot(ax=axes[2], x = fft_freq_axis(50000, 51200), y = signals_env_fft.iloc[0, :]) axes[2].set_title(\"FFT\") axes[2].set_ylabel(\"Amplitude\") axes[2].set_xlabel(\"Frequency (Hz)\") axes[2].set_xlim(0, 6000)   sns.lineplot(ax=axes[3], x = zoomed_fft_freq_axis(0, 1000, 2000), y = signals_env_ZoomedFFT.iloc[0, :]) axes[3].set_title(\"Zoomed FFT\") axes[3].set_ylabel(\"Amplitude\") axes[3].set_xlabel(\"Frequency (Hz)\") axes[3].set_xlim(0, 1000)   plt.subplots_adjust(hspace = 0.75) fig.show() In\u00a0[17]: Copied! <pre>t = np.linspace(0, 50000/52600, 48)\nf = fft_freq_axis(2048, 51200)\n\nfig, ax = plt.subplots(figsize = (16, 8))\n\nax = sns.heatmap(signals_env_STFT[0, :, :], xticklabels = np.round(f, decimals = 2), yticklabels = np.round(t, decimals = 2), annot = False, cbar = False)\nax.set(xlabel = 'Frequency (Hz)', ylabel = 'Time (sec)')\nax.set_title('STFT')\nax.set_xticks(ax.get_xticks()[::20])\nax.set_yticks(ax.get_yticks()[::5])\n\n\nfig.show()\n</pre> t = np.linspace(0, 50000/52600, 48) f = fft_freq_axis(2048, 51200)  fig, ax = plt.subplots(figsize = (16, 8))  ax = sns.heatmap(signals_env_STFT[0, :, :], xticklabels = np.round(f, decimals = 2), yticklabels = np.round(t, decimals = 2), annot = False, cbar = False) ax.set(xlabel = 'Frequency (Hz)', ylabel = 'Time (sec)') ax.set_title('STFT') ax.set_xticks(ax.get_xticks()[::20]) ax.set_yticks(ax.get_yticks()[::5])   fig.show()"},{"location":"notebooks/dataset_demos/MaFaulDa_demo/#cloning-the-damavand-repository","title":"Cloning the damavand repository\u00b6","text":""},{"location":"notebooks/dataset_demos/MaFaulDa_demo/#importings","title":"Importings\u00b6","text":""},{"location":"notebooks/dataset_demos/MaFaulDa_demo/#instantiating-a-downloader-object","title":"Instantiating a downloader object\u00b6","text":""},{"location":"notebooks/dataset_demos/MaFaulDa_demo/#instantiating-a-digestor-object","title":"Instantiating a digestor object\u00b6","text":""},{"location":"notebooks/dataset_demos/MaFaulDa_demo/#aggregating-data-over-the-third-channel","title":"Aggregating data over the third channel\u00b6","text":""},{"location":"notebooks/dataset_demos/MaFaulDa_demo/#signal-processing","title":"Signal Processing\u00b6","text":""},{"location":"notebooks/dataset_demos/MaFaulDa_demo/#envelope-extraction","title":"Envelope Extraction\u00b6","text":""},{"location":"notebooks/dataset_demos/MaFaulDa_demo/#fft","title":"FFT\u00b6","text":""},{"location":"notebooks/dataset_demos/MaFaulDa_demo/#zoomedfft","title":"ZoomedFFT\u00b6","text":""},{"location":"notebooks/dataset_demos/MaFaulDa_demo/#stft","title":"STFT\u00b6","text":""},{"location":"notebooks/dataset_demos/MaFaulDa_demo/#statistical-features","title":"Statistical Features\u00b6","text":""},{"location":"notebooks/dataset_demos/MaFaulDa_demo/#visualization","title":"Visualization\u00b6","text":""},{"location":"notebooks/dataset_demos/PU_demo/","title":"Cloning the damavand repository","text":"In\u00a0[1]: Copied! <pre>!git clone https://github.com/amirberenji1995/damavand\n</pre> !git clone https://github.com/amirberenji1995/damavand <pre>Cloning into 'damavand'...\nremote: Enumerating objects: 263, done.\nremote: Counting objects: 100% (263/263), done.\nremote: Compressing objects: 100% (197/197), done.\nremote: Total 263 (delta 132), reused 196 (delta 65), pack-reused 0 (from 0)\nReceiving objects: 100% (263/263), 6.62 MiB | 12.75 MiB/s, done.\nResolving deltas: 100% (132/132), done.\n</pre> In\u00a0[2]: Copied! <pre>!pip install -r damavand/requirements.txt\n</pre> !pip install -r damavand/requirements.txt <pre>Collecting certifi==2024.7.4 (from -r damavand/requirements.txt (line 1))\n  Downloading certifi-2024.7.4-py3-none-any.whl.metadata (2.2 kB)\nCollecting charset-normalizer==3.3.2 (from -r damavand/requirements.txt (line 2))\n  Downloading charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\nCollecting idna==3.7 (from -r damavand/requirements.txt (line 3))\n  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\nCollecting numpy==1.26.4 (from -r damavand/requirements.txt (line 4))\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 61.0/61.0 kB 4.9 MB/s eta 0:00:00\nCollecting pandas==2.1.4 (from -r damavand/requirements.txt (line 5))\n  Downloading pandas-2.1.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\nRequirement already satisfied: python-dateutil==2.9.0.post0 in /usr/local/lib/python3.11/dist-packages (from -r damavand/requirements.txt (line 6)) (2.9.0.post0)\nCollecting pytz==2024.1 (from -r damavand/requirements.txt (line 7))\n  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\nCollecting rarfile==4.2 (from -r damavand/requirements.txt (line 8))\n  Downloading rarfile-4.2-py3-none-any.whl.metadata (4.4 kB)\nRequirement already satisfied: requests==2.32.3 in /usr/local/lib/python3.11/dist-packages (from -r damavand/requirements.txt (line 9)) (2.32.3)\nCollecting scipy==1.13.1 (from -r damavand/requirements.txt (line 10))\n  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 60.6/60.6 kB 5.5 MB/s eta 0:00:00\nCollecting six==1.16.0 (from -r damavand/requirements.txt (line 11))\n  Downloading six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\nCollecting tzdata==2024.1 (from -r damavand/requirements.txt (line 12))\n  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\nCollecting urllib3==2.2.2 (from -r damavand/requirements.txt (line 13))\n  Downloading urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\nDownloading certifi-2024.7.4-py3-none-any.whl (162 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 163.0/163.0 kB 14.8 MB/s eta 0:00:00\nDownloading charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (140 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 140.3/140.3 kB 12.7 MB/s eta 0:00:00\nDownloading idna-3.7-py3-none-any.whl (66 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 66.8/66.8 kB 5.7 MB/s eta 0:00:00\nDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 81.6 MB/s eta 0:00:00\nDownloading pandas-2.1.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 12.2/12.2 MB 109.7 MB/s eta 0:00:00\nDownloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 505.5/505.5 kB 38.7 MB/s eta 0:00:00\nDownloading rarfile-4.2-py3-none-any.whl (29 kB)\nDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 38.6/38.6 MB 18.6 MB/s eta 0:00:00\nDownloading six-1.16.0-py2.py3-none-any.whl (11 kB)\nDownloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 345.4/345.4 kB 31.2 MB/s eta 0:00:00\nDownloading urllib3-2.2.2-py3-none-any.whl (121 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 121.4/121.4 kB 9.6 MB/s eta 0:00:00\nInstalling collected packages: pytz, urllib3, tzdata, six, rarfile, numpy, idna, charset-normalizer, certifi, scipy, pandas\n  Attempting uninstall: pytz\n    Found existing installation: pytz 2025.2\n    Uninstalling pytz-2025.2:\n      Successfully uninstalled pytz-2025.2\n  Attempting uninstall: urllib3\n    Found existing installation: urllib3 2.5.0\n    Uninstalling urllib3-2.5.0:\n      Successfully uninstalled urllib3-2.5.0\n  Attempting uninstall: tzdata\n    Found existing installation: tzdata 2025.2\n    Uninstalling tzdata-2025.2:\n      Successfully uninstalled tzdata-2025.2\n  Attempting uninstall: six\n    Found existing installation: six 1.17.0\n    Uninstalling six-1.17.0:\n      Successfully uninstalled six-1.17.0\n  Attempting uninstall: numpy\n    Found existing installation: numpy 2.0.2\n    Uninstalling numpy-2.0.2:\n      Successfully uninstalled numpy-2.0.2\n  Attempting uninstall: idna\n    Found existing installation: idna 3.10\n    Uninstalling idna-3.10:\n      Successfully uninstalled idna-3.10\n  Attempting uninstall: charset-normalizer\n    Found existing installation: charset-normalizer 3.4.2\n    Uninstalling charset-normalizer-3.4.2:\n      Successfully uninstalled charset-normalizer-3.4.2\n  Attempting uninstall: certifi\n    Found existing installation: certifi 2025.7.14\n    Uninstalling certifi-2025.7.14:\n      Successfully uninstalled certifi-2025.7.14\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.16.0\n    Uninstalling scipy-1.16.0:\n      Successfully uninstalled scipy-1.16.0\n  Attempting uninstall: pandas\n    Found existing installation: pandas 2.2.2\n    Uninstalling pandas-2.2.2:\n      Successfully uninstalled pandas-2.2.2\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.1.4 which is incompatible.\nopencv-python 4.12.0.88 requires numpy&lt;2.3.0,&gt;=2; python_version &gt;= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy&lt;2.3.0,&gt;=2; python_version &gt;= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy&lt;2.3.0,&gt;=2; python_version &gt;= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nplotnine 0.14.5 requires pandas&gt;=2.2.0, but you have pandas 2.1.4 which is incompatible.\ntsfresh 0.21.0 requires scipy&gt;=1.14.0; python_version &gt;= \"3.10\", but you have scipy 1.13.1 which is incompatible.\nthinc 8.3.6 requires numpy&lt;3.0.0,&gt;=2.0.0, but you have numpy 1.26.4 which is incompatible.\nxarray 2025.7.1 requires pandas&gt;=2.2, but you have pandas 2.1.4 which is incompatible.\nmizani 0.13.5 requires pandas&gt;=2.2.0, but you have pandas 2.1.4 which is incompatible.\nSuccessfully installed certifi-2024.7.4 charset-normalizer-3.3.2 idna-3.7 numpy-1.26.4 pandas-2.1.4 pytz-2024.1 rarfile-4.2 scipy-1.13.1 six-1.16.0 tzdata-2024.1 urllib3-2.2.2\n</pre> In\u00a0[1]: Copied! <pre>from damavand.damavand.datasets.downloaders import read_addresses, PuDownloader\nfrom damavand.damavand.datasets.digestors import PU\nfrom damavand.damavand.signal_processing.transformations import *\nfrom damavand.damavand.signal_processing.feature_extraction import *\nfrom damavand.damavand.utils import *\nimport os\nimport pandas as pd\nimport numpy as np\nimport scipy\n</pre> from damavand.damavand.datasets.downloaders import read_addresses, PuDownloader from damavand.damavand.datasets.digestors import PU from damavand.damavand.signal_processing.transformations import * from damavand.damavand.signal_processing.feature_extraction import * from damavand.damavand.utils import * import os import pandas as pd import numpy as np import scipy In\u00a0[2]: Copied! <pre>addresses = read_addresses()\naddresses['PU'].pop('real_damage')\ndownloader = PuDownloader(addresses['PU'])\ndownloader.download_extract(download_path = 'PU_compressed/', extraction_path = 'PU/', timeout = 10)\n</pre> addresses = read_addresses() addresses['PU'].pop('real_damage') downloader = PuDownloader(addresses['PU']) downloader.download_extract(download_path = 'PU_compressed/', extraction_path = 'PU/', timeout = 10) <pre>Downloading:  K001.rar\nDownloading:  K002.rar\nDownloading:  K003.rar\nDownloading:  K004.rar\nDownloading:  K005.rar\nDownloading:  K006.rar\nDownloading:  KA01.rar\nDownloading:  KA03.rar\nDownloading:  KA05.rar\nDownloading:  KA06.rar\nDownloading:  KA07.rar\nDownloading:  KA08.rar\nDownloading:  KA09.rar\nDownloading:  KI01.rar\nDownloading:  KI03.rar\nDownloading:  KI05.rar\nDownloading:  KI07.rar\nDownloading:  KI08.rar\nExtracting:  K001.rar\nExtracting:  K002.rar\nExtracting:  K003.rar\nExtracting:  K004.rar\nExtracting:  K005.rar\nExtracting:  K006.rar\nExtracting:  KA01.rar\nExtracting:  KA03.rar\nExtracting:  KA05.rar\nExtracting:  KA06.rar\nExtracting:  KA07.rar\nExtracting:  KA08.rar\nExtracting:  KA09.rar\nExtracting:  KI01.rar\nExtracting:  KI03.rar\nExtracting:  KI05.rar\nExtracting:  KI07.rar\nExtracting:  KI08.rar\n</pre> In\u00a0[3]: Copied! <pre>mining_params = {'win_len': 16000, 'hop_len': 16000}\n\npu = PU('PU/', os.listdir('PU/'), ['Vib'],reps = [1])\npu.mine(mining_params)\n</pre> mining_params = {'win_len': 16000, 'hop_len': 16000}  pu = PU('PU/', os.listdir('PU/'), ['Vib'],reps = [1]) pu.mine(mining_params) In\u00a0[\u00a0]: Copied! <pre>df = pd.concat(pu.data['Vib']).reset_index(drop = True)\ndf\n</pre> df = pd.concat(pu.data['Vib']).reset_index(drop = True) df In\u00a0[5]: Copied! <pre>signals, metadata = df.iloc[:, : -5], df.iloc[:, -5 :]\nsignals\n</pre> signals, metadata = df.iloc[:, : -5], df.iloc[:, -5 :] signals Out[5]: 0 1 2 3 4 5 6 7 8 9 ... 15990 15991 15992 15993 15994 15995 15996 15997 15998 15999 0 0.012207 0.057983 0.085449 0.177002 0.039673 0.027466 0.030518 -0.003052 0.018311 -0.051880 ... -0.158691 -0.115967 0.009155 0.170898 0.195312 0.189209 -0.018311 -0.213623 -0.146484 -0.042725 1 -0.033569 -0.067139 -0.231934 -0.149536 -0.177002 -0.009155 0.067139 0.088501 -0.061035 -0.231934 ... -0.082397 0.500488 0.653076 0.244141 -0.363159 -0.650024 -0.491333 0.061035 0.424194 0.360107 2 -0.125122 -0.680542 -0.949097 -0.793457 -0.360107 -0.045776 -0.067139 -0.189209 -0.286865 -0.115967 ... 0.692749 0.692749 0.802612 0.524902 -0.070190 -0.701904 -1.309204 -1.556396 -1.226807 -1.062012 3 -0.851440 -0.515747 -0.225830 0.033569 0.262451 0.360107 0.140381 -0.082397 -0.338745 -0.280762 ... -0.338745 -0.180054 -0.305176 0.143433 0.137329 0.787354 0.100708 0.250244 -0.082397 0.274658 4 0.030518 -0.744629 -0.073242 0.335693 -0.140381 0.885010 0.137329 1.092529 0.027466 -0.967407 ... -0.186157 -0.073242 0.045776 0.183105 0.146484 0.094604 1.336670 0.100708 0.765991 0.332642 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 1147 -0.057983 0.100708 -0.048828 0.152588 0.125122 0.064087 -0.048828 0.012207 -0.283813 -0.054932 ... 0.109863 -0.207520 0.125122 0.161743 0.109863 0.210571 -0.012207 -0.146484 -0.155640 -0.234985 1148 -0.119019 0.067139 0.009155 0.180054 0.051880 0.509644 0.009155 0.106812 0.030518 0.051880 ... -0.146484 -0.079346 -0.198364 -0.137329 -0.131226 -0.054932 0.003052 0.119019 -0.054932 0.076294 1149 0.018311 0.015259 -0.036621 -0.012207 -0.027466 -0.057983 -0.027466 -0.079346 0.039673 0.006104 ... -0.085449 0.204468 -0.054932 -0.057983 -0.042725 1.083374 -0.134277 0.161743 -0.094604 -0.436401 1150 0.051880 1.135254 0.042725 -0.802612 -0.082397 -0.836182 -0.140381 -0.262451 -0.003052 0.250244 ... 0.000000 -0.131226 -0.036621 -0.085449 -0.109863 -0.097656 -0.186157 -0.149536 1.480103 -0.057983 1151 -0.042725 0.039673 0.076294 0.030518 0.024414 0.094604 0.131226 0.085449 0.021362 -0.024414 ... 0.000000 -0.042725 0.003052 -0.143433 -0.094604 -0.094604 -0.024414 -0.045776 0.024414 -0.018311 <p>1152 rows \u00d7 16000 columns</p> In\u00a0[6]: Copied! <pre>metadata\n</pre> metadata Out[6]: rot_speed load_torque radial_force code rep 0 N15 M07 F04 KI01 1 1 N15 M07 F04 KI01 1 2 N15 M07 F04 KI01 1 3 N15 M07 F04 KI01 1 4 N15 M07 F04 KI01 1 ... ... ... ... ... ... 1147 N09 M07 F10 K001 1 1148 N09 M07 F10 K001 1 1149 N09 M07 F10 K001 1 1150 N09 M07 F10 K001 1 1151 N09 M07 F10 K001 1 <p>1152 rows \u00d7 5 columns</p> In\u00a0[7]: Copied! <pre>signals_env = env(signals)\nsignals_env.shape\n</pre> signals_env = env(signals) signals_env.shape Out[7]: <pre>(1152, 16000)</pre> In\u00a0[8]: Copied! <pre>window = scipy.signal.windows.hann(signals_env.shape[1])\nfreq_filter = scipy.signal.butter(25, [5, 31000], 'bandpass', fs = 64000, output='sos')\n</pre> window = scipy.signal.windows.hann(signals_env.shape[1]) freq_filter = scipy.signal.butter(25, [5, 31000], 'bandpass', fs = 64000, output='sos') In\u00a0[9]: Copied! <pre>signals_env_fft = fft(signals_env, freq_filter = freq_filter, window = window)\nsignals_env_fft.shape\n</pre> signals_env_fft = fft(signals_env, freq_filter = freq_filter, window = window) signals_env_fft.shape Out[9]: <pre>(1152, 8000)</pre> In\u00a0[10]: Copied! <pre>signals_env_ZoomedFFT = zoomed_fft(signals_env, 0, 1000, 2000, 64000, freq_filter = freq_filter, window = window)\nsignals_env_ZoomedFFT.shape\n</pre> signals_env_ZoomedFFT = zoomed_fft(signals_env, 0, 1000, 2000, 64000, freq_filter = freq_filter, window = window) signals_env_ZoomedFFT.shape Out[10]: <pre>(1152, 2000)</pre> In\u00a0[11]: Copied! <pre>STFT_window = scipy.signal.windows.hann(2000)\nSTFT_freq_filter = scipy.signal.butter(25, [5, 31000], 'bandpass', fs = 64000, output='sos')\nsignals_env_STFT = stft(signals_env, 2000, 200, STFT_freq_filter, STFT_window)\nsignals_env_STFT.shape\n</pre> STFT_window = scipy.signal.windows.hann(2000) STFT_freq_filter = scipy.signal.butter(25, [5, 31000], 'bandpass', fs = 64000, output='sos') signals_env_STFT = stft(signals_env, 2000, 200, STFT_freq_filter, STFT_window) signals_env_STFT.shape Out[11]: <pre>(1152, 71, 1000)</pre> In\u00a0[16]: Copied! <pre># Defining the feature-set to be extracted\nfeatures = {'mean': (np.mean, (), {}), 'var': (np.var, (), {}), 'rms': (rms, (), {})}\n</pre> # Defining the feature-set to be extracted features = {'mean': (np.mean, (), {}), 'var': (np.var, (), {}), 'rms': (rms, (), {})} In\u00a0[17]: Copied! <pre># Extracting the desired feature-set from time-domain signals\nfeatures_df = feature_extractor(signals, features)\nfeatures_df\n</pre> # Extracting the desired feature-set from time-domain signals features_df = feature_extractor(signals, features) features_df Out[17]: mean var rms 0 -0.014983 0.267011 0.516949 1 -0.014994 0.357743 0.598304 2 -0.015197 0.325597 0.570813 3 -0.011013 0.286674 0.535533 4 -0.011621 0.383353 0.619264 ... ... ... ... 1147 -0.013029 0.120181 0.346917 1148 -0.019953 0.138645 0.372884 1149 -0.019041 0.140663 0.375533 1150 -0.014897 0.137413 0.370991 1151 -0.011115 0.132731 0.364493 <p>1152 rows \u00d7 3 columns</p> In\u00a0[18]: Copied! <pre>import seaborn as sns\nfrom matplotlib import pyplot as plt\nsns.set()\n\nfig, axes = plt.subplots(4, 1, figsize = (16, 10))\n\nsns.lineplot(ax=axes[0], x=range(len(signals.iloc[0,:])), y = signals.iloc[0,:])\naxes[0].set_title(\"Original Time Signal\")\naxes[0].set_ylabel(\"Amplitude\")\naxes[0].set_xlabel(\"sample\")\naxes[0].set_xlim(0, 16000)\n\nsns.lineplot(ax=axes[1], x=range(len(signals_env.iloc[0,:])), y = signals_env.iloc[0, :])\naxes[1].set_title(\"Envelope\")\naxes[1].set_ylabel(\"Amplitude\")\naxes[1].set_xlabel(\"sample\")\naxes[1].set_xlim(0, 16000)\n\n\nsns.lineplot(ax=axes[2], x = fft_freq_axis(16000, 64000), y = signals_env_fft.iloc[0, :])\naxes[2].set_title(\"FFT\")\naxes[2].set_ylabel(\"Amplitude\")\naxes[2].set_xlabel(\"Frequency (Hz)\")\naxes[2].set_xlim(0, 32000)\n\n\nsns.lineplot(ax=axes[3], x = zoomed_fft_freq_axis(0, 1000, 2000), y = signals_env_ZoomedFFT.iloc[0, :])\naxes[3].set_title(\"Zoomed FFT\")\naxes[3].set_ylabel(\"Amplitude\")\naxes[3].set_xlabel(\"Frequency (Hz)\")\naxes[3].set_xlim(0, 1000)\n\n\nplt.subplots_adjust(hspace = 0.75)\nfig.show()\n</pre> import seaborn as sns from matplotlib import pyplot as plt sns.set()  fig, axes = plt.subplots(4, 1, figsize = (16, 10))  sns.lineplot(ax=axes[0], x=range(len(signals.iloc[0,:])), y = signals.iloc[0,:]) axes[0].set_title(\"Original Time Signal\") axes[0].set_ylabel(\"Amplitude\") axes[0].set_xlabel(\"sample\") axes[0].set_xlim(0, 16000)  sns.lineplot(ax=axes[1], x=range(len(signals_env.iloc[0,:])), y = signals_env.iloc[0, :]) axes[1].set_title(\"Envelope\") axes[1].set_ylabel(\"Amplitude\") axes[1].set_xlabel(\"sample\") axes[1].set_xlim(0, 16000)   sns.lineplot(ax=axes[2], x = fft_freq_axis(16000, 64000), y = signals_env_fft.iloc[0, :]) axes[2].set_title(\"FFT\") axes[2].set_ylabel(\"Amplitude\") axes[2].set_xlabel(\"Frequency (Hz)\") axes[2].set_xlim(0, 32000)   sns.lineplot(ax=axes[3], x = zoomed_fft_freq_axis(0, 1000, 2000), y = signals_env_ZoomedFFT.iloc[0, :]) axes[3].set_title(\"Zoomed FFT\") axes[3].set_ylabel(\"Amplitude\") axes[3].set_xlabel(\"Frequency (Hz)\") axes[3].set_xlim(0, 1000)   plt.subplots_adjust(hspace = 0.75) fig.show() In\u00a0[19]: Copied! <pre>t = np.linspace(0, 0.25, 70)\nf = fft_freq_axis(2000, 64000)\n\nfig, ax = plt.subplots(figsize = (16, 8))\n\nax = sns.heatmap(signals_env_STFT[0, :, :], xticklabels = np.round(f, decimals = 2), yticklabels = np.round(t, decimals = 2), annot = False, cbar = False)\nax.set(xlabel = 'Frequency (Hz)', ylabel = 'Time (sec)')\nax.set_title('STFT')\nax.set_xticks(ax.get_xticks()[::20])\nax.set_yticks(ax.get_yticks()[::5])\n\n\nfig.show()\n</pre> t = np.linspace(0, 0.25, 70) f = fft_freq_axis(2000, 64000)  fig, ax = plt.subplots(figsize = (16, 8))  ax = sns.heatmap(signals_env_STFT[0, :, :], xticklabels = np.round(f, decimals = 2), yticklabels = np.round(t, decimals = 2), annot = False, cbar = False) ax.set(xlabel = 'Frequency (Hz)', ylabel = 'Time (sec)') ax.set_title('STFT') ax.set_xticks(ax.get_xticks()[::20]) ax.set_yticks(ax.get_yticks()[::5])   fig.show() In\u00a0[19]: Copied! <pre>\n</pre>"},{"location":"notebooks/dataset_demos/PU_demo/#cloning-the-damavand-repository","title":"Cloning the damavand repository\u00b6","text":""},{"location":"notebooks/dataset_demos/PU_demo/#importings","title":"Importings\u00b6","text":""},{"location":"notebooks/dataset_demos/PU_demo/#instantiating-a-downloader-object","title":"Instantiating a downloader object\u00b6","text":""},{"location":"notebooks/dataset_demos/PU_demo/#instantiating-a-digestor-object","title":"Instantiating a digestor object\u00b6","text":""},{"location":"notebooks/dataset_demos/PU_demo/#aggregating-vibration-data","title":"Aggregating vibration data\u00b6","text":""},{"location":"notebooks/dataset_demos/PU_demo/#signals-metadata-declaration","title":"Signals-Metadata declaration\u00b6","text":""},{"location":"notebooks/dataset_demos/PU_demo/#signal-processing","title":"Signal Processing\u00b6","text":""},{"location":"notebooks/dataset_demos/PU_demo/#envelope-extraction","title":"Envelope Extraction\u00b6","text":""},{"location":"notebooks/dataset_demos/PU_demo/#fft","title":"FFT\u00b6","text":""},{"location":"notebooks/dataset_demos/PU_demo/#zoomed-fft","title":"Zoomed FFT\u00b6","text":""},{"location":"notebooks/dataset_demos/PU_demo/#stft","title":"STFT\u00b6","text":""},{"location":"notebooks/dataset_demos/PU_demo/#statistical-features","title":"Statistical Features\u00b6","text":""},{"location":"notebooks/dataset_demos/PU_demo/#visualization","title":"Visualization\u00b6","text":""},{"location":"notebooks/dataset_demos/SEU_demo/","title":"Cloning the damavand repository","text":"In\u00a0[1]: Copied! <pre>!git clone https://github.com/amirberenji1995/damavand\n</pre> !git clone https://github.com/amirberenji1995/damavand <pre>Cloning into 'damavand'...\nremote: Enumerating objects: 263, done.\nremote: Counting objects: 100% (263/263), done.\nremote: Compressing objects: 100% (197/197), done.\nremote: Total 263 (delta 132), reused 196 (delta 65), pack-reused 0 (from 0)\nReceiving objects: 100% (263/263), 6.62 MiB | 16.67 MiB/s, done.\nResolving deltas: 100% (132/132), done.\n</pre> In\u00a0[2]: Copied! <pre>!pip install -r damavand/requirements.txt\n</pre> !pip install -r damavand/requirements.txt <pre>Collecting certifi==2024.7.4 (from -r damavand/requirements.txt (line 1))\n  Downloading certifi-2024.7.4-py3-none-any.whl.metadata (2.2 kB)\nCollecting charset-normalizer==3.3.2 (from -r damavand/requirements.txt (line 2))\n  Downloading charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\nCollecting idna==3.7 (from -r damavand/requirements.txt (line 3))\n  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\nCollecting numpy==1.26.4 (from -r damavand/requirements.txt (line 4))\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 61.0/61.0 kB 3.1 MB/s eta 0:00:00\nCollecting pandas==2.1.4 (from -r damavand/requirements.txt (line 5))\n  Downloading pandas-2.1.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\nRequirement already satisfied: python-dateutil==2.9.0.post0 in /usr/local/lib/python3.11/dist-packages (from -r damavand/requirements.txt (line 6)) (2.9.0.post0)\nCollecting pytz==2024.1 (from -r damavand/requirements.txt (line 7))\n  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\nCollecting rarfile==4.2 (from -r damavand/requirements.txt (line 8))\n  Downloading rarfile-4.2-py3-none-any.whl.metadata (4.4 kB)\nRequirement already satisfied: requests==2.32.3 in /usr/local/lib/python3.11/dist-packages (from -r damavand/requirements.txt (line 9)) (2.32.3)\nCollecting scipy==1.13.1 (from -r damavand/requirements.txt (line 10))\n  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 60.6/60.6 kB 4.5 MB/s eta 0:00:00\nCollecting six==1.16.0 (from -r damavand/requirements.txt (line 11))\n  Downloading six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\nCollecting tzdata==2024.1 (from -r damavand/requirements.txt (line 12))\n  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\nCollecting urllib3==2.2.2 (from -r damavand/requirements.txt (line 13))\n  Downloading urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\nDownloading certifi-2024.7.4-py3-none-any.whl (162 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 163.0/163.0 kB 9.2 MB/s eta 0:00:00\nDownloading charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (140 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 140.3/140.3 kB 11.4 MB/s eta 0:00:00\nDownloading idna-3.7-py3-none-any.whl (66 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 66.8/66.8 kB 4.4 MB/s eta 0:00:00\nDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 83.8 MB/s eta 0:00:00\nDownloading pandas-2.1.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 12.2/12.2 MB 97.6 MB/s eta 0:00:00\nDownloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 505.5/505.5 kB 26.5 MB/s eta 0:00:00\nDownloading rarfile-4.2-py3-none-any.whl (29 kB)\nDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 38.6/38.6 MB 15.8 MB/s eta 0:00:00\nDownloading six-1.16.0-py2.py3-none-any.whl (11 kB)\nDownloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 345.4/345.4 kB 22.6 MB/s eta 0:00:00\nDownloading urllib3-2.2.2-py3-none-any.whl (121 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 121.4/121.4 kB 9.9 MB/s eta 0:00:00\nInstalling collected packages: pytz, urllib3, tzdata, six, rarfile, numpy, idna, charset-normalizer, certifi, scipy, pandas\n  Attempting uninstall: pytz\n    Found existing installation: pytz 2025.2\n    Uninstalling pytz-2025.2:\n      Successfully uninstalled pytz-2025.2\n  Attempting uninstall: urllib3\n    Found existing installation: urllib3 2.5.0\n    Uninstalling urllib3-2.5.0:\n      Successfully uninstalled urllib3-2.5.0\n  Attempting uninstall: tzdata\n    Found existing installation: tzdata 2025.2\n    Uninstalling tzdata-2025.2:\n      Successfully uninstalled tzdata-2025.2\n  Attempting uninstall: six\n    Found existing installation: six 1.17.0\n    Uninstalling six-1.17.0:\n      Successfully uninstalled six-1.17.0\n  Attempting uninstall: numpy\n    Found existing installation: numpy 2.0.2\n    Uninstalling numpy-2.0.2:\n      Successfully uninstalled numpy-2.0.2\n  Attempting uninstall: idna\n    Found existing installation: idna 3.10\n    Uninstalling idna-3.10:\n      Successfully uninstalled idna-3.10\n  Attempting uninstall: charset-normalizer\n    Found existing installation: charset-normalizer 3.4.2\n    Uninstalling charset-normalizer-3.4.2:\n      Successfully uninstalled charset-normalizer-3.4.2\n  Attempting uninstall: certifi\n    Found existing installation: certifi 2025.7.14\n    Uninstalling certifi-2025.7.14:\n      Successfully uninstalled certifi-2025.7.14\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.16.0\n    Uninstalling scipy-1.16.0:\n      Successfully uninstalled scipy-1.16.0\n  Attempting uninstall: pandas\n    Found existing installation: pandas 2.2.2\n    Uninstalling pandas-2.2.2:\n      Successfully uninstalled pandas-2.2.2\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.1.4 which is incompatible.\nopencv-python 4.12.0.88 requires numpy&lt;2.3.0,&gt;=2; python_version &gt;= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy&lt;2.3.0,&gt;=2; python_version &gt;= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy&lt;2.3.0,&gt;=2; python_version &gt;= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nplotnine 0.14.5 requires pandas&gt;=2.2.0, but you have pandas 2.1.4 which is incompatible.\ntsfresh 0.21.0 requires scipy&gt;=1.14.0; python_version &gt;= \"3.10\", but you have scipy 1.13.1 which is incompatible.\nthinc 8.3.6 requires numpy&lt;3.0.0,&gt;=2.0.0, but you have numpy 1.26.4 which is incompatible.\nxarray 2025.7.1 requires pandas&gt;=2.2, but you have pandas 2.1.4 which is incompatible.\nmizani 0.13.5 requires pandas&gt;=2.2.0, but you have pandas 2.1.4 which is incompatible.\nSuccessfully installed certifi-2024.7.4 charset-normalizer-3.3.2 idna-3.7 numpy-1.26.4 pandas-2.1.4 pytz-2024.1 rarfile-4.2 scipy-1.13.1 six-1.16.0 tzdata-2024.1 urllib3-2.2.2\n</pre> In\u00a0[1]: Copied! <pre>from damavand.damavand.datasets.downloaders import read_addresses, ZipDatasetDownloader\nfrom damavand.damavand.datasets.digestors import SEU\nfrom damavand.damavand.signal_processing.transformations import *\nfrom damavand.damavand.signal_processing.feature_extraction import *\nfrom damavand.damavand.utils import *\nimport os\nimport pandas as pd\nimport numpy as np\nimport scipy\n</pre> from damavand.damavand.datasets.downloaders import read_addresses, ZipDatasetDownloader from damavand.damavand.datasets.digestors import SEU from damavand.damavand.signal_processing.transformations import * from damavand.damavand.signal_processing.feature_extraction import * from damavand.damavand.utils import * import os import pandas as pd import numpy as np import scipy In\u00a0[2]: Copied! <pre>addresses = read_addresses()\ndownloader = ZipDatasetDownloader(addresses['SEU'])\ndownloader.download_extract('SEU.zip', 'SEU/')\n</pre> addresses = read_addresses() downloader = ZipDatasetDownloader(addresses['SEU']) downloader.download_extract('SEU.zip', 'SEU/') In\u00a0[3]: Copied! <pre>seu = SEU('SEU/')\nmining_params = {\n    'win_len': 10000,\n    'hop_len': 10000\n}\nseu.mine(mining_params)\n</pre> seu = SEU('SEU/') mining_params = {     'win_len': 10000,     'hop_len': 10000 } seu.mine(mining_params) <pre>Mining:  Health_20_0.csv\nMining:  Root_30_2.csv\nMining:  Miss_30_2.csv\nMining:  Root_20_0.csv\nMining:  Miss_20_0.csv\nMining:  Surface_20_0.csv\nMining:  Surface_30_2.csv\nMining:  Health_30_2.csv\nMining:  Chipped_30_2.csv\nMining:  Chipped_20_0.csv\nMining:  outer_20_0.csv\nMining:  comb_20_0.csv\nMining:  health_20_0.csv\nMining:  ball_30_2.csv\nMining:  comb_30_2.csv\nMining:  health_30_2.csv\nMining:  outer_30_2.csv\nMining:  inner_20_0.csv\nMining:  inner_30_2.csv\nMining:  ball_20_0.csv\n</pre> In\u00a0[4]: Copied! <pre>df = pd.concat(seu.data[1]).reset_index(drop = True)\ndf\n</pre> df = pd.concat(seu.data[1]).reset_index(drop = True) df Out[4]: 0 1 2 3 4 5 6 7 8 9 ... 9993 9994 9995 9996 9997 9998 9999 test_bed state rot_speed 0 0.008813 -0.006192 0.005939 0.016813 0.002066 0.016346 0.013306 -0.001801 0.005945 -0.002171 ... -0.005212 -0.000072 -0.005836 0.006762 0.010804 0.014676 0.010185 Gear_set Health 20 1 0.005423 0.004086 0.002605 -0.002048 -0.000333 0.004418 0.007993 0.006566 0.002242 0.007952 ... 0.002311 -0.002023 0.002367 0.002530 -0.002308 -0.001899 -0.000598 Gear_set Health 20 2 0.003530 -0.002198 -0.001231 -0.001452 -0.000398 0.008441 0.003691 -0.003535 0.004591 0.009463 ... 0.002589 0.002497 0.003873 0.015830 0.010234 -0.000416 0.005373 Gear_set Health 20 3 0.008167 0.004586 0.004731 -0.004278 -0.011252 -0.003437 -0.012201 0.000237 0.004898 0.003796 ... 0.002348 -0.005813 -0.011439 -0.001320 -0.008225 -0.002196 0.007448 Gear_set Health 20 4 0.001681 0.006210 0.009668 0.001734 0.009592 0.005008 -0.002534 0.003953 0.006863 -0.000491 ... -0.011886 0.002227 0.001158 -0.007437 -0.007825 0.007242 -0.004482 Gear_set Health 20 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 2075 0.004690 0.003175 0.000706 -0.002266 -0.000335 0.000126 0.003114 0.003561 0.003369 0.004345 ... 0.002997 0.000696 0.004355 0.000876 0.004166 0.002538 0.002812 Bearing_set ball 20 2076 -0.000436 0.000293 0.004985 0.001394 0.003667 0.003562 0.004380 0.003843 0.002679 0.001819 ... 0.005540 0.001757 0.002246 -0.001831 0.000353 0.003362 0.000786 Bearing_set ball 20 2077 0.002798 0.005493 0.002176 -0.001878 0.003673 0.001307 -0.001187 0.002478 0.005165 0.004226 ... 0.002526 0.002630 0.001738 0.001863 0.001026 0.000129 0.000445 Bearing_set ball 20 2078 0.002276 0.000391 0.003090 0.003029 0.002033 0.001818 0.000091 0.000380 -0.002043 -0.003357 ... 0.003529 0.004483 0.001863 0.002882 0.003272 0.000004 -0.000987 Bearing_set ball 20 2079 0.000753 -0.000579 0.003065 0.004425 0.003166 0.003641 0.004364 0.000494 0.000817 0.003343 ... 0.002980 0.001419 0.003171 0.001575 0.003867 0.002372 -0.000206 Bearing_set ball 20 <p>2080 rows \u00d7 10003 columns</p> In\u00a0[5]: Copied! <pre>signals, metadata = df.iloc[:, : - 3], df.iloc[:, - 3 :]\nsignals\n</pre> signals, metadata = df.iloc[:, : - 3], df.iloc[:, - 3 :] signals Out[5]: 0 1 2 3 4 5 6 7 8 9 ... 9990 9991 9992 9993 9994 9995 9996 9997 9998 9999 0 0.008813 -0.006192 0.005939 0.016813 0.002066 0.016346 0.013306 -0.001801 0.005945 -0.002171 ... 0.009387 0.001698 -0.014420 -0.005212 -0.000072 -0.005836 0.006762 0.010804 0.014676 0.010185 1 0.005423 0.004086 0.002605 -0.002048 -0.000333 0.004418 0.007993 0.006566 0.002242 0.007952 ... 0.007765 0.004016 -0.002347 0.002311 -0.002023 0.002367 0.002530 -0.002308 -0.001899 -0.000598 2 0.003530 -0.002198 -0.001231 -0.001452 -0.000398 0.008441 0.003691 -0.003535 0.004591 0.009463 ... -0.012412 -0.010130 -0.004557 0.002589 0.002497 0.003873 0.015830 0.010234 -0.000416 0.005373 3 0.008167 0.004586 0.004731 -0.004278 -0.011252 -0.003437 -0.012201 0.000237 0.004898 0.003796 ... 0.004174 0.000271 -0.005994 0.002348 -0.005813 -0.011439 -0.001320 -0.008225 -0.002196 0.007448 4 0.001681 0.006210 0.009668 0.001734 0.009592 0.005008 -0.002534 0.003953 0.006863 -0.000491 ... -0.011536 -0.007182 0.003451 -0.011886 0.002227 0.001158 -0.007437 -0.007825 0.007242 -0.004482 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 2075 0.004690 0.003175 0.000706 -0.002266 -0.000335 0.000126 0.003114 0.003561 0.003369 0.004345 ... 0.002524 -0.000216 0.001526 0.002997 0.000696 0.004355 0.000876 0.004166 0.002538 0.002812 2076 -0.000436 0.000293 0.004985 0.001394 0.003667 0.003562 0.004380 0.003843 0.002679 0.001819 ... 0.002059 0.001695 0.003431 0.005540 0.001757 0.002246 -0.001831 0.000353 0.003362 0.000786 2077 0.002798 0.005493 0.002176 -0.001878 0.003673 0.001307 -0.001187 0.002478 0.005165 0.004226 ... 0.000304 0.000989 0.002574 0.002526 0.002630 0.001738 0.001863 0.001026 0.000129 0.000445 2078 0.002276 0.000391 0.003090 0.003029 0.002033 0.001818 0.000091 0.000380 -0.002043 -0.003357 ... 0.002007 0.000339 -0.000720 0.003529 0.004483 0.001863 0.002882 0.003272 0.000004 -0.000987 2079 0.000753 -0.000579 0.003065 0.004425 0.003166 0.003641 0.004364 0.000494 0.000817 0.003343 ... -0.000396 0.003937 0.003322 0.002980 0.001419 0.003171 0.001575 0.003867 0.002372 -0.000206 <p>2080 rows \u00d7 10000 columns</p> In\u00a0[6]: Copied! <pre>metadata\n</pre> metadata Out[6]: test_bed state rot_speed 0 Gear_set Health 20 1 Gear_set Health 20 2 Gear_set Health 20 3 Gear_set Health 20 4 Gear_set Health 20 ... ... ... ... 2075 Bearing_set ball 20 2076 Bearing_set ball 20 2077 Bearing_set ball 20 2078 Bearing_set ball 20 2079 Bearing_set ball 20 <p>2080 rows \u00d7 3 columns</p> In\u00a0[7]: Copied! <pre>signals_env = env(signals)\nsignals_env.shape\n</pre> signals_env = env(signals) signals_env.shape Out[7]: <pre>(2080, 10000)</pre> In\u00a0[8]: Copied! <pre>window = scipy.signal.windows.hann(signals_env.shape[1])\nfreq_filter = scipy.signal.butter(25, [15, 950], 'bandpass', fs = 2000, output='sos')\n</pre> window = scipy.signal.windows.hann(signals_env.shape[1]) freq_filter = scipy.signal.butter(25, [15, 950], 'bandpass', fs = 2000, output='sos') In\u00a0[9]: Copied! <pre>signals_env_fft = fft(signals_env, freq_filter = freq_filter, window = window)\nsignals_env_fft.shape\n</pre> signals_env_fft = fft(signals_env, freq_filter = freq_filter, window = window) signals_env_fft.shape Out[9]: <pre>(2080, 5000)</pre> In\u00a0[10]: Copied! <pre>signals_env_ZoomedFFT = zoomed_fft(signals_env, 0, 750, 1500, 2000, freq_filter = freq_filter, window = window)\nsignals_env_ZoomedFFT.shape\n</pre> signals_env_ZoomedFFT = zoomed_fft(signals_env, 0, 750, 1500, 2000, freq_filter = freq_filter, window = window) signals_env_ZoomedFFT.shape Out[10]: <pre>(2080, 1500)</pre> In\u00a0[11]: Copied! <pre>STFT_window = scipy.signal.windows.hann(1000)\nSTFT_freq_filter = scipy.signal.butter(25, [15, 950], 'bandpass', fs = 2000, output='sos')\nsignals_env_STFT = stft(signals_env, 1000, 250, STFT_freq_filter, STFT_window)\nsignals_env_STFT.shape\n</pre> STFT_window = scipy.signal.windows.hann(1000) STFT_freq_filter = scipy.signal.butter(25, [15, 950], 'bandpass', fs = 2000, output='sos') signals_env_STFT = stft(signals_env, 1000, 250, STFT_freq_filter, STFT_window) signals_env_STFT.shape Out[11]: <pre>(2080, 37, 500)</pre> In\u00a0[12]: Copied! <pre># Defining the feature-set to be extracted\nfeatures = {'mean': (np.mean, (), {}), 'var': (np.var, (), {}), 'rms': (rms, (), {})}\n</pre> # Defining the feature-set to be extracted features = {'mean': (np.mean, (), {}), 'var': (np.var, (), {}), 'rms': (rms, (), {})} In\u00a0[13]: Copied! <pre># Extracting the desired feature-set from time-domain signals\nfeatures_df = feature_extractor(signals, features)\nfeatures_df\n</pre> # Extracting the desired feature-set from time-domain signals features_df = feature_extractor(signals, features) features_df Out[13]: mean var rms 0 0.001029 0.000050 0.007164 1 0.001021 0.000055 0.007467 2 0.000977 0.000052 0.007311 3 0.000963 0.000051 0.007173 4 0.000977 0.000048 0.007005 ... ... ... ... 2075 0.000928 0.000005 0.002377 2076 0.000898 0.000005 0.002454 2077 0.000851 0.000006 0.002540 2078 0.000914 0.000005 0.002428 2079 0.000898 0.000005 0.002496 <p>2080 rows \u00d7 3 columns</p> In\u00a0[14]: Copied! <pre>import seaborn as sns\nfrom matplotlib import pyplot as plt\nsns.set()\n\nfig, axes = plt.subplots(4, 1, figsize = (16, 10))\n\nsns.lineplot(ax=axes[0], x=range(len(signals.iloc[0,:])), y = signals.iloc[0,:])\naxes[0].set_title(\"Original Time Signal\")\naxes[0].set_ylabel(\"Amplitude\")\naxes[0].set_xlabel(\"sample\")\naxes[0].set_xlim(0, 10000)\n\nsns.lineplot(ax=axes[1], x=range(len(signals_env.iloc[0,:])), y = signals_env.iloc[0, :])\naxes[1].set_title(\"Envelope\")\naxes[1].set_ylabel(\"Amplitude\")\naxes[1].set_xlabel(\"sample\")\naxes[1].set_xlim(0, 10000)\n\n\nsns.lineplot(ax=axes[2], x = fft_freq_axis(10000, 2000), y = signals_env_fft.iloc[0, :])\naxes[2].set_title(\"FFT\")\naxes[2].set_ylabel(\"Amplitude\")\naxes[2].set_xlabel(\"Frequency (Hz)\")\naxes[2].set_xlim(0, 1000)\n\n\nsns.lineplot(ax=axes[3], x = zoomed_fft_freq_axis(0, 750, 1500), y = signals_env_ZoomedFFT.iloc[0, :])\naxes[3].set_title(\"Zoomed FFT\")\naxes[3].set_ylabel(\"Amplitude\")\naxes[3].set_xlabel(\"Frequency (Hz)\")\naxes[3].set_xlim(0, 750)\n\n\nplt.subplots_adjust(hspace = 0.75)\nfig.show()\n</pre> import seaborn as sns from matplotlib import pyplot as plt sns.set()  fig, axes = plt.subplots(4, 1, figsize = (16, 10))  sns.lineplot(ax=axes[0], x=range(len(signals.iloc[0,:])), y = signals.iloc[0,:]) axes[0].set_title(\"Original Time Signal\") axes[0].set_ylabel(\"Amplitude\") axes[0].set_xlabel(\"sample\") axes[0].set_xlim(0, 10000)  sns.lineplot(ax=axes[1], x=range(len(signals_env.iloc[0,:])), y = signals_env.iloc[0, :]) axes[1].set_title(\"Envelope\") axes[1].set_ylabel(\"Amplitude\") axes[1].set_xlabel(\"sample\") axes[1].set_xlim(0, 10000)   sns.lineplot(ax=axes[2], x = fft_freq_axis(10000, 2000), y = signals_env_fft.iloc[0, :]) axes[2].set_title(\"FFT\") axes[2].set_ylabel(\"Amplitude\") axes[2].set_xlabel(\"Frequency (Hz)\") axes[2].set_xlim(0, 1000)   sns.lineplot(ax=axes[3], x = zoomed_fft_freq_axis(0, 750, 1500), y = signals_env_ZoomedFFT.iloc[0, :]) axes[3].set_title(\"Zoomed FFT\") axes[3].set_ylabel(\"Amplitude\") axes[3].set_xlabel(\"Frequency (Hz)\") axes[3].set_xlim(0, 750)   plt.subplots_adjust(hspace = 0.75) fig.show() In\u00a0[15]: Copied! <pre>t = np.linspace(0, 5, 36)\nf = fft_freq_axis(1000, 2000)\n\nfig, ax = plt.subplots(figsize = (16, 8))\n\nax = sns.heatmap(signals_env_STFT[0, :, :], xticklabels = np.round(f, decimals = 2), yticklabels = np.round(t, decimals = 2), annot = False, cbar = False)\nax.set(xlabel = 'Frequency (Hz)', ylabel = 'Time (sec)')\nax.set_title('STFT')\nax.set_xticks(ax.get_xticks()[::20])\nax.set_yticks(ax.get_yticks()[::5])\n\n\nfig.show()\n</pre> t = np.linspace(0, 5, 36) f = fft_freq_axis(1000, 2000)  fig, ax = plt.subplots(figsize = (16, 8))  ax = sns.heatmap(signals_env_STFT[0, :, :], xticklabels = np.round(f, decimals = 2), yticklabels = np.round(t, decimals = 2), annot = False, cbar = False) ax.set(xlabel = 'Frequency (Hz)', ylabel = 'Time (sec)') ax.set_title('STFT') ax.set_xticks(ax.get_xticks()[::20]) ax.set_yticks(ax.get_yticks()[::5])   fig.show()"},{"location":"notebooks/dataset_demos/SEU_demo/#cloning-the-damavand-repository","title":"Cloning the damavand repository\u00b6","text":""},{"location":"notebooks/dataset_demos/SEU_demo/#importings","title":"Importings\u00b6","text":""},{"location":"notebooks/dataset_demos/SEU_demo/#instantiating-a-downloader-object","title":"Instantiating a downloader object\u00b6","text":""},{"location":"notebooks/dataset_demos/SEU_demo/#instantiating-a-digestor-object","title":"Instantiating a digestor object\u00b6","text":""},{"location":"notebooks/dataset_demos/SEU_demo/#aggregating-data-over-the-second-channel","title":"Aggregating data over the second channel\u00b6","text":""},{"location":"notebooks/dataset_demos/SEU_demo/#signals-metadata-declaration","title":"Signals-Metadata declaration\u00b6","text":""},{"location":"notebooks/dataset_demos/SEU_demo/#signal-processing","title":"Signal Processing\u00b6","text":""},{"location":"notebooks/dataset_demos/SEU_demo/#envelope-extraction","title":"Envelope Extraction\u00b6","text":""},{"location":"notebooks/dataset_demos/SEU_demo/#fft","title":"FFT\u00b6","text":""},{"location":"notebooks/dataset_demos/SEU_demo/#zoomedfft","title":"ZoomedFFT\u00b6","text":""},{"location":"notebooks/dataset_demos/SEU_demo/#stft","title":"STFT\u00b6","text":""},{"location":"notebooks/dataset_demos/SEU_demo/#statistical-features","title":"Statistical Features\u00b6","text":""},{"location":"notebooks/dataset_demos/SEU_demo/#visualization","title":"Visualization\u00b6","text":""},{"location":"notebooks/dataset_demos/UoO_demo/","title":"Cloning the damavand repository","text":"In\u00a0[1]: Copied! <pre>!git clone https://github.com/amirberenji1995/damavand\n</pre> !git clone https://github.com/amirberenji1995/damavand <pre>Cloning into 'damavand'...\nremote: Enumerating objects: 263, done.\nremote: Counting objects: 100% (263/263), done.\nremote: Compressing objects: 100% (197/197), done.\nremote: Total 263 (delta 132), reused 196 (delta 65), pack-reused 0 (from 0)\nReceiving objects: 100% (263/263), 6.62 MiB | 18.69 MiB/s, done.\nResolving deltas: 100% (132/132), done.\n</pre> In\u00a0[2]: Copied! <pre>!pip install -r damavand/requirements.txt\n</pre> !pip install -r damavand/requirements.txt <pre>Collecting certifi==2024.7.4 (from -r damavand/requirements.txt (line 1))\n  Downloading certifi-2024.7.4-py3-none-any.whl.metadata (2.2 kB)\nCollecting charset-normalizer==3.3.2 (from -r damavand/requirements.txt (line 2))\n  Downloading charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\nCollecting idna==3.7 (from -r damavand/requirements.txt (line 3))\n  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\nCollecting numpy==1.26.4 (from -r damavand/requirements.txt (line 4))\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 61.0/61.0 kB 4.4 MB/s eta 0:00:00\nCollecting pandas==2.1.4 (from -r damavand/requirements.txt (line 5))\n  Downloading pandas-2.1.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\nRequirement already satisfied: python-dateutil==2.9.0.post0 in /usr/local/lib/python3.11/dist-packages (from -r damavand/requirements.txt (line 6)) (2.9.0.post0)\nCollecting pytz==2024.1 (from -r damavand/requirements.txt (line 7))\n  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\nCollecting rarfile==4.2 (from -r damavand/requirements.txt (line 8))\n  Downloading rarfile-4.2-py3-none-any.whl.metadata (4.4 kB)\nRequirement already satisfied: requests==2.32.3 in /usr/local/lib/python3.11/dist-packages (from -r damavand/requirements.txt (line 9)) (2.32.3)\nCollecting scipy==1.13.1 (from -r damavand/requirements.txt (line 10))\n  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 60.6/60.6 kB 4.0 MB/s eta 0:00:00\nCollecting six==1.16.0 (from -r damavand/requirements.txt (line 11))\n  Downloading six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\nCollecting tzdata==2024.1 (from -r damavand/requirements.txt (line 12))\n  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\nCollecting urllib3==2.2.2 (from -r damavand/requirements.txt (line 13))\n  Downloading urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\nDownloading certifi-2024.7.4-py3-none-any.whl (162 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 163.0/163.0 kB 6.7 MB/s eta 0:00:00\nDownloading charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (140 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 140.3/140.3 kB 10.6 MB/s eta 0:00:00\nDownloading idna-3.7-py3-none-any.whl (66 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 66.8/66.8 kB 4.8 MB/s eta 0:00:00\nDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.3/18.3 MB 34.6 MB/s eta 0:00:00\nDownloading pandas-2.1.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 12.2/12.2 MB 33.5 MB/s eta 0:00:00\nDownloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 505.5/505.5 kB 21.1 MB/s eta 0:00:00\nDownloading rarfile-4.2-py3-none-any.whl (29 kB)\nDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 38.6/38.6 MB 14.4 MB/s eta 0:00:00\nDownloading six-1.16.0-py2.py3-none-any.whl (11 kB)\nDownloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 345.4/345.4 kB 14.1 MB/s eta 0:00:00\nDownloading urllib3-2.2.2-py3-none-any.whl (121 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 121.4/121.4 kB 7.9 MB/s eta 0:00:00\nInstalling collected packages: pytz, urllib3, tzdata, six, rarfile, numpy, idna, charset-normalizer, certifi, scipy, pandas\n  Attempting uninstall: pytz\n    Found existing installation: pytz 2025.2\n    Uninstalling pytz-2025.2:\n      Successfully uninstalled pytz-2025.2\n  Attempting uninstall: urllib3\n    Found existing installation: urllib3 2.5.0\n    Uninstalling urllib3-2.5.0:\n      Successfully uninstalled urllib3-2.5.0\n  Attempting uninstall: tzdata\n    Found existing installation: tzdata 2025.2\n    Uninstalling tzdata-2025.2:\n      Successfully uninstalled tzdata-2025.2\n  Attempting uninstall: six\n    Found existing installation: six 1.17.0\n    Uninstalling six-1.17.0:\n      Successfully uninstalled six-1.17.0\n  Attempting uninstall: numpy\n    Found existing installation: numpy 2.0.2\n    Uninstalling numpy-2.0.2:\n      Successfully uninstalled numpy-2.0.2\n  Attempting uninstall: idna\n    Found existing installation: idna 3.10\n    Uninstalling idna-3.10:\n      Successfully uninstalled idna-3.10\n  Attempting uninstall: charset-normalizer\n    Found existing installation: charset-normalizer 3.4.2\n    Uninstalling charset-normalizer-3.4.2:\n      Successfully uninstalled charset-normalizer-3.4.2\n  Attempting uninstall: certifi\n    Found existing installation: certifi 2025.7.14\n    Uninstalling certifi-2025.7.14:\n      Successfully uninstalled certifi-2025.7.14\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.16.0\n    Uninstalling scipy-1.16.0:\n      Successfully uninstalled scipy-1.16.0\n  Attempting uninstall: pandas\n    Found existing installation: pandas 2.2.2\n    Uninstalling pandas-2.2.2:\n      Successfully uninstalled pandas-2.2.2\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.1.4 which is incompatible.\nopencv-python 4.12.0.88 requires numpy&lt;2.3.0,&gt;=2; python_version &gt;= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy&lt;2.3.0,&gt;=2; python_version &gt;= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy&lt;2.3.0,&gt;=2; python_version &gt;= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nplotnine 0.14.5 requires pandas&gt;=2.2.0, but you have pandas 2.1.4 which is incompatible.\ntsfresh 0.21.0 requires scipy&gt;=1.14.0; python_version &gt;= \"3.10\", but you have scipy 1.13.1 which is incompatible.\nthinc 8.3.6 requires numpy&lt;3.0.0,&gt;=2.0.0, but you have numpy 1.26.4 which is incompatible.\nxarray 2025.7.1 requires pandas&gt;=2.2, but you have pandas 2.1.4 which is incompatible.\nmizani 0.13.5 requires pandas&gt;=2.2.0, but you have pandas 2.1.4 which is incompatible.\nSuccessfully installed certifi-2024.7.4 charset-normalizer-3.3.2 idna-3.7 numpy-1.26.4 pandas-2.1.4 pytz-2024.1 rarfile-4.2 scipy-1.13.1 six-1.16.0 tzdata-2024.1 urllib3-2.2.2\n</pre> In\u00a0[1]: Copied! <pre>from damavand.damavand.datasets.downloaders import read_addresses, ZipDatasetDownloader\nfrom damavand.damavand.datasets.digestors import UoO\nfrom damavand.damavand.signal_processing.transformations import *\nfrom damavand.damavand.signal_processing.feature_extraction import *\nfrom damavand.damavand.utils import *\nimport os\nimport pandas as pd\nimport numpy as np\nimport scipy\n</pre> from damavand.damavand.datasets.downloaders import read_addresses, ZipDatasetDownloader from damavand.damavand.datasets.digestors import UoO from damavand.damavand.signal_processing.transformations import * from damavand.damavand.signal_processing.feature_extraction import * from damavand.damavand.utils import * import os import pandas as pd import numpy as np import scipy In\u00a0[2]: Copied! <pre>addresses = read_addresses()\ndownloader = ZipDatasetDownloader(addresses['UoO'])\ndownloader.download_extract('UoO.zip', 'UoO/')\n</pre> addresses = read_addresses() downloader = ZipDatasetDownloader(addresses['UoO']) downloader.download_extract('UoO.zip', 'UoO/') In\u00a0[3]: Copied! <pre>dataset = UoO('UoO/', ['Channel_1', 'Channel_2'], [1])\nmining_params = {'win_len': 10000, 'hop_len': 10000}\ndataset.mine(mining_params)\n</pre> dataset = UoO('UoO/', ['Channel_1', 'Channel_2'], [1]) mining_params = {'win_len': 10000, 'hop_len': 10000} dataset.mine(mining_params) In\u00a0[4]: Copied! <pre>df = pd.concat(dataset.data['Channel_1']).reset_index(drop = True)\ndf\n</pre> df = pd.concat(dataset.data['Channel_1']).reset_index(drop = True) df Out[4]: 0 1 2 3 4 5 6 7 8 9 ... 9993 9994 9995 9996 9997 9998 9999 state loading rep 0 -0.002528 -0.001213 -0.001871 -0.002199 -0.002199 -0.000555 0.001747 0.001747 0.000102 -0.000226 ... -0.007461 -0.008448 -0.010092 -0.011078 -0.011407 -0.010092 -0.008448 O C 1 1 -0.009105 -0.008448 -0.008776 -0.008448 -0.007461 -0.006474 -0.007132 -0.008119 -0.008448 -0.007132 ... -0.000226 0.002733 0.003720 0.005693 0.007008 0.007995 0.007008 O C 1 2 0.006022 0.005364 0.004706 0.004706 0.003062 0.002076 0.001747 0.000431 -0.000226 -0.000555 ... -0.010749 -0.011736 -0.011407 -0.011407 -0.010092 -0.009763 -0.008119 O C 1 3 -0.007461 -0.007461 -0.006146 -0.005488 -0.003515 -0.002857 -0.003515 -0.002857 -0.005488 -0.005159 ... 0.007666 0.007666 0.007337 0.007008 -0.001213 0.002076 0.002404 O C 1 4 0.003062 0.003391 0.003062 0.002076 0.000760 0.000431 0.001747 0.002733 -0.000226 0.001747 ... 0.003720 0.003391 0.003391 0.002733 0.002404 0.000431 0.000431 O C 1 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 2395 0.005693 0.003062 0.005693 0.006679 0.008324 0.011941 0.016545 0.018189 0.017860 0.015229 ... -0.016998 -0.012394 -0.008119 -0.008119 -0.007461 -0.011078 -0.005488 O D 1 2396 0.000102 0.003720 0.001747 -0.001871 -0.005159 -0.005488 -0.005159 -0.004501 -0.006146 -0.009763 ... -0.007132 -0.007790 -0.009434 -0.009434 -0.007790 -0.004830 -0.002199 O D 1 2397 -0.000555 0.001418 0.001747 0.007008 0.011612 0.014572 0.014243 0.014572 0.014572 0.015558 ... 0.000431 0.001747 0.000431 -0.000884 -0.000884 -0.003844 0.001418 O D 1 2398 0.002733 0.003720 0.002076 0.001089 0.001418 0.005035 0.006679 0.007995 0.006351 0.003720 ... -0.025219 -0.054486 -0.028178 -0.027192 -0.025876 -0.023903 -0.025876 O D 1 2399 -0.024561 -0.024561 -0.023575 -0.022259 -0.020944 -0.021273 -0.022259 -0.021930 -0.022259 -0.021930 ... 0.017203 0.014901 0.011612 0.009639 0.009310 0.007995 0.001747 O D 1 <p>2400 rows \u00d7 10003 columns</p> In\u00a0[5]: Copied! <pre>signals, metadata = df.iloc[:, : -3], df.iloc[:, -3 :]\nsignals\n</pre> signals, metadata = df.iloc[:, : -3], df.iloc[:, -3 :] signals Out[5]: 0 1 2 3 4 5 6 7 8 9 ... 9990 9991 9992 9993 9994 9995 9996 9997 9998 9999 0 -0.002528 -0.001213 -0.001871 -0.002199 -0.002199 -0.000555 0.001747 0.001747 0.000102 -0.000226 ... -0.006474 -0.005817 -0.007461 -0.007461 -0.008448 -0.010092 -0.011078 -0.011407 -0.010092 -0.008448 1 -0.009105 -0.008448 -0.008776 -0.008448 -0.007461 -0.006474 -0.007132 -0.008119 -0.008448 -0.007132 ... -0.004501 -0.005488 -0.001871 -0.000226 0.002733 0.003720 0.005693 0.007008 0.007995 0.007008 2 0.006022 0.005364 0.004706 0.004706 0.003062 0.002076 0.001747 0.000431 -0.000226 -0.000555 ... -0.008448 -0.009434 -0.010749 -0.010749 -0.011736 -0.011407 -0.011407 -0.010092 -0.009763 -0.008119 3 -0.007461 -0.007461 -0.006146 -0.005488 -0.003515 -0.002857 -0.003515 -0.002857 -0.005488 -0.005159 ... 0.011612 0.010954 0.009310 0.007666 0.007666 0.007337 0.007008 -0.001213 0.002076 0.002404 4 0.003062 0.003391 0.003062 0.002076 0.000760 0.000431 0.001747 0.002733 -0.000226 0.001747 ... 0.004706 0.004706 0.003720 0.003720 0.003391 0.003391 0.002733 0.002404 0.000431 0.000431 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 2395 0.005693 0.003062 0.005693 0.006679 0.008324 0.011941 0.016545 0.018189 0.017860 0.015229 ... -0.016011 -0.018971 -0.019957 -0.016998 -0.012394 -0.008119 -0.008119 -0.007461 -0.011078 -0.005488 2396 0.000102 0.003720 0.001747 -0.001871 -0.005159 -0.005488 -0.005159 -0.004501 -0.006146 -0.009763 ... -0.006474 -0.007132 -0.006474 -0.007132 -0.007790 -0.009434 -0.009434 -0.007790 -0.004830 -0.002199 2397 -0.000555 0.001418 0.001747 0.007008 0.011612 0.014572 0.014243 0.014572 0.014572 0.015558 ... 0.000431 0.000102 -0.000226 0.000431 0.001747 0.000431 -0.000884 -0.000884 -0.003844 0.001418 2398 0.002733 0.003720 0.002076 0.001089 0.001418 0.005035 0.006679 0.007995 0.006351 0.003720 ... -0.021930 -0.023575 -0.023903 -0.025219 -0.054486 -0.028178 -0.027192 -0.025876 -0.023903 -0.025876 2399 -0.024561 -0.024561 -0.023575 -0.022259 -0.020944 -0.021273 -0.022259 -0.021930 -0.022259 -0.021930 ... 0.007995 0.011941 0.015887 0.017203 0.014901 0.011612 0.009639 0.009310 0.007995 0.001747 <p>2400 rows \u00d7 10000 columns</p> In\u00a0[6]: Copied! <pre>metadata\n</pre> metadata Out[6]: state loading rep 0 O C 1 1 O C 1 2 O C 1 3 O C 1 4 O C 1 ... ... ... ... 2395 O D 1 2396 O D 1 2397 O D 1 2398 O D 1 2399 O D 1 <p>2400 rows \u00d7 3 columns</p> In\u00a0[7]: Copied! <pre>metadata['comb'] = metadata['state'] + '_' + metadata['loading']\nmetadata['comb'].value_counts()\n</pre> metadata['comb'] = metadata['state'] + '_' + metadata['loading'] metadata['comb'].value_counts() Out[7]: count comb O_C 200 I_B 200 I_D 200 H_C 200 O_A 200 H_B 200 O_B 200 H_D 200 I_A 200 I_C 200 H_A 200 O_D 200 dtype: int64 In\u00a0[8]: Copied! <pre>signals_env = env(signals)\nsignals_env.shape\n</pre> signals_env = env(signals) signals_env.shape Out[8]: <pre>(2400, 10000)</pre> In\u00a0[9]: Copied! <pre>window = scipy.signal.windows.hann(signals_env.shape[1])\nfreq_filter = scipy.signal.butter(25, [5, 95000], 'bandpass', fs = 200000, output='sos')\n</pre> window = scipy.signal.windows.hann(signals_env.shape[1]) freq_filter = scipy.signal.butter(25, [5, 95000], 'bandpass', fs = 200000, output='sos') In\u00a0[10]: Copied! <pre>signals_env_fft = fft(signals_env, freq_filter = freq_filter, window = window)\nsignals_env_fft.shape\n</pre> signals_env_fft = fft(signals_env, freq_filter = freq_filter, window = window) signals_env_fft.shape Out[10]: <pre>(2400, 5000)</pre> In\u00a0[11]: Copied! <pre>signals_env_ZoomedFFT = zoomed_fft(signals_env, 0, 1000, 2000, 200000, freq_filter = freq_filter, window = window)\nsignals_env_ZoomedFFT.shape\n</pre> signals_env_ZoomedFFT = zoomed_fft(signals_env, 0, 1000, 2000, 200000, freq_filter = freq_filter, window = window) signals_env_ZoomedFFT.shape Out[11]: <pre>(2400, 2000)</pre> In\u00a0[12]: Copied! <pre>STFT_window = scipy.signal.windows.hann(1024)\nSTFT_freq_filter = scipy.signal.butter(25, [5, 95000], 'bandpass', fs = 200000, output='sos')\nsignals_env_STFT = stft(signals_env, 1024, 200, STFT_freq_filter, STFT_window)\nsignals_env_STFT.shape\n</pre> STFT_window = scipy.signal.windows.hann(1024) STFT_freq_filter = scipy.signal.butter(25, [5, 95000], 'bandpass', fs = 200000, output='sos') signals_env_STFT = stft(signals_env, 1024, 200, STFT_freq_filter, STFT_window) signals_env_STFT.shape Out[12]: <pre>(2400, 45, 512)</pre> In\u00a0[13]: Copied! <pre># Defining the feature-set to be extracted\nfeatures = {'mean': (np.mean, (), {}), 'var': (np.var, (), {}), 'rms': (rms, (), {})}\n</pre> # Defining the feature-set to be extracted features = {'mean': (np.mean, (), {}), 'var': (np.var, (), {}), 'rms': (rms, (), {})} In\u00a0[14]: Copied! <pre># Extracting the desired feature-set from time-domain signals\nfeatures_df = feature_extractor(signals, features)\nfeatures_df\n</pre> # Extracting the desired feature-set from time-domain signals features_df = feature_extractor(signals, features) features_df Out[14]: mean var rms 0 0.001243 0.000042 0.006595 1 0.002147 0.000039 0.006611 2 0.001401 0.000041 0.006567 3 0.001226 0.000046 0.006917 4 0.001823 0.000051 0.007366 ... ... ... ... 2395 0.000709 0.000113 0.010639 2396 0.000366 0.000145 0.012040 2397 0.001220 0.000139 0.011851 2398 0.000523 0.000104 0.010229 2399 0.000303 0.000140 0.011821 <p>2400 rows \u00d7 3 columns</p> In\u00a0[15]: Copied! <pre>import seaborn as sns\nfrom matplotlib import pyplot as plt\nsns.set()\n\nfig, axes = plt.subplots(4, 1, figsize = (16, 10))\n\nsns.lineplot(ax=axes[0], x=range(len(signals.iloc[0,:])), y = signals.iloc[0,:])\naxes[0].set_title(\"Original Time Signal\")\naxes[0].set_ylabel(\"Amplitude\")\naxes[0].set_xlabel(\"sample\")\naxes[0].set_xlim(0, 10000)\n\nsns.lineplot(ax=axes[1], x=range(len(signals_env.iloc[0,:])), y = signals_env.iloc[0, :])\naxes[1].set_title(\"Envelope\")\naxes[1].set_ylabel(\"Amplitude\")\naxes[1].set_xlabel(\"sample\")\naxes[1].set_xlim(0, 10000)\n\n\nsns.lineplot(ax=axes[2], x = fft_freq_axis(10000, 200000), y = signals_env_fft.iloc[0, :])\naxes[2].set_title(\"FFT\")\naxes[2].set_ylabel(\"Amplitude\")\naxes[2].set_xlabel(\"Frequency (Hz)\")\naxes[2].set_xlim(0, 5000)\n\n\nsns.lineplot(ax=axes[3], x = zoomed_fft_freq_axis(0, 1000, 2000), y = signals_env_ZoomedFFT.iloc[0, :])\naxes[3].set_title(\"Zoomed FFT\")\naxes[3].set_ylabel(\"Amplitude\")\naxes[3].set_xlabel(\"Frequency (Hz)\")\naxes[3].set_xlim(0, 1000)\n\n\nplt.subplots_adjust(hspace = 0.75)\nfig.show()\n</pre> import seaborn as sns from matplotlib import pyplot as plt sns.set()  fig, axes = plt.subplots(4, 1, figsize = (16, 10))  sns.lineplot(ax=axes[0], x=range(len(signals.iloc[0,:])), y = signals.iloc[0,:]) axes[0].set_title(\"Original Time Signal\") axes[0].set_ylabel(\"Amplitude\") axes[0].set_xlabel(\"sample\") axes[0].set_xlim(0, 10000)  sns.lineplot(ax=axes[1], x=range(len(signals_env.iloc[0,:])), y = signals_env.iloc[0, :]) axes[1].set_title(\"Envelope\") axes[1].set_ylabel(\"Amplitude\") axes[1].set_xlabel(\"sample\") axes[1].set_xlim(0, 10000)   sns.lineplot(ax=axes[2], x = fft_freq_axis(10000, 200000), y = signals_env_fft.iloc[0, :]) axes[2].set_title(\"FFT\") axes[2].set_ylabel(\"Amplitude\") axes[2].set_xlabel(\"Frequency (Hz)\") axes[2].set_xlim(0, 5000)   sns.lineplot(ax=axes[3], x = zoomed_fft_freq_axis(0, 1000, 2000), y = signals_env_ZoomedFFT.iloc[0, :]) axes[3].set_title(\"Zoomed FFT\") axes[3].set_ylabel(\"Amplitude\") axes[3].set_xlabel(\"Frequency (Hz)\") axes[3].set_xlim(0, 1000)   plt.subplots_adjust(hspace = 0.75) fig.show() In\u00a0[16]: Copied! <pre>t = np.linspace(0, 1/20, 45)\nf = fft_freq_axis(1024, 200000)\n\nfig, ax = plt.subplots(figsize = (16, 8))\n\nax = sns.heatmap(signals_env_STFT[0, :, :], xticklabels = np.round(f, decimals = 2), yticklabels = np.round(t, decimals = 2), annot = False, cbar = False)\nax.set(xlabel = 'Frequency (Hz)', ylabel = 'Time (sec)')\nax.set_title('STFT')\nax.set_xticks(ax.get_xticks()[::20])\nax.set_yticks(ax.get_yticks()[::5])\n\n\nfig.show()\n</pre> t = np.linspace(0, 1/20, 45) f = fft_freq_axis(1024, 200000)  fig, ax = plt.subplots(figsize = (16, 8))  ax = sns.heatmap(signals_env_STFT[0, :, :], xticklabels = np.round(f, decimals = 2), yticklabels = np.round(t, decimals = 2), annot = False, cbar = False) ax.set(xlabel = 'Frequency (Hz)', ylabel = 'Time (sec)') ax.set_title('STFT') ax.set_xticks(ax.get_xticks()[::20]) ax.set_yticks(ax.get_yticks()[::5])   fig.show()"},{"location":"notebooks/dataset_demos/UoO_demo/#cloning-the-damavand-repository","title":"Cloning the damavand repository\u00b6","text":""},{"location":"notebooks/dataset_demos/UoO_demo/#importings","title":"Importings\u00b6","text":""},{"location":"notebooks/dataset_demos/UoO_demo/#instantiating-a-downloader-object","title":"Instantiating a downloader object\u00b6","text":""},{"location":"notebooks/dataset_demos/UoO_demo/#instantiating-a-digestor-object","title":"Instantiating a digestor object\u00b6","text":""},{"location":"notebooks/dataset_demos/UoO_demo/#aggregating-data-over-the-first-channel","title":"Aggregating data over the first channel\u00b6","text":""},{"location":"notebooks/dataset_demos/UoO_demo/#signals-metadata-declaration","title":"Signals-Metadata declaration\u00b6","text":""},{"location":"notebooks/dataset_demos/UoO_demo/#signal-processing","title":"Signal Processing\u00b6","text":""},{"location":"notebooks/dataset_demos/UoO_demo/#envelope-extraction","title":"Envelope Extraction\u00b6","text":""},{"location":"notebooks/dataset_demos/UoO_demo/#fft","title":"FFT\u00b6","text":""},{"location":"notebooks/dataset_demos/UoO_demo/#zoomedfft","title":"ZoomedFFT\u00b6","text":""},{"location":"notebooks/dataset_demos/UoO_demo/#stft","title":"STFT\u00b6","text":""},{"location":"notebooks/dataset_demos/UoO_demo/#statistical-features","title":"Statistical Features\u00b6","text":""},{"location":"notebooks/dataset_demos/UoO_demo/#visualization","title":"Visualization\u00b6","text":""},{"location":"notebooks/tutorials/Classification_demo/","title":"Cloning the damavand repository","text":"In\u00a0[1]: Copied! <pre>!git clone https://github.com/amirberenji1995/damavand\n</pre> !git clone https://github.com/amirberenji1995/damavand <pre>fatal: destination path 'damavand' already exists and is not an empty directory.\n</pre> In\u00a0[2]: Copied! <pre>!pip install -r damavand/requirements.txt\n</pre> !pip install -r damavand/requirements.txt <pre>Requirement already satisfied: certifi==2024.7.4 in /usr/local/lib/python3.10/dist-packages (from -r damavand/requirements.txt (line 1)) (2024.7.4)\nRequirement already satisfied: charset-normalizer==3.3.2 in /usr/local/lib/python3.10/dist-packages (from -r damavand/requirements.txt (line 2)) (3.3.2)\nRequirement already satisfied: idna==3.7 in /usr/local/lib/python3.10/dist-packages (from -r damavand/requirements.txt (line 3)) (3.7)\nRequirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.10/dist-packages (from -r damavand/requirements.txt (line 4)) (1.26.4)\nRequirement already satisfied: pandas==2.1.4 in /usr/local/lib/python3.10/dist-packages (from -r damavand/requirements.txt (line 5)) (2.1.4)\nRequirement already satisfied: python-dateutil==2.9.0.post0 in /usr/local/lib/python3.10/dist-packages (from -r damavand/requirements.txt (line 6)) (2.9.0.post0)\nRequirement already satisfied: pytz==2024.1 in /usr/local/lib/python3.10/dist-packages (from -r damavand/requirements.txt (line 7)) (2024.1)\nRequirement already satisfied: rarfile==4.2 in /usr/local/lib/python3.10/dist-packages (from -r damavand/requirements.txt (line 8)) (4.2)\nRequirement already satisfied: requests==2.32.3 in /usr/local/lib/python3.10/dist-packages (from -r damavand/requirements.txt (line 9)) (2.32.3)\nRequirement already satisfied: scipy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from -r damavand/requirements.txt (line 10)) (1.13.1)\nRequirement already satisfied: six==1.16.0 in /usr/local/lib/python3.10/dist-packages (from -r damavand/requirements.txt (line 11)) (1.16.0)\nRequirement already satisfied: tzdata==2024.1 in /usr/local/lib/python3.10/dist-packages (from -r damavand/requirements.txt (line 12)) (2024.1)\nRequirement already satisfied: urllib3==2.2.2 in /usr/local/lib/python3.10/dist-packages (from -r damavand/requirements.txt (line 13)) (2.2.2)\n</pre> In\u00a0[23]: Copied! <pre>from damavand.damavand.datasets.downloaders import read_addresses, ZipDatasetDownloader\nfrom damavand.damavand.datasets.digestors import SEU\nfrom damavand.damavand.signal_processing import *\nfrom damavand.damavand.utils import *\n\nimport pandas as pd\nfrom scipy.stats import skew, kurtosis\n\nfrom sklearn.preprocessing import LabelEncoder, LabelBinarizer, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import optimizers\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nsns.set()\n</pre> from damavand.damavand.datasets.downloaders import read_addresses, ZipDatasetDownloader from damavand.damavand.datasets.digestors import SEU from damavand.damavand.signal_processing import * from damavand.damavand.utils import *  import pandas as pd from scipy.stats import skew, kurtosis  from sklearn.preprocessing import LabelEncoder, LabelBinarizer, MinMaxScaler from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score, confusion_matrix  import tensorflow as tf from tensorflow.keras.layers import Dense, Input from tensorflow.keras.models import Model from tensorflow.keras import optimizers  from matplotlib import pyplot as plt import seaborn as sns sns.set() In\u00a0[4]: Copied! <pre>addresses = read_addresses()\ndownloader = ZipDatasetDownloader(addresses['SEU'])\ndownloader.download_extract('SEU.zip', 'SEU/')\n</pre> addresses = read_addresses() downloader = ZipDatasetDownloader(addresses['SEU']) downloader.download_extract('SEU.zip', 'SEU/') In\u00a0[5]: Copied! <pre>seu = SEU('SEU/')\nmining_params = {\n    'win_len': 10000,\n    'hop_len': 10000\n}\nseu.mine(mining_params)\n</pre> seu = SEU('SEU/') mining_params = {     'win_len': 10000,     'hop_len': 10000 } seu.mine(mining_params) <pre>Mining:  Chipped_20_0.csv\nMining:  Miss_20_0.csv\nMining:  Root_30_2.csv\nMining:  Surface_20_0.csv\nMining:  Health_20_0.csv\nMining:  Root_20_0.csv\nMining:  Miss_30_2.csv\nMining:  Health_30_2.csv\nMining:  Chipped_30_2.csv\nMining:  Surface_30_2.csv\nMining:  ball_20_0.csv\nMining:  comb_20_0.csv\nMining:  ball_30_2.csv\nMining:  inner_20_0.csv\nMining:  health_20_0.csv\nMining:  comb_30_2.csv\nMining:  outer_30_2.csv\nMining:  health_30_2.csv\nMining:  inner_30_2.csv\nMining:  outer_20_0.csv\n</pre> In\u00a0[6]: Copied! <pre>df = pd.concat(seu.data[1]).reset_index(drop = True)\ndf\n</pre> df = pd.concat(seu.data[1]).reset_index(drop = True) df Out[6]: 0 1 2 3 4 5 6 7 8 9 ... 9993 9994 9995 9996 9997 9998 9999 test_bed state rot_speed 0 0.000503 0.002112 0.003276 0.002187 0.000864 0.003220 -0.000612 0.000082 0.000796 0.003134 ... -0.001155 0.000290 0.000763 -0.000211 0.002804 0.003674 0.000706 Gear_set Chipped 20 1 0.003633 -0.000229 0.000552 -0.000004 0.002151 0.003697 0.001304 0.004830 0.004209 0.003994 ... 0.000752 -0.002675 0.002458 0.002286 -0.001379 0.002416 0.005218 Gear_set Chipped 20 2 0.002939 -0.000224 0.000743 0.000638 0.000941 0.004123 -0.001305 0.001551 0.005114 0.000157 ... 0.001551 0.000154 0.001876 0.001844 0.001475 0.001525 0.000941 Gear_set Chipped 20 3 0.000901 0.001673 0.000249 0.001255 0.001456 0.000186 0.001425 0.000232 -0.000758 0.002090 ... 0.001602 0.000980 0.001359 0.007073 0.002937 0.004625 0.004481 Gear_set Chipped 20 4 0.003072 0.000039 0.000633 0.000852 -0.000666 0.001584 0.000923 0.002730 0.004590 0.000892 ... -0.002054 -0.002071 0.003704 0.000231 0.003415 0.005243 0.000980 Gear_set Chipped 20 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 2075 0.003477 0.004098 -0.000902 0.002244 -0.000417 -0.000520 0.001959 0.001307 -0.002608 0.004534 ... -0.000941 -0.001752 -0.000422 -0.001873 -0.000856 0.001448 0.001341 Bearing_set outer 20 2076 0.001140 0.001454 0.002049 0.000772 0.001109 -0.001004 -0.000777 0.000978 0.000858 0.001173 ... -0.000085 0.000597 -0.000103 0.000166 0.000979 0.000370 0.000114 Bearing_set outer 20 2077 0.000862 -0.000015 0.000430 0.000468 0.000056 0.001246 0.001090 0.000552 0.000275 -0.000464 ... -0.001698 0.002050 0.000086 0.000114 0.002718 0.000453 0.000899 Bearing_set outer 20 2078 0.002031 0.000311 0.000124 -0.002179 -0.001022 -0.000153 -0.001156 -0.000972 -0.000298 -0.000947 ... 0.000472 -0.001147 0.001435 -0.000050 0.000645 -0.002612 0.003531 Bearing_set outer 20 2079 0.000778 -0.002289 0.002877 0.000565 -0.000243 0.002654 0.000502 -0.003823 0.002394 -0.001581 ... 0.001876 0.001363 -0.000221 0.002683 0.003662 0.002092 0.002457 Bearing_set outer 20 <p>2080 rows \u00d7 10003 columns</p> In\u00a0[7]: Copied! <pre>signals, metadata = df.iloc[:, : - 3], df.iloc[:, - 3 :]\nsignals\n</pre> signals, metadata = df.iloc[:, : - 3], df.iloc[:, - 3 :] signals Out[7]: 0 1 2 3 4 5 6 7 8 9 ... 9990 9991 9992 9993 9994 9995 9996 9997 9998 9999 0 0.000503 0.002112 0.003276 0.002187 0.000864 0.003220 -0.000612 0.000082 0.000796 0.003134 ... 0.001869 0.001483 0.000743 -0.001155 0.000290 0.000763 -0.000211 0.002804 0.003674 0.000706 1 0.003633 -0.000229 0.000552 -0.000004 0.002151 0.003697 0.001304 0.004830 0.004209 0.003994 ... 0.002317 0.000151 -0.000625 0.000752 -0.002675 0.002458 0.002286 -0.001379 0.002416 0.005218 2 0.002939 -0.000224 0.000743 0.000638 0.000941 0.004123 -0.001305 0.001551 0.005114 0.000157 ... 0.001116 0.000269 0.000259 0.001551 0.000154 0.001876 0.001844 0.001475 0.001525 0.000941 3 0.000901 0.001673 0.000249 0.001255 0.001456 0.000186 0.001425 0.000232 -0.000758 0.002090 ... 0.000246 -0.000108 0.001671 0.001602 0.000980 0.001359 0.007073 0.002937 0.004625 0.004481 4 0.003072 0.000039 0.000633 0.000852 -0.000666 0.001584 0.000923 0.002730 0.004590 0.000892 ... 0.001267 0.003988 0.002531 -0.002054 -0.002071 0.003704 0.000231 0.003415 0.005243 0.000980 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 2075 0.003477 0.004098 -0.000902 0.002244 -0.000417 -0.000520 0.001959 0.001307 -0.002608 0.004534 ... -0.001024 -0.000501 -0.000623 -0.000941 -0.001752 -0.000422 -0.001873 -0.000856 0.001448 0.001341 2076 0.001140 0.001454 0.002049 0.000772 0.001109 -0.001004 -0.000777 0.000978 0.000858 0.001173 ... 0.000672 0.000901 0.000944 -0.000085 0.000597 -0.000103 0.000166 0.000979 0.000370 0.000114 2077 0.000862 -0.000015 0.000430 0.000468 0.000056 0.001246 0.001090 0.000552 0.000275 -0.000464 ... 0.000809 0.004516 0.002311 -0.001698 0.002050 0.000086 0.000114 0.002718 0.000453 0.000899 2078 0.002031 0.000311 0.000124 -0.002179 -0.001022 -0.000153 -0.001156 -0.000972 -0.000298 -0.000947 ... 0.000643 0.000139 0.000671 0.000472 -0.001147 0.001435 -0.000050 0.000645 -0.002612 0.003531 2079 0.000778 -0.002289 0.002877 0.000565 -0.000243 0.002654 0.000502 -0.003823 0.002394 -0.001581 ... 0.000547 0.001853 0.001171 0.001876 0.001363 -0.000221 0.002683 0.003662 0.002092 0.002457 <p>2080 rows \u00d7 10000 columns</p> In\u00a0[8]: Copied! <pre>metadata\n</pre> metadata Out[8]: test_bed state rot_speed 0 Gear_set Chipped 20 1 Gear_set Chipped 20 2 Gear_set Chipped 20 3 Gear_set Chipped 20 4 Gear_set Chipped 20 ... ... ... ... 2075 Bearing_set outer 20 2076 Bearing_set outer 20 2077 Bearing_set outer 20 2078 Bearing_set outer 20 2079 Bearing_set outer 20 <p>2080 rows \u00d7 3 columns</p> In\u00a0[9]: Copied! <pre>window = scipy.signal.windows.hann(signals.shape[1])\nfreq_filter = scipy.signal.butter(25, [15, 950], 'bandpass', fs = 2000, output='sos')\n</pre> window = scipy.signal.windows.hann(signals.shape[1]) freq_filter = scipy.signal.butter(25, [15, 950], 'bandpass', fs = 2000, output='sos') In\u00a0[10]: Copied! <pre>signals_fft = fft(signals, freq_filter = freq_filter, window = window)\nsignals_fft.shape\n</pre> signals_fft = fft(signals, freq_filter = freq_filter, window = window) signals_fft.shape Out[10]: <pre>(2080, 5000)</pre> In\u00a0[11]: Copied! <pre>signals_features = feature(signals, {'mean': np.mean, 'var': np.var, 'rms': rms, 'skew': skew, 'kurtosis': kurtosis})\nsignals_features.shape\n</pre> signals_features = feature(signals, {'mean': np.mean, 'var': np.var, 'rms': rms, 'skew': skew, 'kurtosis': kurtosis}) signals_features.shape Out[11]: <pre>(2080, 5)</pre> In\u00a0[12]: Copied! <pre>y = metadata['state']\nle = LabelEncoder()\ny_encoded = le.fit_transform(y)\nnp.unique(y_encoded, return_counts = True)\n</pre> y = metadata['state'] le = LabelEncoder() y_encoded = le.fit_transform(y) np.unique(y_encoded, return_counts = True) Out[12]: <pre>(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n array([208, 208, 208, 208, 208, 208, 208, 208, 208, 208]))</pre> In\u00a0[13]: Copied! <pre>lb = LabelBinarizer()\ny_bin = lb.fit_transform(y_encoded)\n</pre> lb = LabelBinarizer() y_bin = lb.fit_transform(y_encoded) In\u00a0[14]: Copied! <pre>x_fft_train, x_fft_test, x_features_train, x_features_test, y_encoded_train, y_encoded_test, y_bin_train, y_bin_test = train_test_split(signals_fft, signals_features, y_encoded, y_bin, test_size=0.33, random_state=42)\n</pre> x_fft_train, x_fft_test, x_features_train, x_features_test, y_encoded_train, y_encoded_test, y_bin_train, y_bin_test = train_test_split(signals_fft, signals_features, y_encoded, y_bin, test_size=0.33, random_state=42) In\u00a0[15]: Copied! <pre>DT = DecisionTreeClassifier()\nDT.fit(x_features_train, y_encoded_train)\n</pre> DT = DecisionTreeClassifier() DT.fit(x_features_train, y_encoded_train) Out[15]: <pre>DecisionTreeClassifier()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifier<pre>DecisionTreeClassifier()</pre> In\u00a0[16]: Copied! <pre>def model_creator():\n\n  input = Input(shape = (5000,), name='input1')\n  e = Dense(units = 2500, activation='tanh', name = 'HL1')(input)\n  predicted_label = Dense(units = 10, activation='softmax', name='label')(e)\n\n  return Model(inputs = input, outputs = predicted_label)\n</pre> def model_creator():    input = Input(shape = (5000,), name='input1')   e = Dense(units = 2500, activation='tanh', name = 'HL1')(input)   predicted_label = Dense(units = 10, activation='softmax', name='label')(e)    return Model(inputs = input, outputs = predicted_label) In\u00a0[17]: Copied! <pre>scaler = MinMaxScaler()\nx_fft_train_scaled = scaler.fit_transform(x_fft_train)\nx_fft_test_scaled = scaler.transform(x_fft_test)\n</pre> scaler = MinMaxScaler() x_fft_train_scaled = scaler.fit_transform(x_fft_train) x_fft_test_scaled = scaler.transform(x_fft_test) In\u00a0[18]: Copied! <pre>lr = 0.0001\nep = 25\n\nmodel = model_creator()\nopt = optimizers.Adam(learning_rate=lr, decay=lr / ep)\nmodel.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n\nhistories  = model.fit(x_fft_train_scaled, y_bin_train, validation_split=0.25, epochs = ep, batch_size = 250)\n</pre> lr = 0.0001 ep = 25  model = model_creator() opt = optimizers.Adam(learning_rate=lr, decay=lr / ep) model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])  histories  = model.fit(x_fft_train_scaled, y_bin_train, validation_split=0.25, epochs = ep, batch_size = 250) <pre>Epoch 1/25\n</pre> <pre>/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:33: UserWarning: Argument `decay` is no longer supported and will be ignored.\n  warnings.warn(\n</pre> <pre>5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 737ms/step - accuracy: 0.1329 - loss: 2.5406 - val_accuracy: 0.2636 - val_loss: 1.9598\nEpoch 2/25\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 412ms/step - accuracy: 0.2904 - loss: 1.8779 - val_accuracy: 0.6017 - val_loss: 1.4486\nEpoch 3/25\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 405ms/step - accuracy: 0.6108 - loss: 1.3780 - val_accuracy: 0.7593 - val_loss: 1.1201\nEpoch 4/25\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 407ms/step - accuracy: 0.8138 - loss: 1.0299 - val_accuracy: 0.9828 - val_loss: 0.8795\nEpoch 5/25\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 407ms/step - accuracy: 0.9521 - loss: 0.8040 - val_accuracy: 0.9628 - val_loss: 0.7117\nEpoch 6/25\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3s 647ms/step - accuracy: 0.9742 - loss: 0.6438 - val_accuracy: 0.9857 - val_loss: 0.5797\nEpoch 7/25\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 408ms/step - accuracy: 0.9917 - loss: 0.4939 - val_accuracy: 1.0000 - val_loss: 0.4681\nEpoch 8/25\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 408ms/step - accuracy: 1.0000 - loss: 0.4052 - val_accuracy: 1.0000 - val_loss: 0.4004\nEpoch 9/25\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 407ms/step - accuracy: 1.0000 - loss: 0.3366 - val_accuracy: 1.0000 - val_loss: 0.3459\nEpoch 10/25\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3s 455ms/step - accuracy: 1.0000 - loss: 0.2738 - val_accuracy: 1.0000 - val_loss: 0.2988\nEpoch 11/25\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 768ms/step - accuracy: 1.0000 - loss: 0.2409 - val_accuracy: 1.0000 - val_loss: 0.2632\nEpoch 12/25\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3s 412ms/step - accuracy: 1.0000 - loss: 0.2074 - val_accuracy: 1.0000 - val_loss: 0.2260\nEpoch 13/25\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3s 410ms/step - accuracy: 1.0000 - loss: 0.1778 - val_accuracy: 1.0000 - val_loss: 0.2030\nEpoch 14/25\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 406ms/step - accuracy: 1.0000 - loss: 0.1610 - val_accuracy: 1.0000 - val_loss: 0.1859\nEpoch 15/25\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 433ms/step - accuracy: 1.0000 - loss: 0.1504 - val_accuracy: 1.0000 - val_loss: 0.1671\nEpoch 16/25\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 687ms/step - accuracy: 1.0000 - loss: 0.1291 - val_accuracy: 1.0000 - val_loss: 0.1518\nEpoch 17/25\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 414ms/step - accuracy: 1.0000 - loss: 0.1148 - val_accuracy: 1.0000 - val_loss: 0.1380\nEpoch 18/25\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3s 451ms/step - accuracy: 1.0000 - loss: 0.1058 - val_accuracy: 1.0000 - val_loss: 0.1291\nEpoch 19/25\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 414ms/step - accuracy: 1.0000 - loss: 0.0931 - val_accuracy: 1.0000 - val_loss: 0.1182\nEpoch 20/25\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3s 409ms/step - accuracy: 1.0000 - loss: 0.0856 - val_accuracy: 1.0000 - val_loss: 0.1096\nEpoch 21/25\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4s 696ms/step - accuracy: 1.0000 - loss: 0.0815 - val_accuracy: 1.0000 - val_loss: 0.1015\nEpoch 22/25\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 413ms/step - accuracy: 1.0000 - loss: 0.0769 - val_accuracy: 1.0000 - val_loss: 0.0955\nEpoch 23/25\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3s 416ms/step - accuracy: 1.0000 - loss: 0.0667 - val_accuracy: 1.0000 - val_loss: 0.0898\nEpoch 24/25\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 421ms/step - accuracy: 1.0000 - loss: 0.0644 - val_accuracy: 1.0000 - val_loss: 0.0832\nEpoch 25/25\n5/5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 414ms/step - accuracy: 1.0000 - loss: 0.0593 - val_accuracy: 1.0000 - val_loss: 0.0778\n</pre> In\u00a0[19]: Copied! <pre>fig, axes = plt.subplots(1, 2, figsize=(16,4))\nfig.suptitle('Deep Learning Model Training Process')\naxes[0].plot(histories.history['loss'], label='Training Loss')\naxes[0].plot(histories.history['val_loss'], label='Validation Loss')\naxes[0].set_xlabel('Epochs')\naxes[0].set_ylabel('Loss')\naxes[0].set_title('Loss Vs. Epochs')\naxes[0].legend()\n\naxes[1].plot(histories.history['accuracy'], label='Training Accuracy')\naxes[1].plot(histories.history['val_accuracy'], label='Validation Accuracy')\naxes[1].set_xlabel('Epochs')\naxes[1].set_ylabel('Accuracy')\naxes[1].set_title('Accuracy Vs. Epochs')\naxes[1].legend()\nplt.show()\n</pre> fig, axes = plt.subplots(1, 2, figsize=(16,4)) fig.suptitle('Deep Learning Model Training Process') axes[0].plot(histories.history['loss'], label='Training Loss') axes[0].plot(histories.history['val_loss'], label='Validation Loss') axes[0].set_xlabel('Epochs') axes[0].set_ylabel('Loss') axes[0].set_title('Loss Vs. Epochs') axes[0].legend()  axes[1].plot(histories.history['accuracy'], label='Training Accuracy') axes[1].plot(histories.history['val_accuracy'], label='Validation Accuracy') axes[1].set_xlabel('Epochs') axes[1].set_ylabel('Accuracy') axes[1].set_title('Accuracy Vs. Epochs') axes[1].legend() plt.show() In\u00a0[21]: Copied! <pre>y_train_pred_DT = DT.predict(x_features_train)\ny_test_pred_DT = DT.predict(x_features_test)\ny_train_pred_DL = np.argmax(model.predict(x_fft_train_scaled), axis = 1)\ny_test_pred_DL = np.argmax(model.predict(x_fft_test_scaled), axis = 1)\n</pre> y_train_pred_DT = DT.predict(x_features_train) y_test_pred_DT = DT.predict(x_features_test) y_train_pred_DL = np.argmax(model.predict(x_fft_train_scaled), axis = 1) y_test_pred_DL = np.argmax(model.predict(x_fft_test_scaled), axis = 1) <pre>44/44 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 27ms/step\n22/22 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 44ms/step\n</pre> In\u00a0[22]: Copied! <pre>print('Decision tree training accuracy: ', np.round(accuracy_score(y_encoded_train, y_train_pred_DT), 4))\nprint('Decision tree testing accuracy: ', np.round(accuracy_score(y_encoded_test, y_test_pred_DT), 4))\nprint('Deep learnign model training accuracy: ', np.round(accuracy_score(y_encoded_train, y_train_pred_DL), 4))\nprint('Deep learnign model testing accuracy: ', np.round(accuracy_score(y_encoded_test, y_test_pred_DL),  4))\n</pre> print('Decision tree training accuracy: ', np.round(accuracy_score(y_encoded_train, y_train_pred_DT), 4)) print('Decision tree testing accuracy: ', np.round(accuracy_score(y_encoded_test, y_test_pred_DT), 4)) print('Deep learnign model training accuracy: ', np.round(accuracy_score(y_encoded_train, y_train_pred_DL), 4)) print('Deep learnign model testing accuracy: ', np.round(accuracy_score(y_encoded_test, y_test_pred_DL),  4)) <pre>Decision tree training accuracy:  1.0\nDecision tree testing accuracy:  0.8195\nDeep learnign model training accuracy:  1.0\nDeep learnign model testing accuracy:  1.0\n</pre> In\u00a0[24]: Copied! <pre>confusion_matrix_DT = confusion_matrix(y_encoded_test, y_test_pred_DT)\nconfusion_matrix_DL = confusion_matrix(y_encoded_test, y_test_pred_DL)\n</pre> confusion_matrix_DT = confusion_matrix(y_encoded_test, y_test_pred_DT) confusion_matrix_DL = confusion_matrix(y_encoded_test, y_test_pred_DL) In\u00a0[44]: Copied! <pre>fig, axes = plt.subplots(1, 2, figsize=(12,4))\n\nsns.heatmap(confusion_matrix_DT, annot=True, square = True, cmap='Blues', fmt='g', cbar = False, ax=axes[0])\naxes[0].set_xlabel(\"Predicted\", fontsize=14, labelpad=20)\naxes[0].xaxis.set_ticklabels(le.classes_)\naxes[0].set_ylabel(\"Actual\", fontsize=14, labelpad=20)\naxes[0].yaxis.set_ticklabels(le.classes_)\naxes[0].set_title(\"Decision Tree Confusion Matrix\", fontsize=14, pad=20)\naxes[0].tick_params(axis='x', rotation=90)\naxes[0].tick_params(axis='y', rotation=360)\n\nsns.heatmap(confusion_matrix_DL, annot=True, square = True, cmap='Blues', fmt='g', cbar = False, ax=axes[1])\naxes[1].set_xlabel(\"Predicted\", fontsize=14, labelpad=20)\naxes[1].xaxis.set_ticklabels(le.classes_)\naxes[1].set_ylabel(\"Actual\", fontsize=14, labelpad=20)\naxes[1].yaxis.set_ticklabels(le.classes_)\naxes[1].set_title(\"Deep Learning Confusion Matrix\", fontsize=14, pad=20)\naxes[1].tick_params(axis='x', rotation=90)\naxes[1].tick_params(axis='y', rotation=360)\n\nplt.show()\n</pre> fig, axes = plt.subplots(1, 2, figsize=(12,4))  sns.heatmap(confusion_matrix_DT, annot=True, square = True, cmap='Blues', fmt='g', cbar = False, ax=axes[0]) axes[0].set_xlabel(\"Predicted\", fontsize=14, labelpad=20) axes[0].xaxis.set_ticklabels(le.classes_) axes[0].set_ylabel(\"Actual\", fontsize=14, labelpad=20) axes[0].yaxis.set_ticklabels(le.classes_) axes[0].set_title(\"Decision Tree Confusion Matrix\", fontsize=14, pad=20) axes[0].tick_params(axis='x', rotation=90) axes[0].tick_params(axis='y', rotation=360)  sns.heatmap(confusion_matrix_DL, annot=True, square = True, cmap='Blues', fmt='g', cbar = False, ax=axes[1]) axes[1].set_xlabel(\"Predicted\", fontsize=14, labelpad=20) axes[1].xaxis.set_ticklabels(le.classes_) axes[1].set_ylabel(\"Actual\", fontsize=14, labelpad=20) axes[1].yaxis.set_ticklabels(le.classes_) axes[1].set_title(\"Deep Learning Confusion Matrix\", fontsize=14, pad=20) axes[1].tick_params(axis='x', rotation=90) axes[1].tick_params(axis='y', rotation=360)  plt.show()"},{"location":"notebooks/tutorials/Classification_demo/#cloning-the-damavand-repository","title":"Cloning the damavand repository\u00b6","text":""},{"location":"notebooks/tutorials/Classification_demo/#importings","title":"Importings\u00b6","text":""},{"location":"notebooks/tutorials/Classification_demo/#instantiating-a-downloader-object","title":"Instantiating a downloader object\u00b6","text":""},{"location":"notebooks/tutorials/Classification_demo/#instantiating-a-digestor-object","title":"Instantiating a digestor object\u00b6","text":""},{"location":"notebooks/tutorials/Classification_demo/#aggregating-data-over-the-second-channel","title":"Aggregating data over the second channel\u00b6","text":""},{"location":"notebooks/tutorials/Classification_demo/#signals-metadata-declaration","title":"Signals-Metadata declaration\u00b6","text":""},{"location":"notebooks/tutorials/Classification_demo/#signal-processing","title":"Signal Processing\u00b6","text":""},{"location":"notebooks/tutorials/Classification_demo/#fft","title":"FFT\u00b6","text":""},{"location":"notebooks/tutorials/Classification_demo/#statistical-features","title":"Statistical Features\u00b6","text":""},{"location":"notebooks/tutorials/Classification_demo/#health-classification-task","title":"Health classification task\u00b6","text":""},{"location":"notebooks/tutorials/Classification_demo/#target-declaration","title":"Target declaration\u00b6","text":""},{"location":"notebooks/tutorials/Classification_demo/#train-test-split","title":"Train-test split\u00b6","text":""},{"location":"notebooks/tutorials/Classification_demo/#training","title":"Training\u00b6","text":""},{"location":"notebooks/tutorials/Classification_demo/#classic-machine-learning","title":"Classic machine learning\u00b6","text":""},{"location":"notebooks/tutorials/Classification_demo/#deep-learning","title":"Deep Learning\u00b6","text":""},{"location":"notebooks/tutorials/Classification_demo/#evaluation","title":"Evaluation\u00b6","text":""},{"location":"notebooks/tutorials/Classification_demo/#accuracies","title":"Accuracies\u00b6","text":""},{"location":"notebooks/tutorials/Classification_demo/#confusion-matrices","title":"Confusion Matrices\u00b6","text":""},{"location":"notebooks/tutorials/custom_digestor_development/","title":"How to develop a custom digestor?","text":"In\u00a0[1]: Copied! <pre>!git clone https://github.com/amirberenji1995/damavand\n</pre> !git clone https://github.com/amirberenji1995/damavand <pre>fatal: destination path 'damavand' already exists and is not an empty directory.\n</pre> In\u00a0[2]: Copied! <pre>!pip install -r damavand/requirements.txt\n</pre> !pip install -r damavand/requirements.txt <pre>Requirement already satisfied: certifi==2024.7.4 in /usr/local/lib/python3.11/dist-packages (from -r damavand/requirements.txt (line 1)) (2024.7.4)\nRequirement already satisfied: charset-normalizer==3.3.2 in /usr/local/lib/python3.11/dist-packages (from -r damavand/requirements.txt (line 2)) (3.3.2)\nRequirement already satisfied: idna==3.7 in /usr/local/lib/python3.11/dist-packages (from -r damavand/requirements.txt (line 3)) (3.7)\nRequirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (from -r damavand/requirements.txt (line 4)) (1.26.4)\nRequirement already satisfied: pandas==2.1.4 in /usr/local/lib/python3.11/dist-packages (from -r damavand/requirements.txt (line 5)) (2.1.4)\nRequirement already satisfied: python-dateutil==2.9.0.post0 in /usr/local/lib/python3.11/dist-packages (from -r damavand/requirements.txt (line 6)) (2.9.0.post0)\nRequirement already satisfied: pytz==2024.1 in /usr/local/lib/python3.11/dist-packages (from -r damavand/requirements.txt (line 7)) (2024.1)\nRequirement already satisfied: rarfile==4.2 in /usr/local/lib/python3.11/dist-packages (from -r damavand/requirements.txt (line 8)) (4.2)\nRequirement already satisfied: requests==2.32.3 in /usr/local/lib/python3.11/dist-packages (from -r damavand/requirements.txt (line 9)) (2.32.3)\nRequirement already satisfied: scipy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from -r damavand/requirements.txt (line 10)) (1.13.1)\nRequirement already satisfied: six==1.16.0 in /usr/local/lib/python3.11/dist-packages (from -r damavand/requirements.txt (line 11)) (1.16.0)\nRequirement already satisfied: tzdata==2024.1 in /usr/local/lib/python3.11/dist-packages (from -r damavand/requirements.txt (line 12)) (2024.1)\nRequirement already satisfied: urllib3==2.2.2 in /usr/local/lib/python3.11/dist-packages (from -r damavand/requirements.txt (line 13)) (2.2.2)\n</pre> <p>A digestor is basically a crawler walking the directories inside the base directory of a downloaded dataset to not only extract data from the raw dataset files (usually files with mat, csv or xlsx extensions) but also organize the corresponding metadata.</p> <p>In this section, we demonstrate how to develop a digestor for UoO dataset from scratch.</p> <p>Let's start from downloading the dataset:</p> In\u00a0[3]: Copied! <pre>from damavand.damavand.datasets.downloaders import read_addresses, ZipDatasetDownloader\n\naddresses = read_addresses()\ndownloader = ZipDatasetDownloader(addresses['UoO'])\ndownloader.download_extract('UoO.zip', 'UoO/')\n</pre> from damavand.damavand.datasets.downloaders import read_addresses, ZipDatasetDownloader  addresses = read_addresses() downloader = ZipDatasetDownloader(addresses['UoO']) downloader.download_extract('UoO.zip', 'UoO/') <p>The code snippet above, downloads and extract the dataset into <code>UoO</code> folder. Next step would be to investigate its content:</p> In\u00a0[4]: Copied! <pre>import os\n\nos.listdir('UoO')\n</pre> import os  os.listdir('UoO') Out[4]: <pre>['H-B-2.mat',\n 'O-D-2.mat',\n 'I-D-1.mat',\n 'H-D-3.mat',\n 'I-A-3.mat',\n 'H-C-1.mat',\n 'H-A-3.mat',\n 'H-B-3.mat',\n 'H-B-1.mat',\n 'H-C-2.mat',\n 'O-B-3.mat',\n 'O-A-2.mat',\n 'H-A-2.mat',\n 'I-B-3.mat',\n 'I-D-2.mat',\n 'I-D-3.mat',\n 'H-D-2.mat',\n 'I-C-1.mat',\n 'I-B-2.mat',\n 'O-C-3.mat',\n 'I-B-1.mat',\n 'H-A-1.mat',\n 'I-A-1.mat',\n 'O-D-1.mat',\n 'H-C-3.mat',\n 'O-A-3.mat',\n 'H-D-1.mat',\n 'I-A-2.mat',\n 'O-A-1.mat',\n 'O-B-2.mat',\n 'O-D-3.mat',\n 'I-C-3.mat',\n 'O-C-2.mat',\n 'O-C-1.mat',\n 'I-C-2.mat',\n 'O-B-1.mat']</pre> <p>According to the official paper, use a naming convention in the form of C-L-R.mat, where:</p> <ul> <li>C corresponding to the health class (H for the healthy, O for the outer race and I for the inner-race faults)</li> <li>L corresponding to the loading pattern (A for increasing rotational speed, B for decreasing rotational speed, C for increasing the decreasing rotational speed and D for decreasing the increasing rotational speed)</li> <li>R corresponding to the repetition number (from 1 to 3)</li> </ul> <p>Let's examine what is stored in one of the files; to do so, we need to open a .mat, easily by <code>scipy.io.loadmat</code> as below:</p> In\u00a0[5]: Copied! <pre>from scipy.io import loadmat\n\nbase_dir = 'UoO/'\nmat_data = loadmat(base_dir + 'H-B-2.mat')\nmat_data.keys()\n</pre> from scipy.io import loadmat  base_dir = 'UoO/' mat_data = loadmat(base_dir + 'H-B-2.mat') mat_data.keys() Out[5]: <pre>dict_keys(['__header__', '__version__', '__globals__', 'Channel_1', 'Channel_2'])</pre> <p>According to the authors, they have collected accelerometer data and encoder as Channel_1 and Channel_2 respectively.</p> <p>Mining a dataset involves openning, minining (splitting the signals) and attaching the metadata.</p> <p>Following code snippet fulfills this for the Channel_1.</p> In\u00a0[6]: Copied! <pre>from damavand.damavand.utils import splitter\n\nbase_dir = 'UoO/'\nwin_len, hop_len = 10000, 10000\ndata = []\n\nfor file in os.listdir(base_dir):\n  # Splitting the data file to extract three pieces of metadata state, loading and repetition\n  state = file.split('.')[0].split('-')[:-1][0]\n  loading = file.split('.')[0].split('-')[:-1][1]\n  rep = file.split('.')[0].split('-')[-1]\n\n  # Opening the .mat file as a python dictionary\n  mat_data = loadmat(base_dir + file)\n  # Mining the data available in Channel_1, using the splitter function and saving it to the temp_df variable\n  temp_df = splitter(mat_data['Channel_1'].reshape((-1)), win_len, hop_len)\n  # Assigning the metadata, as new columns to the temp_df\n  temp_df['state'], temp_df['loading'], temp_df['rep'] = state, loading, rep\n  # Appending the temp_df to the data list\n  data.append(temp_df)\n</pre> from damavand.damavand.utils import splitter  base_dir = 'UoO/' win_len, hop_len = 10000, 10000 data = []  for file in os.listdir(base_dir):   # Splitting the data file to extract three pieces of metadata state, loading and repetition   state = file.split('.')[0].split('-')[:-1][0]   loading = file.split('.')[0].split('-')[:-1][1]   rep = file.split('.')[0].split('-')[-1]    # Opening the .mat file as a python dictionary   mat_data = loadmat(base_dir + file)   # Mining the data available in Channel_1, using the splitter function and saving it to the temp_df variable   temp_df = splitter(mat_data['Channel_1'].reshape((-1)), win_len, hop_len)   # Assigning the metadata, as new columns to the temp_df   temp_df['state'], temp_df['loading'], temp_df['rep'] = state, loading, rep   # Appending the temp_df to the data list   data.append(temp_df) <p>The code snippet above, mines merely Channel_1 data; moreover, it goes through all the files available, however, cases are possible where the user is only interested in a limited number of repetitions. To fix those, the above code snippet can be modified as below:</p> In\u00a0[7]: Copied! <pre>base_dir = 'UoO/'\n\nchannels = ['Channel_1', 'Channel_2']\nwin_len, hop_len = 10000, 10000\nreps = [1]\ndata = {channel: [] for channel in channels}\n\nfor file in os.listdir(base_dir):\n  if file.endswith('.mat'):\n    rep = int(file.split('.')[0].split('-')[-1])\n    if rep in reps:\n      state = file.split('.')[0].split('-')[:-1][0]\n      loading = file.split('.')[0].split('-')[:-1][1]\n      mat_data = loadmat(base_dir + file)\n      for channel in data.keys():\n        temp_df = splitter(mat_data[channel].reshape((-1)), win_len, hop_len)\n        temp_df['state'], temp_df['loading'], temp_df['rep'] = state, loading, rep\n        data[channel].append(temp_df)\n</pre> base_dir = 'UoO/'  channels = ['Channel_1', 'Channel_2'] win_len, hop_len = 10000, 10000 reps = [1] data = {channel: [] for channel in channels}  for file in os.listdir(base_dir):   if file.endswith('.mat'):     rep = int(file.split('.')[0].split('-')[-1])     if rep in reps:       state = file.split('.')[0].split('-')[:-1][0]       loading = file.split('.')[0].split('-')[:-1][1]       mat_data = loadmat(base_dir + file)       for channel in data.keys():         temp_df = splitter(mat_data[channel].reshape((-1)), win_len, hop_len)         temp_df['state'], temp_df['loading'], temp_df['rep'] = state, loading, rep         data[channel].append(temp_df) <p>We regard object-oriented programming essential to develop reusable and easily maintainable code; therefore, in Damavand, every digestor must be implemented as a Python class.</p> <p>The code snippet above can be transformed into the following class, easily:</p> In\u00a0[8]: Copied! <pre>class UoO():\n  # Instantiating of a digestor object, requires the declaration of the base directory, channels and repetitions the user is interested in.\n  def __init__(self, base_directory, channels = ['Channel_1', 'Channel_2'], reps = list(range(1,4))):\n    self.base_dir = base_directory\n    self.channels = channels\n    self.reps = reps\n\n    # Once the dataset is mined, data will be presented in a Python dictionary whose keys are elements of the channels, user has specified during the instantiation.\n    self.data = {key: [] for key in self.channels}\n\n  # To mine the dataset, user is supposed to declare window length and hop length. To do so, these must be passed in the form of a Python dictionary whose keys are 'win_len' and 'hop_len'.\n  def mine(self, mining_params):\n    for file in os.listdir(self.base_dir):\n      if file.endswith('.mat'):\n        rep = int(file.split('.')[0].split('-')[-1])\n        if rep in self.reps:\n          state = file.split('.')[0].split('-')[:-1][0]\n          loading = file.split('.')[0].split('-')[:-1][1]\n          mat_data = loadmat(self.base_dir + file)\n          for channel in self.data.keys():\n            temp_df = splitter(mat_data[channel].reshape((-1)), mining_params['win_len'], mining_params['hop_len'])\n            temp_df['state'], temp_df['loading'], temp_df['rep'] = state, loading, rep\n            self.data[channel].append(temp_df)\n</pre> class UoO():   # Instantiating of a digestor object, requires the declaration of the base directory, channels and repetitions the user is interested in.   def __init__(self, base_directory, channels = ['Channel_1', 'Channel_2'], reps = list(range(1,4))):     self.base_dir = base_directory     self.channels = channels     self.reps = reps      # Once the dataset is mined, data will be presented in a Python dictionary whose keys are elements of the channels, user has specified during the instantiation.     self.data = {key: [] for key in self.channels}    # To mine the dataset, user is supposed to declare window length and hop length. To do so, these must be passed in the form of a Python dictionary whose keys are 'win_len' and 'hop_len'.   def mine(self, mining_params):     for file in os.listdir(self.base_dir):       if file.endswith('.mat'):         rep = int(file.split('.')[0].split('-')[-1])         if rep in self.reps:           state = file.split('.')[0].split('-')[:-1][0]           loading = file.split('.')[0].split('-')[:-1][1]           mat_data = loadmat(self.base_dir + file)           for channel in self.data.keys():             temp_df = splitter(mat_data[channel].reshape((-1)), mining_params['win_len'], mining_params['hop_len'])             temp_df['state'], temp_df['loading'], temp_df['rep'] = state, loading, rep             self.data[channel].append(temp_df) <p>The digestor is easily usable as below:</p> In\u00a0[9]: Copied! <pre>dataset = UoO('UoO/', ['Channel_1', 'Channel_2'], [1])\nmining_params = {'win_len': 10000, 'hop_len': 10000}\ndataset.mine(mining_params)\n\n# Concatenate all observations under `Channel_1`\nimport pandas as pd\n\ndf = pd.concat(dataset.data['Channel_1']).reset_index(drop = True)\ndf\n</pre> dataset = UoO('UoO/', ['Channel_1', 'Channel_2'], [1]) mining_params = {'win_len': 10000, 'hop_len': 10000} dataset.mine(mining_params)  # Concatenate all observations under `Channel_1` import pandas as pd  df = pd.concat(dataset.data['Channel_1']).reset_index(drop = True) df Out[9]: 0 1 2 3 4 5 6 7 8 9 ... 9993 9994 9995 9996 9997 9998 9999 state loading rep 0 -0.026534 -0.012723 0.008981 0.025424 0.026410 0.016874 0.008324 0.021806 0.042195 0.051074 ... -0.032453 -0.040675 -0.037386 -0.018642 0.000102 0.008981 0.009968 I D 1 1 0.014572 0.032658 0.059624 0.085274 0.097770 0.101716 0.104676 0.116843 0.137232 0.149399 ... 0.002076 0.016545 0.022135 0.011941 -0.001871 -0.006474 -0.002199 I D 1 2 0.003391 0.003720 -0.002199 -0.004501 0.001418 0.013585 0.019176 0.013585 0.004377 -0.001213 ... 0.010297 -0.011078 -0.033111 -0.050211 -0.061392 -0.067311 -0.069942 I D 1 3 -0.068627 -0.060077 -0.039688 -0.010749 0.017860 0.036933 0.040880 0.045812 0.049101 0.049430 ... -0.007132 0.007995 0.011283 0.000431 -0.014367 -0.016011 -0.000884 I D 1 4 0.013914 0.015558 0.006022 -0.002857 0.005364 0.019176 0.022793 0.011941 -0.004173 -0.009434 ... 0.010297 0.021478 0.029699 0.034303 0.035618 0.033645 0.036605 I D 1 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 2395 -0.002199 -0.001213 0.000431 0.001747 0.003062 0.003391 0.003391 0.003720 0.004049 0.004377 ... 0.003391 0.003062 0.002404 0.001747 0.001089 0.001089 0.002404 O B 1 2396 0.002404 0.000760 0.000102 -0.000884 -0.001213 -0.011736 -0.003186 -0.001542 -0.000226 0.000431 ... -0.002857 -0.001542 -0.000226 0.001418 0.003062 0.004377 0.006022 O B 1 2397 0.006351 0.008324 0.005693 0.009310 0.007995 0.009310 0.008653 0.007666 0.007995 0.000102 ... -0.001542 -0.002528 -0.001871 -0.001213 -0.001213 -0.001213 -0.000884 O B 1 2398 -0.001542 -0.002199 -0.000555 -0.000226 0.000431 0.001747 0.002076 0.002733 0.004049 -0.016669 ... 0.002076 0.002404 0.005693 0.006351 0.006679 0.006679 0.007995 O B 1 2399 0.008981 0.009968 0.008981 0.007666 0.006679 0.006022 0.006679 0.007995 0.007666 0.007995 ... -0.000884 -0.000555 -0.000884 -0.001542 -0.002528 -0.001213 -0.001871 O B 1 <p>2400 rows \u00d7 10003 columns</p> In\u00a0[9]: Copied! <pre>\n</pre> In\u00a0[9]: Copied! <pre>\n</pre>"},{"location":"notebooks/tutorials/custom_digestor_development/#how-to-develop-a-custom-digestor","title":"How to develop a custom digestor?\u00b6","text":""},{"location":"notebooks/tutorials/custom_feature_extraction/","title":"How to extract a custom feature with Damavand?","text":"In\u00a0[\u00a0]: Copied! <p>Hand-crafted features developed by experts are of great importance for rotating machinery condition monitoring. Damavand simplifies the extraction of a wide range of features. through the employment of a Python function as <code>feature_extractor</code>. This function takes a collection of features as key-value pairs of the following format:</p> <pre>features = {\n  'feature_name': (feature, (args), {kwargs})\n}\n</pre> <p>In this notation, <code>feature</code>, <code>args</code> and <code>kwargs</code> are the feature implemented as a Python function, tuple of its positional arguments and the dict of keyword arguments.</p> <p>Once such dictionary is declared, the <code>feature_extractor</code> function can be used easily as below:</p> <pre>from numpy import mean, std\nfrom damavand.damavand.signal_processing.feature_extraction import rms, feature_extractor\n\nfeatures = {\n  'mean': (mean, (), {}),\n  'std': (std, (), {}),\n  'rms': (rms, (), {}),\n}\n\n# Assuming the desired signals are stored in the `signals` variable\nfeatures_df = feature_extractor(signals, features)\n</pre> <p>The application of <code>feature_extractor</code> is not limited to the features available in Damavand; custom features can be implemented (as Python functions) and extracted by this function. To do so, let's implement the following feature (known as Squared Mean of Square Roots of Absolutes or SMSA) as a Python function:</p> <p>$f = \\left(\\frac{\\sum_{n=1}^{N} \\sqrt{\\|x(n)\\|}}{N}\\right)^{2}$</p> <pre>import numpy\nfrom damavand.damavand.signal_processing.feature_extraction import feature_extractor\n\ndef smsa(x):\n  return np.square(np.mean(np.sqrt(np.abs(x))))\n\nfeatures = {\n  'smsa': (feature, (), {}),\n}\n\n# Assuming the desired signals are stored in the `signals` variable\nfeatures_df = feature_extractor(signals, features)\n</pre> <p>The above function takes in no argument, except for the signal itself; however, it is not always the case. Consider the following feature (known as spectral centroid or SC for short):</p> <p>$f = \\frac{\\sum_{k=1}^{K} f_k \\cdot s(k)}{\\sum_{k=1}^{K} s(k)}$</p> <p>where $s(k)$ and $f_k$ correspond to the frequency spectrum and corresponding frequency axis. In contrast to the previous feature, this feature also requires the frequency axis as an additional argument. The below implementation makes the extraction of this feature, possible:</p> <pre>import numpy as np\nfrom damavand.damavand.signal_processing.feature_extraction import feature_extractor\n\ndef feature(spectrum, freq_axis):\n  return np.sum(spectrum * freq_axis) / np.sum(spectrum)\n\n# Assuming that the `freq_axis` stores the frequency axis for detail on how to achieve this checkout:\n# https://github.com/amirberenji1995/damavand/blob/main/documentations/utils.md#fft_freq_axistime_len-sampling_freq\nfeatures = {\n  'sc': (feature, (freq_axis), {})\n}\n\n# Assuming that the `signals_fft` includes frequency domain signals\nfeatures_df = feature_extractor(signals_fft, features)\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/tutorials/custom_feature_extraction/#how-to-extract-a-custom-feature-with-damavand","title":"How to extract a custom feature with Damavand?\u00b6","text":""},{"location":"notebooks/tutorials/custom_feature_extraction/#1-feature-extraction-by-damavand","title":"1. Feature Extraction by Damavand\u00b6","text":""},{"location":"notebooks/tutorials/custom_feature_extraction/#2-extraction-of-a-custom-feature","title":"2. Extraction of a custom feature\u00b6","text":""},{"location":"notebooks/tutorials/signal_processing_101/","title":"Signal Processing - 101","text":"In\u00a0[1]: Copied! <pre>!git clone https://github.com/amirberenji1995/damavand\n</pre> !git clone https://github.com/amirberenji1995/damavand <pre>fatal: destination path 'damavand' already exists and is not an empty directory.\n</pre> In\u00a0[2]: Copied! <pre>!pip install -r damavand/requirements.txt\n</pre> !pip install -r damavand/requirements.txt <pre>Requirement already satisfied: certifi==2024.7.4 in /usr/local/lib/python3.11/dist-packages (from -r damavand/requirements.txt (line 1)) (2024.7.4)\nRequirement already satisfied: charset-normalizer==3.3.2 in /usr/local/lib/python3.11/dist-packages (from -r damavand/requirements.txt (line 2)) (3.3.2)\nRequirement already satisfied: idna==3.7 in /usr/local/lib/python3.11/dist-packages (from -r damavand/requirements.txt (line 3)) (3.7)\nRequirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (from -r damavand/requirements.txt (line 4)) (1.26.4)\nRequirement already satisfied: pandas==2.1.4 in /usr/local/lib/python3.11/dist-packages (from -r damavand/requirements.txt (line 5)) (2.1.4)\nRequirement already satisfied: python-dateutil==2.9.0.post0 in /usr/local/lib/python3.11/dist-packages (from -r damavand/requirements.txt (line 6)) (2.9.0.post0)\nRequirement already satisfied: pytz==2024.1 in /usr/local/lib/python3.11/dist-packages (from -r damavand/requirements.txt (line 7)) (2024.1)\nRequirement already satisfied: rarfile==4.2 in /usr/local/lib/python3.11/dist-packages (from -r damavand/requirements.txt (line 8)) (4.2)\nRequirement already satisfied: requests==2.32.3 in /usr/local/lib/python3.11/dist-packages (from -r damavand/requirements.txt (line 9)) (2.32.3)\nRequirement already satisfied: scipy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from -r damavand/requirements.txt (line 10)) (1.13.1)\nRequirement already satisfied: six==1.16.0 in /usr/local/lib/python3.11/dist-packages (from -r damavand/requirements.txt (line 11)) (1.16.0)\nRequirement already satisfied: tzdata==2024.1 in /usr/local/lib/python3.11/dist-packages (from -r damavand/requirements.txt (line 12)) (2024.1)\nRequirement already satisfied: urllib3==2.2.2 in /usr/local/lib/python3.11/dist-packages (from -r damavand/requirements.txt (line 13)) (2.2.2)\n</pre> <p>Vibration data from rotating machinery is a goldmine of information, but it's hidden in noise and complexity.  Signal processing is the key to unlocking this information.  Think of it as translating the machine's \"language\" of vibrations into a clear, understandable message about its health.</p> <p>We start with the signal itself\u2014a representation of vibration over time, either as a continuous analog wave or a discrete digital sequence.  Signal processing then uses various techniques to enhance and analyze this signal:</p> <ul> <li>Time-domain analysis: Looking directly at the signal's shape over time.  Think of it like reading a story\u2014we look for sudden changes, repeating patterns, or unusual spikes.</li> <li>Frequency-domain analysis (FFT): Transforming the signal to see its hidden frequencies.  This is like musical analysis\u2014we identify the dominant notes (frequencies) that reveal the machine's internal workings and potential problems.</li> <li>Filtering: Cleaning up the signal by removing unwanted noise. This is like editing a recording\u2014we remove background noise to hear the main instrument clearly.</li> </ul> <p>By combining these techniques, we can extract crucial features from the vibration data, such as characteristic frequencies associated with specific faults, enabling early detection and diagnosis of problems.  This allows for proactive maintenance, preventing costly breakdowns and ensuring operational efficiency.  This notebook explores these techniques and how they are applied using [Package Name] to analyze vibration data and improve rotating machinery reliability.</p> <p>Damavand offers two main approaches to process vibration signals:</p> <ol> <li>Applying transformations: Damavand facilitates the application of the most frequent signal processing transformations.</li> <li>Fetaure extraction: In addition to the transformations, Damavand also supports the extraction of expert-defined features.</li> </ol> In\u00a0[3]: Copied! <pre>from damavand.damavand.datasets.downloaders import read_addresses, ZipDatasetDownloader\nfrom damavand.damavand.datasets.digestors import MFPT\nimport pandas as pd\n\n# Downloading the MFPT dataset\naddresses = read_addresses()\ndownloader = ZipDatasetDownloader(addresses['MFPT'])\ndownloader.download_extract('MFPT.zip', 'MFPT/')\n\nmfpt = MFPT('MFPT/MFPT Fault Data Sets/', [\n    '1 - Three Baseline Conditions',\n    '2 - Three Outer Race Fault Conditions',\n    '3 - Seven More Outer Race Fault Conditions',\n    '4 - Seven Inner Race Fault Conditions',\n])\n\n# Mining the dataset\nmining_params = {\n    97656: {'win_len': 16671, 'hop_len': 2000},\n    48828: {'win_len': 8337, 'hop_len': 1000},\n}\nmfpt.mine(mining_params)\n\n# Signal/Metadata split\ndf = pd.concat(mfpt.data[48828]).reset_index(drop = True)\nsignals, metadata = df.iloc[:, : - 4], df.iloc[:, - 4 :]\n</pre> from damavand.damavand.datasets.downloaders import read_addresses, ZipDatasetDownloader from damavand.damavand.datasets.digestors import MFPT import pandas as pd  # Downloading the MFPT dataset addresses = read_addresses() downloader = ZipDatasetDownloader(addresses['MFPT']) downloader.download_extract('MFPT.zip', 'MFPT/')  mfpt = MFPT('MFPT/MFPT Fault Data Sets/', [     '1 - Three Baseline Conditions',     '2 - Three Outer Race Fault Conditions',     '3 - Seven More Outer Race Fault Conditions',     '4 - Seven Inner Race Fault Conditions', ])  # Mining the dataset mining_params = {     97656: {'win_len': 16671, 'hop_len': 2000},     48828: {'win_len': 8337, 'hop_len': 1000}, } mfpt.mine(mining_params)  # Signal/Metadata split df = pd.concat(mfpt.data[48828]).reset_index(drop = True) signals, metadata = df.iloc[:, : - 4], df.iloc[:, - 4 :] In\u00a0[4]: Copied! <pre>from damavand.damavand.signal_processing.transformations import env\n\n# Envelope extraction\nsignals_env = env(signals)\n</pre> from damavand.damavand.signal_processing.transformations import env  # Envelope extraction signals_env = env(signals) In\u00a0[5]: Copied! <pre>from scipy.signal.windows import hann\nfrom scipy.signal import butter\nfrom damavand.damavand.signal_processing.transformations import fft\n\n# Defining a window to avoid frequency leakage\nwindow = hann(signals_env.shape[1])\n# Defining a bandpass frequency filter to both remove near-DC component and avoid aliasing\nfreq_filter = butter(25, [5, 23500], 'bandpass', fs = 48828, output='sos')\n\n# Frequency spectra extraction, through FFT\nsignals_fft = fft(signals, freq_filter = freq_filter, window = window)\n</pre> from scipy.signal.windows import hann from scipy.signal import butter from damavand.damavand.signal_processing.transformations import fft  # Defining a window to avoid frequency leakage window = hann(signals_env.shape[1]) # Defining a bandpass frequency filter to both remove near-DC component and avoid aliasing freq_filter = butter(25, [5, 23500], 'bandpass', fs = 48828, output='sos')  # Frequency spectra extraction, through FFT signals_fft = fft(signals, freq_filter = freq_filter, window = window) In\u00a0[6]: Copied! <pre>from scipy.signal.windows import hann\nfrom scipy.signal import butter\nfrom damavand.damavand.signal_processing.transformations import zoomed_fft\n\n# Defining a window to avoid frequency leakage\nwindow = hann(signals_env.shape[1])\n# Defining a bandpass frequency filter to both remove near-DC component and avoid aliasing\nfreq_filter = butter(25, [5, 23500], 'bandpass', fs = 48828, output='sos')\n\n# Frequency spectra extraction within the range of 0 to 2500 Hz, through zoomed_FFT\nsignals_ZoomedFFT = zoomed_fft(signals_env, 0, 2500, 2500, 48828, freq_filter = freq_filter, window = window)\n</pre> from scipy.signal.windows import hann from scipy.signal import butter from damavand.damavand.signal_processing.transformations import zoomed_fft  # Defining a window to avoid frequency leakage window = hann(signals_env.shape[1]) # Defining a bandpass frequency filter to both remove near-DC component and avoid aliasing freq_filter = butter(25, [5, 23500], 'bandpass', fs = 48828, output='sos')  # Frequency spectra extraction within the range of 0 to 2500 Hz, through zoomed_FFT signals_ZoomedFFT = zoomed_fft(signals_env, 0, 2500, 2500, 48828, freq_filter = freq_filter, window = window) In\u00a0[7]: Copied! <pre>from scipy.signal.windows import hann\nfrom scipy.signal import butter\nfrom damavand.damavand.signal_processing.transformations import stft\n\n# Defining a window to avoid frequency leakage (unlike previous transformations, the lenght of the window must match the window_len of the stft)\nSTFT_window = hann(2400)\n# Defining a bandpass frequency filter to both remove near-DC component and avoid aliasing\nSTFT_freq_filter = butter(25, [5, 23500], 'bandpass', fs = float(metadata.iloc[0, 0]), output='sos')\n\n# Time-Frequency representation extraction using 2400-point long segments and a hop lenght of 200 points.\nsignals_STFT = stft(signals, 2400, 200, STFT_freq_filter, STFT_window)\n</pre> from scipy.signal.windows import hann from scipy.signal import butter from damavand.damavand.signal_processing.transformations import stft  # Defining a window to avoid frequency leakage (unlike previous transformations, the lenght of the window must match the window_len of the stft) STFT_window = hann(2400) # Defining a bandpass frequency filter to both remove near-DC component and avoid aliasing STFT_freq_filter = butter(25, [5, 23500], 'bandpass', fs = float(metadata.iloc[0, 0]), output='sos')  # Time-Frequency representation extraction using 2400-point long segments and a hop lenght of 200 points. signals_STFT = stft(signals, 2400, 200, STFT_freq_filter, STFT_window) <p>The following cells visualize the same observation, under different transformations.</p> In\u00a0[8]: Copied! <pre>import seaborn as sns\nfrom matplotlib import pyplot as plt\nsns.set()\n\nfrom damavand.damavand.utils import *\n\nfig, axes = plt.subplots(4, 1, figsize = (16, 10))\n\nsns.lineplot(ax=axes[0], x=range(len(signals.iloc[0,:])), y = signals.iloc[0,:])\naxes[0].set_title(\"Original Time Signal\")\naxes[0].set_ylabel(\"Amplitude\")\naxes[0].set_xlabel(\"sample\")\naxes[0].set_xlim(0, 8337)\n\nsns.lineplot(ax=axes[1], x=range(len(signals_env.iloc[0,:])), y = signals_env.iloc[0, :])\naxes[1].set_title(\"Envelope\")\naxes[1].set_ylabel(\"Amplitude\")\naxes[1].set_xlabel(\"sample\")\naxes[1].set_xlim(0, 8337)\n\n\nsns.lineplot(ax=axes[2], x = fft_freq_axis(8337, 48828), y = signals_fft.iloc[0, :])\naxes[2].set_title(\"FFT\")\naxes[2].set_ylabel(\"Amplitude\")\naxes[2].set_xlabel(\"Frequency (Hz)\")\naxes[2].set_xlim(0, 24424)\n\n\nsns.lineplot(ax=axes[3], x = zoomed_fft_freq_axis(0, 2500, 2500), y = signals_ZoomedFFT.iloc[0, :])\naxes[3].set_title(\"Zoomed FFT\")\naxes[3].set_ylabel(\"Amplitude\")\naxes[3].set_xlabel(\"Frequency (Hz)\")\naxes[3].set_xlim(0, 2500)\n\n\nplt.subplots_adjust(hspace = 0.75)\nfig.show()\n</pre> import seaborn as sns from matplotlib import pyplot as plt sns.set()  from damavand.damavand.utils import *  fig, axes = plt.subplots(4, 1, figsize = (16, 10))  sns.lineplot(ax=axes[0], x=range(len(signals.iloc[0,:])), y = signals.iloc[0,:]) axes[0].set_title(\"Original Time Signal\") axes[0].set_ylabel(\"Amplitude\") axes[0].set_xlabel(\"sample\") axes[0].set_xlim(0, 8337)  sns.lineplot(ax=axes[1], x=range(len(signals_env.iloc[0,:])), y = signals_env.iloc[0, :]) axes[1].set_title(\"Envelope\") axes[1].set_ylabel(\"Amplitude\") axes[1].set_xlabel(\"sample\") axes[1].set_xlim(0, 8337)   sns.lineplot(ax=axes[2], x = fft_freq_axis(8337, 48828), y = signals_fft.iloc[0, :]) axes[2].set_title(\"FFT\") axes[2].set_ylabel(\"Amplitude\") axes[2].set_xlabel(\"Frequency (Hz)\") axes[2].set_xlim(0, 24424)   sns.lineplot(ax=axes[3], x = zoomed_fft_freq_axis(0, 2500, 2500), y = signals_ZoomedFFT.iloc[0, :]) axes[3].set_title(\"Zoomed FFT\") axes[3].set_ylabel(\"Amplitude\") axes[3].set_xlabel(\"Frequency (Hz)\") axes[3].set_xlim(0, 2500)   plt.subplots_adjust(hspace = 0.75) fig.show() In\u00a0[9]: Copied! <pre>import numpy as np\nfrom damavand.damavand.utils import fft_freq_axis\n\nt = np.linspace(0, 0.1707, 30)\nf = fft_freq_axis(2400, 48828)\n\nfig, ax = plt.subplots(figsize = (16, 8))\n\nax = sns.heatmap(signals_STFT[0, :, :], xticklabels = np.round(f, decimals = 2), yticklabels = np.round(t, decimals = 2), annot = False, cbar = False)\nax.set(xlabel = 'Frequency (Hz)', ylabel = 'Time (sec)')\nax.set_title('STFT')\nax.set_xticks(ax.get_xticks()[::30])\nax.set_yticks(ax.get_yticks()[::2])\n\n\nfig.show()\n</pre> import numpy as np from damavand.damavand.utils import fft_freq_axis  t = np.linspace(0, 0.1707, 30) f = fft_freq_axis(2400, 48828)  fig, ax = plt.subplots(figsize = (16, 8))  ax = sns.heatmap(signals_STFT[0, :, :], xticklabels = np.round(f, decimals = 2), yticklabels = np.round(t, decimals = 2), annot = False, cbar = False) ax.set(xlabel = 'Frequency (Hz)', ylabel = 'Time (sec)') ax.set_title('STFT') ax.set_xticks(ax.get_xticks()[::30]) ax.set_yticks(ax.get_yticks()[::2])   fig.show() <p>Hand-crafted features (from both time and frequency domains) are widely used for rotating machinery conidition monitoring. Damavand, facilitates the extraction of such features from raw (time and frequency) data.</p> <p>Features must be implemented as a Python function; then, they must be passed as key-vlaue pairs of <code>\"feature_name\": (feature_function, (args), (kwargs))</code> to the <code>feature_extractor</code> function, alongside the signal bank.</p> <p>The following section, demonstrate the extraction of both time and frequency domains, respectively.</p> In\u00a0[10]: Copied! <pre>from damavand.damavand.signal_processing.feature_extraction import *\nfrom numpy import mean, std\nfrom scipy.stats import skew, kurtosis\n\n# Defining the desired features (below is a wide set of time-domain features)\ntime_features = {\n  'mean': (mean, (), {}),\n  'std': (std, (), {}),\n  'smsa': (smsa, (), {}),\n  'rms': (rms, (), {}),\n  'peak': (peak, (), {}),\n  'skew': (skew, (), {}),\n  'kurtosis': (kurtosis, (), {}),\n  'crest_factor': (crest_factor, (), {}),\n  'clearance_factor': (clearance_factor, (), {}),\n  'shape_factor': (shape_factor, (), {}),\n  'impulse_factor': (impulse_factor, (), {}),\n}\n\n# Extracting the features from the signals\ntime_features_df = feature_extractor(signals, time_features)\ntime_features_df\n</pre> from damavand.damavand.signal_processing.feature_extraction import * from numpy import mean, std from scipy.stats import skew, kurtosis  # Defining the desired features (below is a wide set of time-domain features) time_features = {   'mean': (mean, (), {}),   'std': (std, (), {}),   'smsa': (smsa, (), {}),   'rms': (rms, (), {}),   'peak': (peak, (), {}),   'skew': (skew, (), {}),   'kurtosis': (kurtosis, (), {}),   'crest_factor': (crest_factor, (), {}),   'clearance_factor': (clearance_factor, (), {}),   'shape_factor': (shape_factor, (), {}),   'impulse_factor': (impulse_factor, (), {}), }  # Extracting the features from the signals time_features_df = feature_extractor(signals, time_features) time_features_df Out[10]: mean std smsa rms peak skew kurtosis crest_factor clearance_factor shape_factor impulse_factor 0 -0.184024 0.816661 0.531492 0.837138 4.259008 -0.012312 1.455025 5.087580 8.013310 1.309186 6.660590 1 -0.180836 0.806005 0.527821 0.826042 4.259008 -0.013430 1.443762 5.155922 8.069034 1.304167 6.724184 2 -0.183012 0.827373 0.542704 0.847372 4.259008 -0.016724 1.362095 5.026139 7.847754 1.302203 6.545052 3 -0.178946 0.826251 0.541090 0.845407 4.259008 -0.017216 1.457751 5.037820 7.871167 1.303262 6.565601 4 -0.180836 0.830811 0.546731 0.850263 4.259008 -0.018926 1.333448 5.009045 7.789949 1.298817 6.505832 ... ... ... ... ... ... ... ... ... ... ... ... 1941 -0.212814 1.913001 0.700623 1.924802 23.788190 1.280296 35.833671 12.358773 33.952902 1.957477 24.192016 1942 -0.214259 1.927105 0.705142 1.938979 23.788190 1.253180 34.835319 12.268409 33.735319 1.952201 23.950398 1943 -0.210920 2.010897 0.709647 2.021928 23.788190 1.254061 35.683098 11.765102 33.521169 2.003571 23.572221 1944 -0.207534 2.051129 0.721359 2.061601 23.788190 1.225027 33.720097 11.538696 32.976889 2.002352 23.104527 1945 -0.211032 2.028678 0.718398 2.039624 23.788190 1.409082 34.159432 11.663025 33.112834 1.997823 23.300664 <p>1946 rows \u00d7 11 columns</p> In\u00a0[12]: Copied! <pre>from scipy.signal.windows import hann\nfrom scipy.signal import butter\nfrom numpy import mean, var\nfrom scipy.stats import skew, kurtosis\nfrom damavand.damavand.signal_processing.feature_extraction import *\nfrom damavand.damavand.utils import *\n\n# Applying the FFT to transform data into frequency-domain\nwindow = hann(signals.shape[1])\nfreq_filter = butter(25, [5, 12500], 'bandpass', fs = 25600, output='sos')\nsignals_fft = fft(signals, freq_filter = freq_filter, window = window)\n\n# Extracting frequency axis, as it is essential for some of the features\nfreq_axis = fft_freq_axis(8337, 48828)\n\n# Defining the desired features (below is a wide set of frequency-domain features)\nfreq_features = {\n  'mean': (mean, (), {}),\n  'var': (var, (), {}),\n  'skew': (skew, (), {}),\n  'kurtosis': (kurtosis, (), {}),\n  'spectral_centroid': (spectral_centroid, (freq_axis,), {}),\n  'P17': (P17, (freq_axis,), {}),\n  'P18': (P18, (freq_axis,), {}),\n  'P19': (P19, (freq_axis,), {}),\n  'P20': (P20, (freq_axis,), {}),\n  'P21': (P21, (freq_axis,), {}),\n  'P22': (P22, (freq_axis,), {}),\n  'P23': (P23, (freq_axis,), {}),\n  'P24': (P24, (freq_axis,), {}),\n}\n\n# Extracting the features from the frequency domain signals (assuming that frequency domain signals are stored in signals_fft)\nfreq_features_df = feature_extractor(signals_fft, freq_features)\nfreq_features_df\n</pre> from scipy.signal.windows import hann from scipy.signal import butter from numpy import mean, var from scipy.stats import skew, kurtosis from damavand.damavand.signal_processing.feature_extraction import * from damavand.damavand.utils import *  # Applying the FFT to transform data into frequency-domain window = hann(signals.shape[1]) freq_filter = butter(25, [5, 12500], 'bandpass', fs = 25600, output='sos') signals_fft = fft(signals, freq_filter = freq_filter, window = window)  # Extracting frequency axis, as it is essential for some of the features freq_axis = fft_freq_axis(8337, 48828)  # Defining the desired features (below is a wide set of frequency-domain features) freq_features = {   'mean': (mean, (), {}),   'var': (var, (), {}),   'skew': (skew, (), {}),   'kurtosis': (kurtosis, (), {}),   'spectral_centroid': (spectral_centroid, (freq_axis,), {}),   'P17': (P17, (freq_axis,), {}),   'P18': (P18, (freq_axis,), {}),   'P19': (P19, (freq_axis,), {}),   'P20': (P20, (freq_axis,), {}),   'P21': (P21, (freq_axis,), {}),   'P22': (P22, (freq_axis,), {}),   'P23': (P23, (freq_axis,), {}),   'P24': (P24, (freq_axis,), {}), }  # Extracting the features from the frequency domain signals (assuming that frequency domain signals are stored in signals_fft) freq_features_df = feature_extractor(signals_fft, freq_features) freq_features_df Out[12]: mean var skew kurtosis spectral_centroid P17 P18 P19 P20 P21 P22 P23 0 0.008128 0.000048 3.972099 34.700943 10103.754012 594.861703 12067.302042 3.041661e+08 0.691918 0.058875 3.296088 243.828309 1 0.008224 0.000049 4.226418 39.724596 10101.204514 597.026539 12057.112161 3.035333e+08 0.692054 0.059104 3.284973 242.601454 2 0.008337 0.000052 4.387611 42.846097 10115.274554 599.343719 12058.357781 3.026115e+08 0.693179 0.059251 3.203823 241.252598 3 0.008446 0.000055 4.522593 45.004077 10085.433805 602.427202 12028.582999 3.012369e+08 0.693043 0.059732 3.150419 238.789827 4 0.008544 0.000056 4.704061 47.915756 10064.189983 606.186304 12012.376031 3.003047e+08 0.693183 0.060232 3.074401 234.696083 ... ... ... ... ... ... ... ... ... ... ... ... ... 1941 0.017581 0.000211 1.855698 5.432757 9096.302345 865.657575 11196.719650 2.974461e+08 0.649212 0.095166 4.027094 121.352483 1942 0.018051 0.000267 2.143322 7.144127 8743.698604 851.344485 10798.311351 2.840365e+08 0.640721 0.097367 4.327539 126.561441 1943 0.018985 0.000384 2.816263 12.007638 8295.449088 866.144272 10408.203953 2.765825e+08 0.625840 0.104412 4.587824 124.835702 1944 0.019875 0.000496 3.453754 18.189097 8033.496912 891.651113 10224.472190 2.762766e+08 0.615133 0.110992 4.730160 119.649973 1945 0.020374 0.000548 3.771482 21.122118 7969.487499 909.717767 10204.517608 2.774169e+08 0.612669 0.114150 4.696645 114.307708 <p>1946 rows \u00d7 12 columns</p> In\u00a0[11]: Copied! <pre>\n</pre>"},{"location":"notebooks/tutorials/signal_processing_101/#signal-processing-101","title":"Signal Processing - 101\u00b6","text":"<p>This notebook is a brief introduction to Signal Processing for vibration data analysis, with a focus on its application in rotating machinery condition monitoring.</p>"},{"location":"notebooks/tutorials/signal_processing_101/#1-signal-processing-unlocking-insights-from-vibration-data","title":"1. Signal Processing: Unlocking Insights from Vibration Data\u00b6","text":""},{"location":"notebooks/tutorials/signal_processing_101/#2-essential-theory","title":"2. Essential Theory\u00b6","text":""},{"location":"notebooks/tutorials/signal_processing_101/#21-sampling-of-vibration-signals","title":"2.1 Sampling of Vibration Signals\u00b6","text":"<p>Real-world vibration signals are continuous-time and analog.  Digital processing requires sampling\u2014converting the signal into a discrete-time sequence by measuring amplitude at regular intervals determined by the sampling frequency, f<sub>s</sub>, with sampling interval T<sub>s</sub> = 1/f<sub>s</sub>.</p> <p></p>"},{"location":"notebooks/tutorials/signal_processing_101/#22-nyquist-shannon-sampling-theorem","title":"2.2 Nyquist-Shannon Sampling Theorem\u00b6","text":"<p>Aliasing is phenomena which causes higher frequencies to appear as lower ones in the sampled data.</p> <p></p> <p>To avoid information loss (aliasing), the sampling frequency must be at least twice the maximum frequency present in the signal (f<sub>s</sub> \u2265 2f<sub>max</sub>). Anti-aliasing filters are used before sampling to remove frequencies above f<sub>max</sub>.</p>"},{"location":"notebooks/tutorials/signal_processing_101/#23-leakage-error","title":"2.3 Leakage Error\u00b6","text":"<p>Leakage error arises in frequency spectrum analysis when the analyzed signal's duration is not an integer multiple of its fundamental period. This discontinuity at the observation window's edges causes spectral energy to \"leak\" into adjacent frequency bins, blurring the true frequency components and distorting their amplitudes.</p> <p>Windowing techniques (e.g., Hanning, Hamming) mitigate this by smoothly tapering the signal's ends, reducing the discontinuity and minimizing spectral leakage.</p>"},{"location":"notebooks/tutorials/signal_processing_101/#24-modulation-and-demodulation","title":"2.4 Modulation and Demodulation\u00b6","text":"<p>Modulation modifies a carrier signal's properties using a message signal.  In Amplitude Modulation (AM), the modulated signal is:  s(t) = A<sub>c</sub>[1 + m(t)]cos(2\u03c0f<sub>c</sub>t), where A<sub>c</sub> and f<sub>c</sub> are the carrier's amplitude and frequency, and m(t) is the message signal.</p> <p></p> <p>In RCM, machine speed variations modulate fault frequencies. Demodulation thecniques (etc. envelope extraction using Hilbert transform) recovers the message signal.</p>"},{"location":"notebooks/tutorials/signal_processing_101/#25-frequency-filtering","title":"2.5 Frequency Filtering\u00b6","text":"<p>Filtering selectively removes or attenuates specific frequency components.  Common filter types include:</p> <ul> <li>Low-pass filters: Pass frequencies below a cutoff frequency and attenuate higher frequencies.</li> <li>High-pass filters: Pass frequencies above a cutoff frequency and attenuate lower frequencies.</li> <li>Band-pass filters: Pass frequencies within a specific band and attenuate frequencies outside this band.</li> <li>Band-stop filters (notch filters): Attenuate frequencies within a specific band and pass frequencies outside this band.</li> </ul> <p></p>"},{"location":"notebooks/tutorials/signal_processing_101/#3-signal-processing-using-damavand","title":"3. Signal Processing using Damavand\u00b6","text":""},{"location":"notebooks/tutorials/signal_processing_101/#31-transformations","title":"3.1 Transformations\u00b6","text":""},{"location":"notebooks/tutorials/signal_processing_101/#310-downloading-and-mining-the-mfpt-dataset","title":"3.1.0 Downloading and Mining the MFPT dataset\u00b6","text":""},{"location":"notebooks/tutorials/signal_processing_101/#311-envelope-extraction-hilbert-transform","title":"3.1.1 Envelope extraction (Hilbert transform)\u00b6","text":"<p>Extracting the envelope of of signals.</p>"},{"location":"notebooks/tutorials/signal_processing_101/#312-frequency-spectrum-extraction-fast-fourier-transform","title":"3.1.2 Frequency spectrum extraction (Fast-Fourier Transform)\u00b6","text":"<p>Applying the Fast-Fourier Transform algorithim to derive frequency domain representation of a set of signals</p>"},{"location":"notebooks/tutorials/signal_processing_101/#313-refined-frequency-range-spectrum-extraction-zoomfft-algorithm","title":"3.1.3 Refined frequency range spectrum extraction (ZoomFFT Algorithm)\u00b6","text":"<p>Applying the ZoomFFT algorithm to derive a fine-grained frequency representation in a desired frequency range</p>"},{"location":"notebooks/tutorials/signal_processing_101/#314-time-frequency-representation-extraction-short-time-fourier-transform","title":"3.1.4 Time-Frequency representation extraction (Short-Time Fourier Transform)\u00b6","text":"<p>Application of Short-Time Fourier Transform to derive Time-Frequency representation of the inputted signals</p>"},{"location":"notebooks/tutorials/signal_processing_101/#32-feature-extraction","title":"3.2 Feature Extraction\u00b6","text":""},{"location":"notebooks/tutorials/signal_processing_101/#321-time-domain-features","title":"3.2.1 Time-domain Features\u00b6","text":""},{"location":"notebooks/tutorials/signal_processing_101/#322-frequency-domain-features","title":"3.2.2 Frequency-domain Features\u00b6","text":""}]}